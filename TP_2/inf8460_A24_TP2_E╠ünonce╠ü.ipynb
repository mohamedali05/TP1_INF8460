{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INF8460 – Traitement automatique de la langue naturelle - Automne 2024\n",
    "\n",
    "## TP2: Plongements de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification de l'équipe:\n",
    "\n",
    "### Groupe de laboratoire:\n",
    "\n",
    "### Equipe numéro :\n",
    "\n",
    "### Membres:\n",
    "\n",
    "- membre 1 (% de contribution, nature de la contribution)\n",
    "- membre 2 (% de contribution, nature de la contribution)\n",
    "- membre 3 (% de contribution, nature de la contribution)\n",
    "\n",
    "* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. Soyez précis ! N'indiquez pas seulement : travail réparti équitablement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif du TP\n",
    "\n",
    "L'objectif de ce TP est d'entraîner un modèle de plongements lexicaux qui intègre des notions de synonymie et d'antonymie en utilisant des réseaux de neurones. \n",
    "\n",
    "Contrairement aux plongements vus en cours comme GloVe ou Word2Vec, qui positionnent souvent les mots à proximité équivalente de leurs synonymes et antonymes, nous chercherons à faire distinguer à nos modèles les synonymes et antonymes, en rapprochant les mots de leurs synonymes et en les éloignant de leurs antonymes.\n",
    "\n",
    "### Jeux de données\n",
    "\n",
    "**Paires de synonymes et antonymes** (entraînement) : Les fichiers `train_synonyms.txt` et `train_antonymms.txt` contiennent les paires de synonymes et d'antonymes qui serviront à l'entraînement de nos modèles.\n",
    "- train_synonyms: ~640k paires de synonymes\n",
    "- train_antonyms: ~12k paires d'antonymes\n",
    "\n",
    "**SimLex-999** (test) : Le fichier `simlex_english.txt` contient 1000 paires de mots et leur similarité entre 0 et 10. Des antonymes auront une similarité de 0 et des mots proches auront une similarité plus élevée. Par exemples :\n",
    "\n",
    "- *nice* & *cruel* -> 0\n",
    "- *violent* & *angry* -> 5.9\n",
    "- *essential* & *necessary* -> 9.8\n",
    "\n",
    "### Développement du TP\n",
    "\n",
    "Le TP suivra les étapes suivantes:\n",
    "\n",
    "- Partie 1 : Familiarisation avec GloVe, modèle de plongements de mots pré-entraîné\n",
    "- Partie 2 : Évaluation de GloVe sur SimLex-999\n",
    "- Partie 3 : Mise en place de la méthode d'entraînement\n",
    "- Partie 4 : Entraînement de zéro (baseline)\n",
    "- Partie 5 : Entraînement utilisant GloVe pré-entraîné et conclusion\n",
    "\n",
    "Le TP est noté sur 89 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies autorisées\n",
    "\n",
    "+ numpy\n",
    "+ pandas\n",
    "+ torch\n",
    "+ matplotlib\n",
    "\n",
    "Si vous voulez utiliser une autre librairie, veuillez demander à votre chargé de lab.\n",
    "\n",
    "### Imports\n",
    "\n",
    "Les imports effectués dans la cellule suivante devraient être suffisants pour faire tout ce TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:40:48.738262600Z",
     "start_time": "2024-09-26T12:40:45.809409600Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GloVe (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons utiliser le modèle pré-entraîné GloVe qui crée des plongements lexicaux de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Chargement du modèle\n",
    "\n",
    "La cellule suivante permet charger le modèle GloVe pré-entraîné. Le chargement du modèle peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T12:42:21.840934100Z",
     "start_time": "2024-09-26T12:42:21.761905600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n",
      "'unzip' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "# # Décommenter pour télécharger les GloVe embeddings à partir de https://nlp.stanford.edu/projects/glove/\n",
    "!wget http://nlp.stanford.edu/data/glove.42B.300d.zip -P /content\n",
    "!unzip /content/glove.42B.300d.zip -d /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:44:57.604461700Z",
     "start_time": "2024-09-26T13:44:57.600413100Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_path = 'glove.42B.300d.txt'\n",
    "\n",
    "data_root = 'data'\n",
    "\n",
    "train_synonyms_path = f\"{data_root}/train_synonyms.txt\"\n",
    "train_antonyms_path = f\"{data_root}/train_antonyms.txt\"\n",
    "\n",
    "eval_simlex = f'{data_root}/simlex_english.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:44:58.430575200Z",
     "start_time": "2024-09-26T13:44:58.424338300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_word_vectors(filepath, vocab=None):\n",
    "    \"\"\"\n",
    "    Télécharge le modèle pré-entraîné de plongements de mots en pytorch\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    index = 0\n",
    "    index_to_world = { }\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            if vocab is None or word in vocab:\n",
    "                embedding = np.array(split_line[1:], dtype=np.float32)\n",
    "                word_to_index[word] = index\n",
    "                embeddings.append(embedding)\n",
    "                index += 1\n",
    "    \n",
    "    embeddings = np.stack(embeddings)\n",
    "    embeddings = torch.from_numpy(embeddings)\n",
    "    return word_to_index, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:46:31.605856900Z",
     "start_time": "2024-09-26T13:44:59.602741400Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index, embeddings = load_word_vectors(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T13:05:19.529944600Z",
     "start_time": "2024-09-26T13:05:19.528138100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Implémentez la fonction `cosine_similarity` avec pytorch et sans utiliser `torch.nn.CosineSimilarity` (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:48:27.994719300Z",
     "start_time": "2024-09-26T13:48:27.992187300Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de similarité cosinus entre deux matrices\n",
    "\n",
    "    Args : \n",
    "        a   : torch.Tensor, shape=(n, d)\n",
    "        b   : torch.Tensor, shape=(m, d)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, shape=(n, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    a_norm = a / a.norm(dim=1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    \n",
    "    return torch.mm(a_norm, b_norm.T)\n",
    "            \n",
    "            \n",
    "    \n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Complétez la fonction `n_closest_vect` qui retourne les n mots les plus proches d'un mot donné. (5 points)\n",
    "\n",
    "`n_closest_vect` prendra en entrée la matrice des plongements `embeddings`, le dictionnaire de correspondance entre les mots et les indices `word_to_index`, le plongement d'un mot `word` et le nombre `n` de mots attendus. La fonction devra retourner la liste des mots dont les plongements sont les plus proches du vecteur de référence et leur similarité cosinus.\n",
    "\n",
    "C'est-à-dire les n mots avec lesquels le mot a la plus grande similarité cosinus. Utilisez la fonction `cosine_similarity` que vous venez d'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:48:31.286159700Z",
     "start_time": "2024-09-26T13:48:31.265452600Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_closest_vect(embeddings, word_to_index, word, n=5):\n",
    "    \"\"\"\n",
    "    Trouve les n mots les plus proches du vecteur donné et leur similarité\n",
    "\n",
    "    Args : \n",
    "        embeddings      : torch.Tensor, shape=(vocab_size, embedding_dim)\n",
    "        Matrice de plongement de tous les mots\n",
    "\n",
    "        word_to_index   : dict\n",
    "        Dictionnaire qui relie un mot à son index dans le vocabulaire\n",
    "\n",
    "        word            : torch.Tensor, shape=(embedding_dim,)\n",
    "        Plongement du mot dont on cherche les n mots les plus proches\n",
    "\n",
    "        n               : int, number of closest words to return\n",
    "        Nombre de mots à retourner\n",
    "\n",
    "    Returns:\n",
    "    Liste de tuple contenant les n mots les plus similaires avec leur coefficient\n",
    "    de similarité\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    similarities = cosine_similarity(word.unsqueeze(0), embeddings)\n",
    "    top_n_similarities, top_n_indices = torch.topk(similarities.squeeze(0), n)\n",
    "\n",
    "    closest_words = []\n",
    "    for i in range(n):\n",
    "        similar_word = list(word_to_index.keys())[top_n_indices[i].item()]\n",
    "        similarity = top_n_similarities[i].item()\n",
    "        closest_words.append((similar_word, similarity))\n",
    "\n",
    "    return closest_words\n",
    "    # END TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:48:33.333237600Z",
     "start_time": "2024-09-26T13:48:32.099368900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('morning', 0.9999999403953552), ('afternoon', 0.8665475249290466), ('evening', 0.7880069613456726), ('yesterday', 0.7614548802375793), ('sunday', 0.7548925280570984)]\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "\n",
    "print(n_closest_vect(embeddings, word_to_index, embeddings[word_to_index['morning']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue :\n",
    "```\n",
    "[('morning', 1.0), ('afternoon', 0.8665473461151123), ('evening', 0.7880070209503174), ('yesterday', 0.7614548206329346), ('sunday', 0.7548925876617432)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Quelle est la similarité cosinus entre 'fast' et 'slow' ? Entre 'fast' et 'rapid' ? Commentez les résultats et expliquez leur origine. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:48:22.133479200Z",
     "start_time": "2024-09-26T13:48:22.088810800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.6445]])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "cosine_similarity(embeddings[word_to_index['fast']].unsqueeze(0) , embeddings[word_to_index['slow']].unsqueeze(0))\n",
    "\n",
    "cosine_similarity(embeddings[word_to_index['fast']].unsqueeze(0) , embeddings[word_to_index['rapid']].unsqueeze(0)) \n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Évaluation (12 Points)\n",
    "\n",
    "#### Données\n",
    "\n",
    "Les cellules qui suivent permettent de télécharger les données et de se restreindre au vocabulaire qui nous sera utile, pour éviter de charger des plongements inutiles.\n",
    "\n",
    "Comme décrit dans l'introduction, nous avons 3 fichiers de données:\n",
    "- Des paires de synonymes pour l'entraînement (`train_synonyms.txt`) \n",
    "- Des paires d'antonymes pour l'entraînement (`train_antonymms.txt`)\n",
    "- Des paires de mots avec leur similarité pour l'évaluation (`simlex_english.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:50:06.968052Z",
     "start_time": "2024-09-26T13:50:06.946233600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Télécharge les paires de synonymes et antonymes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pair = line.strip().split()\n",
    "            assert pair[0].startswith('en_') and pair[1].startswith('en_')\n",
    "            data.append((pair[0][3:], pair[1][3:]))\n",
    "    return data\n",
    "\n",
    "def data_to_tensor(data, word_to_index):\n",
    "    indices = [word_to_index[word] for pair in data for word in pair if word in word_to_index]\n",
    "    return torch.tensor(indices).view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T13:50:44.867838300Z",
     "start_time": "2024-09-26T13:50:19.376316700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Données d'entraînement\n",
    "train_synonyms = load_data(train_synonyms_path)\n",
    "train_antonyms = load_data(train_antonyms_path)\n",
    "\n",
    "# Données d'évaluation\n",
    "evaluation_simlex = pd.read_csv(eval_simlex, sep='\\t') # pd dataframe with columns 'word 1', 'word 2', 'score'\n",
    "\n",
    "# On se restreint au vocabulaire qu'on va utiliser pour éviter de charger des embeddings inutiles\n",
    "vocab = set([word for pair in train_synonyms + train_antonyms for word in pair])\n",
    "eval_vocab = set(evaluation_simlex['word 1']).union(set(evaluation_simlex['word 2']))\n",
    "vocab.update(eval_vocab)\n",
    "\n",
    "glove_word_to_index, glove_embeddings = load_word_vectors(pretrained_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "     word 1       word 2     score\n0       old          new  0.000000\n1     smart  intelligent  9.769231\n2      hard    difficult  9.692308\n3     happy     cheerful  9.307692\n4      hard         easy  0.000000\n..      ...          ...       ...\n994    join      acquire  2.153846\n995    send       attend  2.076923\n996  gather       attend  2.846154\n997  absorb     withdraw  1.076923\n998  attend       arrive  5.307692\n\n[999 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word 1</th>\n      <th>word 2</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>old</td>\n      <td>new</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>smart</td>\n      <td>intelligent</td>\n      <td>9.769231</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hard</td>\n      <td>difficult</td>\n      <td>9.692308</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy</td>\n      <td>cheerful</td>\n      <td>9.307692</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hard</td>\n      <td>easy</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>join</td>\n      <td>acquire</td>\n      <td>2.153846</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>send</td>\n      <td>attend</td>\n      <td>2.076923</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>gather</td>\n      <td>attend</td>\n      <td>2.846154</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>absorb</td>\n      <td>withdraw</td>\n      <td>1.076923</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>attend</td>\n      <td>arrive</td>\n      <td>5.307692</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_simlex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T14:01:38.626487200Z",
     "start_time": "2024-09-26T14:01:38.602368800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Observation du jeu de données SimLex-999. (2 points)\n",
    "\n",
    "Affichez l'histogramme de fréquence des scores de similarité dans le jeu de données SimLex-999. Utilisez `bins=40`. Votre axe des x doit représenter le score de similarité et votre axe des y doit représenter la fréquence. Comment interpréter un score de 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T14:05:24.061558200Z",
     "start_time": "2024-09-26T14:05:23.937147500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(evaluation_simlex['score'], bins=40, edgecolor='black')\n",
    "plt.xlabel('score')\n",
    "plt.ylabel('frequence')\n",
    "plt.title('fréquence des scores de similarité dans le jeu de données SimLex-999')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Corrélation de Spearman\n",
    "\n",
    "\n",
    "La corrélation de Spearman est une mesure de la relation monotone entre deux variables $x$ et $y$. Elle est comprise entre -1 et 1. Plutôt que de comparer les valeurs brutes des variables, elle compare leurs rangs. Elle est calculée comme suit :\n",
    "$$\\rho (x, y) = 1 - \\frac{6 \\sum_{i=1}^n (r_{x_i} - r_{y_i})^2}{n(n^2 - 1)}$$\n",
    "où $r_{x_i}$ est le rang de la i-ème valeur de la variable x, $r_{y_i}$ est le rang de la i-ème valeur de la variable y, $n$ est le nombre total de paires d'observations $(x, y)$.\n",
    "\n",
    "Les rangs sont attribués en ordonnant les valeurs de chaque variable du plus petit au plus grand. La plus petite valeur reçoit le rang 1, la suivante le rang 2, et ainsi de suite.\n",
    "\n",
    "##### Expliquez pourquoi on utilise la corrélation de Spearman plutôt que la corrélation entre les valeurs des variables. (3 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ici on s'intéresse au rang des variables. La relation de Spearson s'intéresse à la relation linéaire entre les valeurs et non pas au rang. Ici nos données ne sont pas linéaires."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Implémentation de la corrélation de Spearman (2 points)\n",
    "\n",
    "Complétez la fonction suivante pour calculer la corrélation de Spearman entre deux listes de valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T14:57:50.200352200Z",
     "start_time": "2024-09-26T14:57:50.168613800Z"
    }
   },
   "outputs": [],
   "source": [
    "def spearman_rank_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calcule la corrélation de Spearman entre deux listes de valeurs.\n",
    "\n",
    "    Args:\n",
    "        x   : list of float\n",
    "        y   : list of float\n",
    "\n",
    "    Returns:\n",
    "    La corrélation de Spearman entre les deux listes (float). \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    x_df = pd.DataFrame(x, columns=['value'])\n",
    "    x_df['rank'] = x_df['value'].rank()\n",
    "   \n",
    "    \n",
    "    y_df = pd.DataFrame(y, columns=['value'])\n",
    "    y_df['rank'] = y_df['value'].rank()\n",
    "    \n",
    "    sum = 0\n",
    "    n  = len(x_df)\n",
    "    for i in range(n) : \n",
    "        sum += (x_df.iloc[i]['rank'] - y_df.iloc[i]['rank'])**2\n",
    "        \n",
    "    P = 1 - 6*(sum)/(n * (n**2 -1))\n",
    "    \n",
    "    return P\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    # END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Évaluation du modèle GloVe (2 points)\n",
    "\n",
    "\n",
    "Retrouvez les plongements du modèle GloVe de tous les mots du jeu de données SimLex-999, puis calculez la similarité cosinus entre les paires.\n",
    "\n",
    "Calculez ensuite la corrélation de Spearman entre les scores de simlex et les similarités cosinus obtenues et affichez-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T14:54:02.379704300Z",
     "start_time": "2024-09-26T14:53:36.485611800Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "glove_word_to_index_simlex, glove_embeddings_simlex = load_word_vectors(pretrained_model_path, eval_vocab)\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 1056784 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[105], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m dist_bet_pairs \u001B[38;5;241m=\u001B[39m cosine_similarity(glove_embeddings_simlex , glove_embeddings_simlex)\u001B[38;5;241m.\u001B[39mitem()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a Tensor with 1056784 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "dist_bet_pairs = cosine_similarity(glove_embeddings_simlex , glove_embeddings_simlex)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T14:56:21.067697200Z",
     "start_time": "2024-09-26T14:56:21.020547200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1028, 1028])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_bet_pairs.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T14:33:40.678278600Z",
     "start_time": "2024-09-26T14:33:40.674206800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "     word 1       word 2     score\n0       old          new  0.000000\n1     smart  intelligent  9.769231\n2      hard    difficult  9.692308\n3     happy     cheerful  9.307692\n4      hard         easy  0.000000\n..      ...          ...       ...\n994    join      acquire  2.153846\n995    send       attend  2.076923\n996  gather       attend  2.846154\n997  absorb     withdraw  1.076923\n998  attend       arrive  5.307692\n\n[999 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word 1</th>\n      <th>word 2</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>old</td>\n      <td>new</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>smart</td>\n      <td>intelligent</td>\n      <td>9.769231</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>hard</td>\n      <td>difficult</td>\n      <td>9.692308</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>happy</td>\n      <td>cheerful</td>\n      <td>9.307692</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>hard</td>\n      <td>easy</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>join</td>\n      <td>acquire</td>\n      <td>2.153846</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>send</td>\n      <td>attend</td>\n      <td>2.076923</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>gather</td>\n      <td>attend</td>\n      <td>2.846154</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>absorb</td>\n      <td>withdraw</td>\n      <td>1.076923</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>attend</td>\n      <td>arrive</td>\n      <td>5.307692</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_simlex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T14:40:54.445387500Z",
     "start_time": "2024-09-26T14:40:54.443941Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5698103904724121, 0.727012574672699, 0.6683909893035889, 0.5995391607284546, 0.5784835815429688, 0.6445335149765015, 0.73598313331604, 0.8209696412086487, 0.9024451971054077, 0.8554916381835938, 0.6206319332122803, 0.7508782744407654, 0.6711384057998657, 0.5698103904724121, 0.516336977481842, 0.5090340971946716, 0.7747339010238647, 0.5425071716308594, 0.7840446829795837, 0.6912461519241333, 0.5298988223075867, 0.47525885701179504, 0.6119118332862854, 0.5906553864479065, 0.8203239440917969, 0.8137556314468384, 0.5399446487426758, 0.5988856554031372, 0.7970001101493835, 0.42520156502723694, 0.7887670993804932, 0.6948328018188477, 0.5229363441467285, 0.4871543049812317, 0.8073856830596924, 0.37095150351524353, 0.6548494100570679, 0.7951963543891907, 0.4510735273361206, 0.8096188306808472, 0.5933364629745483, 0.21913567185401917, 0.7128517627716064, 0.2711713910102844, 0.7108349204063416, 0.4511776268482208, 0.5022874474525452, 0.5827231407165527, 0.3571168780326843, 0.44359561800956726, 0.6099243760108948, 0.6225377917289734, 0.7551267743110657, 0.4845297336578369, 0.4959017038345337, 0.37405925989151, 0.5749468207359314, 0.5464540123939514, 0.6755222678184509, 0.7081608176231384, 0.6054233312606812, 0.44486042857170105, 0.8062575459480286, 0.5607190132141113, 0.6233803033828735, 0.5551077723503113, 0.4828813374042511, 0.21665985882282257, 0.4831654131412506, 0.5327391624450684, 0.30471980571746826, 0.7067415118217468, 0.5252300500869751, 0.44815656542778015, 0.5543118715286255, 0.3534806966781616, 0.3440539240837097, 0.5114157795906067, 0.52767014503479, 0.5259121060371399, 0.6090880632400513, 0.3757389187812805, 0.21670080721378326, 0.45535197854042053, 0.5123880505561829, 0.7913950085639954, 0.589786171913147, 0.6644142270088196, 0.5182815790176392, 0.6524955630302429, 0.5662515163421631, 0.3563988208770752, 0.6095772385597229, 0.6518635153770447, 0.5480940937995911, 0.31692880392074585, 0.36082467436790466, 0.5275941491127014, 0.37704890966415405, 0.6058970093727112, 0.37134069204330444, 0.2760348320007324, 0.2760348320007324, 0.22575537860393524, 0.18221831321716309, 0.35241618752479553, 0.3134876489639282, 0.4428175389766693, 0.3936574161052704, 0.4335002899169922, 0.26757994294166565, 0.8860776424407959, 0.5084917545318604, 0.7467526793479919, 0.7463729977607727, 0.8924477100372314, 0.5014128684997559, 0.7399045825004578, 0.5849600434303284, 0.7016151547431946, 0.6909394860267639, 0.5237013101577759, 0.5308322906494141, 0.6614281535148621, 0.6124699711799622, 0.8047990798950195, 0.46441224217414856, 0.7831171154975891, 0.6755290031433105, 0.7633395791053772, 0.538856565952301, 0.6023029685020447, 0.44383734464645386, 0.7885448336601257, 0.6963054537773132, 0.650964617729187, 0.5893797874450684, 0.5448446273803711, 0.825413703918457, 0.6226545572280884, 0.6421647667884827, 0.648906946182251, 0.8416598439216614, 0.5599494576454163, 0.48005175590515137, 0.7248430848121643, 0.5782224535942078, 0.6933411955833435, 0.6238389015197754, 0.5631623864173889, 0.6167535781860352, 0.4585356116294861, 0.5553464889526367, 0.5731930732727051, 0.5331240892410278, 0.5303251147270203, 0.4995400011539459, 0.7908806800842285, 0.5819259881973267, 0.5673884153366089, 0.4738737642765045, 0.6288910508155823, 0.5407217741012573, 0.6398143768310547, 0.46146899461746216, 0.5115998983383179, 0.4757763743400574, 0.5647113919258118, 0.51800137758255, 0.5093792080879211, 0.7719451785087585, 0.7425000071525574, 0.5081605315208435, 0.5929229259490967, 0.6235403418540955, 0.5081323981285095, 0.4630742371082306, 0.3923248052597046, 0.5330037474632263, 0.5308814644813538, 0.36119598150253296, 0.5014937520027161, 0.8024330139160156, 0.5302819609642029, 0.4305860102176666, 0.3786420226097107, 0.6789602041244507, 0.333799809217453, 0.5746267437934875, 0.7557224631309509, 0.5318316221237183, 0.6685578227043152, 0.34684690833091736, 0.5520986318588257, 0.5697309374809265, 0.7048037052154541, 0.6349957585334778, 0.6115368008613586, 0.3829953074455261, 0.5057663321495056, 0.6933714151382446, 0.5756979584693909, 0.6302708387374878, 0.6846116781234741, 0.49598729610443115, 0.5414823889732361, 0.7222983837127686, 0.43406710028648376, 0.2465408444404602, 0.6551245450973511, 0.5629791617393494, 0.654604971408844, 0.5572206974029541, 0.36284735798835754, 0.5305774807929993, 0.31919145584106445, 0.7713850736618042, 0.5740607976913452, 0.3285014033317566, 0.4795795977115631, 0.5306125283241272, 0.619629442691803, 0.4669944941997528, 0.2995874583721161, 0.4488436281681061, 0.5551514029502869, 0.4159589111804962, 0.5296791791915894, 0.6845648288726807, 0.5631431341171265, 0.4105066657066345, 0.6414874196052551, 0.5600246787071228, 0.5103113055229187, 0.8893510699272156, 0.5364563465118408, 0.687543511390686, 0.5052431225776672, 0.6943013072013855, 0.5221660733222961, 0.5899982452392578, 0.3667300045490265, 0.3905836045742035, 0.5015714764595032, 0.6554409265518188, 0.7703321576118469, 0.4591533839702606, 0.5347314476966858, 0.4722132682800293, 0.6319149136543274, 0.6093202829360962, 0.49526649713516235, 0.49235859513282776, 0.3628048896789551, 0.4504214823246002, 0.5851925611495972, 0.7330309748649597, 0.5003619194030762, 0.5396956205368042, 0.6170265674591064, 0.4120490252971649, 0.4876396358013153, 0.37119677662849426, 0.678205668926239, 0.4685532748699188, 0.5500031113624573, 0.6439514756202698, 0.5327097177505493, 0.4592970609664917, 0.496578186750412, 0.4535636603832245, 0.7567671537399292, 0.4047733247280121, 0.463142454624176, 0.4413747191429138, 0.5190854668617249, 0.3542253077030182, 0.5128312110900879, 0.6382737159729004, 0.4880050718784332, 0.47073638439178467, 0.40381142497062683, 0.5491122603416443, 0.6468586325645447, 0.5164492130279541, 0.6778017282485962, 0.5398193001747131, 0.4253343641757965, 0.587798535823822, 0.522774338722229, 0.36384105682373047, 0.6210880279541016, 0.3585713505744934, 0.6304234266281128, 0.5304591655731201, 0.5718713402748108, 0.596066951751709, 0.6552092432975769, 0.4793020486831665, 0.5108091831207275, 0.5560415983200073, 0.5645482540130615, 0.5704888701438904, 0.5745067596435547, 0.29936882853507996, 0.5406395196914673, 0.4704878628253937, 0.4207405149936676, 0.43158599734306335, 0.40841615200042725, 0.593346893787384, 0.37218260765075684, 0.5061012506484985, 0.46368151903152466, 0.5069906115531921, 0.5406563878059387, 0.3876332938671112, 0.7112315893173218, 0.428689181804657, 0.45605745911598206, 0.460584431886673, 0.7331258058547974, 0.5246613025665283, 0.6089727282524109, 0.32810330390930176, 0.6146195530891418, 0.5015048980712891, 0.7933000326156616, 0.47246021032333374, 0.3529621958732605, 0.6144934296607971, 0.6276026964187622, 0.49838384985923767, 0.5821906328201294, 0.7953622341156006, 0.6081335544586182, 0.6668009161949158, 0.7394792437553406, 0.7319288849830627, 0.3948807120323181, 0.5325292348861694, 0.44792646169662476, 0.5901069045066833, 0.5946840047836304, 0.45068272948265076, 0.5420559048652649, 0.46700501441955566, 0.772649347782135, 0.5883399248123169, 0.694058895111084, 0.5546398758888245, 0.5082932114601135, 0.620255172252655, 0.5185343027114868, 0.4607061743736267, 0.43548834323883057, 0.5703948736190796, 0.601351797580719, 0.49683550000190735, 0.7370194792747498, 0.5986197590827942, 0.4431878328323364, 0.37957486510276794, 0.6998939514160156, 0.43530938029289246, 0.5146451592445374, 0.398485004901886, 0.6318384408950806, 0.5177642107009888, 0.34833094477653503, 0.7277618646621704, 0.6603135466575623, 0.6962313652038574, 0.5730026960372925, 0.4039645493030548, 0.6127306818962097, 0.5997243523597717, 0.5642287731170654, 0.6871917247772217, 0.3713913857936859, 0.48482444882392883, 0.5620752573013306, 0.4716431498527527, 0.29965755343437195, 0.5545894503593445, 0.48639482259750366, 0.41032087802886963, 0.5834973454475403, 0.4603269696235657, 0.4490346610546112, 0.25523215532302856, 0.49717214703559875, 0.48864272236824036, 0.5176706910133362, 0.7236287593841553, 0.5896639823913574, 0.5026624202728271, 0.5862447619438171, 0.5772139430046082, 0.42634257674217224, 0.4230995774269104, 0.5814241766929626, 0.2506062388420105, 0.5911007523536682, 0.49990665912628174, 0.43884050846099854, 0.6121928095817566, 0.5673593878746033, 0.7952331900596619, 0.4064289927482605, 0.7847972512245178, 0.7430669069290161, 0.5209623575210571, 0.4146920442581177, 0.5321387052536011, 0.5222989916801453, 0.3282773494720459, 0.6500040292739868, 0.33390429615974426, 0.35774460434913635, 0.5162400603294373, 0.4673433005809784, 0.4891834855079651, 0.7162214517593384, 0.6706320643424988, 0.7102348208427429, 0.4061259627342224, 0.5025500655174255, 0.4045230746269226, 0.639209508895874, 0.7100140452384949, 0.49807867407798767, 0.31574517488479614, 0.456208735704422, 0.3683088421821594, 0.5033511519432068, 0.6301157474517822, 0.4071475863456726, 0.46689024567604065, 0.35938403010368347, 0.2678239643573761, 0.42056208848953247, 0.5008290410041809, 0.3854740262031555, 0.5518812537193298, 0.7284512519836426, 0.422428160905838, 0.6489716172218323, 0.5579293966293335, 0.3790975511074066, 0.40157952904701233, 0.6370565295219421, 0.33092066645622253, 0.5921047329902649, 0.4327014088630676, 0.3889663517475128, 0.5210733413696289, 0.35051608085632324, 0.43286558985710144, 0.4327889680862427, 0.6627663373947144, 0.4479548931121826, 0.4650926887989044, 0.7812541723251343, 0.5928946137428284, 0.36630696058273315, 0.4084495007991791, 0.6347312331199646, 0.32832369208335876, 0.4950166940689087, 0.553440511226654, 0.640233039855957, 0.35093238949775696, 0.6015846729278564, 0.5423867702484131, 0.5530127286911011, 0.579521656036377, 0.6134617924690247, 0.6065331697463989, 0.45408937335014343, 0.39562100172042847, 0.3896579444408417, 0.2777974307537079, 0.465108186006546, 0.4073440432548523, 0.7100696563720703, 0.3832088112831116, 0.35225728154182434, 0.20944276452064514, 0.30744314193725586, 0.6129096150398254, 0.5158922076225281, 0.15845820307731628, 0.5382958650588989, 0.6104695796966553, 0.591483473777771, 0.5352595448493958, 0.49327564239501953, 0.4100392460823059, 0.5762959718704224, 0.4165668189525604, 0.5989829897880554, 0.6098549962043762, 0.5275240540504456, 0.43067020177841187, 0.5504026412963867, 0.386618971824646, 0.4764010012149811, 0.5029531717300415, 0.5118352770805359, 0.37654709815979004, 0.40167495608329773, 0.540440559387207, 0.47261348366737366, 0.5047882795333862, 0.35353487730026245, 0.4022957384586334, 0.6740297079086304, 0.45728883147239685, 0.524117648601532, 0.4661867916584015, 0.2240946888923645, 0.6141514778137207, 0.477691113948822, 0.601560115814209, 0.573642373085022, 0.4405827224254608, 0.27465468645095825, 0.48607441782951355, 0.4475133419036865, 0.4592902660369873, 0.42245620489120483, 0.37943023443222046, 0.38164621591567993, 0.46693965792655945, 0.5017666220664978, 0.3649695813655853, 0.5591953992843628, 0.5235958695411682, 0.7771568298339844, 0.5352949500083923, 0.5293918251991272, 0.5145191550254822, 0.5106157064437866, 0.6921433210372925, 0.5667421817779541, 0.49634578824043274, 0.6462588906288147, 0.3731172978878021, 0.5206847786903381, 0.2582801282405853, 0.33615171909332275, 0.5029022097587585, 0.5144504904747009, 0.5243740677833557, 0.4438193142414093, 0.3644733428955078, 0.4455399215221405, 0.45720210671424866, 0.5087554454803467, 0.4057818353176117, 0.3264061212539673, 0.5697841048240662, 0.6009829044342041, 0.42010894417762756, 0.7833920121192932, 0.4056851267814636, 0.5347567200660706, 0.4252130091190338, 0.47065049409866333, 0.35418006777763367, 0.5077667832374573, 0.5019539594650269, 0.5417627096176147, 0.6026679277420044, 0.7038567066192627, 0.5546749830245972, 0.3929518461227417, 0.5431464910507202, 0.6549273729324341, 0.4691909849643707, 0.4693252444267273, 0.6144941449165344, 0.5077950358390808, 0.5004269480705261, 0.5490753054618835, 0.5877417922019958, 0.48002392053604126, 0.5224969983100891, 0.44454342126846313, 0.43369412422180176, 0.41781800985336304, 0.6603121757507324, 0.44823917746543884, 0.5302573442459106, 0.6433523297309875, 0.4667623043060303, 0.4816199541091919, 0.5276675820350647, 0.5318849682807922, 0.1602807492017746, 0.5017417073249817, 0.4302397072315216, 0.32396936416625977, 0.3718755841255188, 0.42825424671173096, 0.7528560757637024, 0.5578677654266357, 0.4622105360031128, 0.5676888227462769, 0.6003634929656982, 0.47164440155029297, 0.4725389778614044, 0.3992958664894104, 0.4650586247444153, 0.5883269906044006, 0.5414631962776184, 0.45354241132736206, 0.608516275882721, 0.5570398569107056, 0.48173511028289795, 0.4650583863258362, 0.3512679636478424, 0.33822792768478394, 0.5912696719169617, 0.3712998032569885, 0.4306272268295288, 0.5690475106239319, 0.5343692302703857, 0.6596940159797668, 0.40031155943870544, 0.3425290882587433, 0.3474716246128082, 0.6387829184532166, 0.3229880928993225, 0.5507723689079285, 0.4677835702896118, 0.37863045930862427, 0.6875914931297302, 0.3124349117279053, 0.7064222097396851, 0.578589916229248, 0.5845455527305603, 0.4010956585407257, 0.6502222418785095, 0.34621864557266235, 0.5830002427101135, 0.29856759309768677, 0.4292570650577545, 0.6226570010185242, 0.5098105072975159, 0.3884899616241455, 0.5121651291847229, 0.42971983551979065, 0.583019495010376, 0.33956876397132874, 0.5179699063301086, 0.5332645773887634, 0.5291706323623657, 0.3842281699180603, 0.7064282894134521, 0.5241364240646362, 0.45314842462539673, 0.3108427822589874, 0.5039986371994019, 0.34756791591644287, 0.5769644975662231, 0.5620061159133911, 0.6416803002357483, 0.5454021692276001, 0.5161647796630859, 0.4926346242427826, 0.5600935816764832, 0.5276012420654297, 0.42009592056274414, 0.6207825541496277, 0.3803057372570038, 0.5389760732650757, 0.4438779652118683, 0.6177598237991333, 0.5515261292457581, 0.5155285000801086, 0.5850184559822083, 0.436827152967453, 0.5669692754745483, 0.4458726644515991, 0.6214789152145386, 0.5987809896469116, 0.45544397830963135, 0.5969106554985046, 0.4921591579914093, 0.5828201174736023, 0.46481722593307495, 0.5451325178146362, 0.5244507789611816, 0.6326334476470947, 0.4711548388004303, 0.6278932690620422, 0.43708130717277527, 0.42267581820487976, 0.39888861775398254, 0.471177875995636, 0.5038559436798096, 0.5439919233322144, 0.43519127368927, 0.5988540649414062, 0.4029673635959625, 0.40330371260643005, 0.6695096492767334, 0.4190714657306671, 0.4846874475479126, 0.1712692230939865, 0.3222651183605194, 0.18689610064029694, 0.28077250719070435, 0.21938635408878326, 0.3957550823688507, 0.28233981132507324, 0.240385964512825, 0.14827309548854828, 0.1424264758825302, 0.2693231999874115, 0.3226580023765564, 0.07181941717863083, 0.19180957973003387, 0.1336028277873993, 0.17203572392463684, 0.34488344192504883, 0.2550206780433655, 0.31796208024024963, 0.3402485251426697, 0.2522757351398468, 0.2388453185558319, 0.20305217802524567, 0.1867804080247879, 0.2929445505142212, 0.34916195273399353, 0.17348432540893555, 0.38439711928367615, 0.2152026891708374, 0.30458545684814453, 0.24553875625133514, 0.2804884910583496, 0.2749972939491272, 0.025422777980566025, 0.19617488980293274, 0.16378425061702728, 0.3622971177101135, 0.14743337035179138, 0.26592451333999634, 0.4390771687030792, 0.2031525820493698, 0.30325889587402344, 0.2586539089679718, 0.25443992018699646, 0.190873920917511, 0.22252701222896576, 0.1771487593650818, 0.35556760430336, 0.22935880720615387, 0.22680456936359406, 0.21579107642173767, 0.1556524783372879, 0.2840445637702942, 0.41180387139320374, 0.19674861431121826, 0.18110543489456177, 0.4020983576774597, 0.18146438896656036, 0.2647842466831207, 0.24169117212295532, 0.4092312157154083, 0.23874327540397644, 0.20421436429023743, 0.2309640794992447, 0.26676636934280396, 0.25003066658973694, 0.8086636662483215, 0.516034722328186, 0.8040581941604614, 0.27509981393814087, 0.743145227432251, 0.7971075177192688, 0.4764535129070282, 0.5028132200241089, 0.3076692819595337, 0.6688909530639648, 0.7180474996566772, 0.764622151851654, 0.7221657633781433, 0.544675350189209, 0.6665040254592896, 0.5907236933708191, 0.4679853022098541, 0.5845524668693542, 0.7410718202590942, 0.7748222947120667, 0.47676098346710205, 0.3886396288871765, 0.6915093064308167, 0.6646197438240051, 0.3575112819671631, 0.5716633200645447, 0.6364084482192993, 0.49917393922805786, 0.4535103738307953, 0.6577101945877075, 0.5039082169532776, 0.6895980834960938, 0.5184070467948914, 0.47804704308509827, 0.5238831639289856, 0.6124699711799622, 0.8498678803443909, 0.6014330983161926, 0.6961919665336609, 0.7680352330207825, 0.6424919962882996, 0.4105624258518219, 0.7127328515052795, 0.5854533314704895, 0.5648213028907776, 0.6984221935272217, 0.5384684205055237, 0.8545225858688354, 0.518459141254425, 0.5267030596733093, 0.624476969242096, 0.37899887561798096, 0.38125503063201904, 0.5796030759811401, 0.529747724533081, 0.6343526244163513, 0.6258148550987244, 0.45853734016418457, 0.675413966178894, 0.44201675057411194, 0.37675032019615173, 0.39527085423469543, 0.5299990773200989, 0.6198503971099854, 0.6193423271179199, 0.6060981750488281, 0.6515894532203674, 0.661106288433075, 0.6635552644729614, 0.3408508002758026, 0.556526243686676, 0.5850139260292053, 0.5599333047866821, 0.7149484157562256, 0.8128945827484131, 0.45071589946746826, 0.6353799700737, 0.5078884959220886, 0.12285687029361725, 0.619644284248352, 0.4723403751850128, 0.6070289015769958, 0.41865500807762146, 0.6857641339302063, 0.43402671813964844, 0.40979495644569397, 0.7909738421440125, 0.5665233135223389, 0.6951197981834412, 0.335725873708725, 0.49783989787101746, 0.5310827493667603, 0.7756375670433044, 0.753437340259552, 0.43655627965927124, 0.5004914999008179, 0.5893052816390991, 0.4013008773326874, 0.356627494096756, 0.43971797823905945, 0.4222446084022522, 0.4660967290401459, 0.5801963210105896, 0.5112690925598145, 0.3205316960811615, 0.47871705889701843, 0.4466993510723114, 0.7847238183021545, 0.49353769421577454, 0.42376014590263367, 0.71651691198349, 0.38877466320991516, 0.7086017727851868, 0.5786185264587402, 0.27579203248023987, 0.6518267393112183, 0.37129995226860046, 0.36405205726623535, 0.6294808983802795, 0.34265032410621643, 0.5370410680770874, 0.5519199371337891, 0.6554664969444275, 0.684738278388977, 0.3218400776386261, 0.6288779377937317, 0.8021336793899536, 0.6043970584869385, 0.6020244359970093, 0.5400412678718567, 0.6581811904907227, 0.6256312131881714, 0.5912657380104065, 0.5517669320106506, 0.581110954284668, 0.7233281135559082, 0.5092807412147522, 0.7385432720184326, 0.49824076890945435, 0.6506989002227783, 0.35518747568130493, 0.5747654438018799, 0.287900447845459, 0.4007295072078705, 0.48722130060195923, 0.3973024785518646, 0.48238348960876465, 0.4609729051589966, 0.7641974091529846, 0.692785918712616, 0.490202933549881, 0.7867919206619263, 0.6343302726745605, 0.5049978494644165, 0.4020572006702423, 0.5317096710205078, 0.5861082077026367, 0.4314805567264557, 0.542511522769928, 0.48327580094337463, 0.5235581994056702, 0.42461249232292175, 0.5436801910400391, 0.33223849534988403, 0.4148479998111725, 0.6563604474067688, 0.5533682107925415, 0.49225929379463196, 0.41307663917541504, 0.6537598967552185, 0.33810290694236755, 0.5751402378082275, 0.41830307245254517, 0.41200730204582214, 0.756436824798584, 0.6595296263694763, 0.7286691069602966, 0.621269702911377, 0.5322365760803223, 0.42521849274635315, 0.42947641015052795, 0.7398533225059509, 0.7259873151779175, 0.6148221492767334, 0.40437933802604675, 0.49889832735061646, 0.5381962060928345, 0.49264654517173767, 0.6330483555793762, 0.6593418717384338, 0.4530178904533386, 0.5615534782409668, 0.5084198117256165, 0.41113364696502686, 0.4575125277042389, 0.2794962525367737, 0.5842418074607849, 0.588630735874176, 0.36360442638397217, 0.5485150218009949, 0.49138495326042175, 0.48588722944259644, 0.377576619386673, 0.334076464176178, 0.36429643630981445, 0.7727162837982178, 0.3819703459739685, 0.3531891107559204, 0.4823713004589081, 0.48048481345176697, 0.5420944690704346, 0.3503749966621399, 0.43879643082618713, 0.23780812323093414, 0.05669925734400749, 0.3612063229084015, 0.37836354970932007, 0.35072562098503113, 0.45977020263671875, 0.5277411341667175, 0.2634493410587311, 0.5152243375778198]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.2916424109480221"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "list = []\n",
    "for row in evaluation_simlex.iterrows() : \n",
    "    #print(row[1]['word 1'])\n",
    "    index_1 = glove_word_to_index_simlex[row[1]['word 1']]\n",
    "    index_2 = glove_word_to_index_simlex[row[1]['word 2']]\n",
    "    list.append(dist_bet_pairs[index_1][index_2])\n",
    "    \n",
    "    \n",
    "list = [t.item() for t in list]\n",
    "print(list)\n",
    "    \n",
    "\n",
    "    \n",
    "spearman_rank_correlation(evaluation_simlex['score'].tolist() , list)\n",
    "    \n",
    "#dist_bet_pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-26T15:00:26.397548900Z",
     "start_time": "2024-09-26T15:00:26.314564100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat attendu: 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Interprétation du résultat (3 points)\n",
    "\n",
    "Qu'est ce que ce nombre représente et que peut-on en conclure sur la qualité des plongements GloVe (2 conclusions) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ce nombre représente le niveau de similarité entre Glove et Simlex mais que au niveau des rangs. Les plongements de Glove et de Simlex donnent des résultats différents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Description de la méthode contrastive (33 Points)\n",
    "\n",
    "Nous allons maintenant implémenter une méthode contrastive de plongements de mots. Elle vise à améliorer les plongements lexicaux de mots en tenant compte des synonymes et antonymes. \n",
    "\n",
    "Notre modèle se basera simplement sur une matrice de plongements de mots, qui associe à chaque mot un vecteur de plongement.\n",
    "\n",
    "L'idée est d'entraîner ce modèle à rapprocher les plongements de synonymes et d'éloigner ceux d'antonymes.\n",
    "\n",
    "La cellule suivante définit le modèle et ses attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T15:07:29.151826500Z",
     "start_time": "2024-09-26T15:07:29.146570100Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveWordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, embeddings, device='cpu', margin_plus=0.6, margin_minus=0., regularization=1e-9):\n",
    "        super(ContrastiveWordEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Hyperparamètres pour les fonctions de coût\n",
    "        self.margin_plus = margin_plus\n",
    "        self.margin_minus = margin_minus\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        # Initialisation des plongements de mots\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings.detach().clone(), freeze=False).to(self.device)\n",
    "        self.original_embeddings = nn.Embedding.from_pretrained(embeddings.detach().clone(), freeze=True).to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Création des négatifs\n",
    "\n",
    "Pendant l'entraînement, au lieu de traiter tout le jeu d'entraînement d'un coup, nous allons avoir des lots (batchs) de paires de synonymes $B_S$ et d'antonymes $B_A$.\n",
    "\n",
    "Dans un lot de synonymes, on définit le négatif d'un mot comme le mot du lot le plus proche qui n'est pas dans la même paire. Intuitivement, c'est le mot que le modèle devrait confondre le plus avec le synonyme. Similairement, dans un lot d'antonymes, on définit le négatif d'un mot comme le mot du lot le plus éloigné qui n'est pas dans la même paire.\n",
    "\n",
    "On répète ce processus pour chaque mot de chaque paire de synonymes et d'antonymes.\n",
    "\n",
    "*Attention, un mot peut apparaître plusieurs fois dans un lot avec des synonymes ou antonymes différents, et il ne peut être le négatif d'aucun de ses synonymes, ou antonymes.*\n",
    "\n",
    "##### 3.1.1 Exemple pour illustrer l'implémentation\n",
    "\n",
    "Prenons un exemple avec un lot $B_S$ de synonymes de taille 3. On veut construire le lot de négatifs $T_S$\n",
    "\n",
    "$B_S$:\n",
    "- (arbre, plante)\n",
    "- (voiture, véhicule)\n",
    "- (arbre, buisson)\n",
    "\n",
    "On a 5 mots uniques dans le lot: arbre, plante, voiture, véhicule, buisson. Supposons que la matrice de similarité cosinus soit la suivante :\n",
    "\n",
    "|       | arbre | plante | voiture | véhicule | buisson |\n",
    "|-------|-------|--------|--------|----------|---------|\n",
    "| arbre | 1     | 0.8    | 0.1    | 0.2      | 0.9     |\n",
    "| plante| 0.8   | 1      | 0.3    | 0.4      | 0.7     |\n",
    "| voiture| 0.1  | 0.3    | 1      | 0.9      | 0.2     |\n",
    "| véhicule| 0.2 | 0.4    | 0.9    | 1        | 0.3     |\n",
    "| buisson| 0.9  | 0.7    | 0.2    | 0.3      | 1       |\n",
    "\n",
    "\n",
    "On commence par calculer les voisins de chaque mot du lot $B_S$. Le voisin d'un mot $m$ est défini comme tout mot qui apparait dans au moins une paire avec $m$ dans $B_S$. Un mot est aussi considéré comme son propre voisin.\n",
    "\n",
    "- voisins de arbre : arbre, plante, buisson\n",
    "- voisins de plante : plante, arbre\n",
    "- voisins de voiture : voiture, véhicule\n",
    "- voisins de véhicule : véhicule, voiture\n",
    "- voisins de buisson : buisson, arbre\n",
    "\n",
    "Après avoir masqué les voisins, la matrice est :\n",
    "\n",
    "|       | arbre | plante | voiture | véhicule | buisson |\n",
    "|-------|-------|--------|--------|----------|---------|\n",
    "| arbre | -inf  | -inf   | 0.1    | 0.2      | -inf    |\n",
    "| plante| -inf  | -inf   | 0.3    | 0.4      | 0.7     |\n",
    "| voiture| 0.1  | 0.3    | -inf   | -inf     | 0.2     |\n",
    "| véhicule| 0.2 | 0.4    | -inf   | -inf     | 0.3     |\n",
    "| buisson| -inf | 0.7    | 0.2    | 0.3      | -inf    |\n",
    "\n",
    "Pour calculer les négatifs, on prend le maximum de chaque ligne (donc le mot le plus similaire qui n'est pas un voisin) :\n",
    "\n",
    "Ici,\n",
    "- le négatif d'arbre est véhicule\n",
    "- le négatif de plante est buisson\n",
    "- le négatif de voiture est plante\n",
    "- le négatif de véhicule est plante\n",
    "- le négatif de buisson est plante\n",
    "\n",
    "En reprenant le batch $B_S$:\n",
    "- (arbre, plante)\n",
    "- (voiture, véhicule)\n",
    "- (arbre, buisson)\n",
    "\n",
    "$T_S$ sera composé de paires composées du négatif de chaque élément de $B_S$ :\n",
    "\n",
    "$B_S$ -> $T_S$\n",
    "- (arbre, plante) $\\rightarrow$ (véhicule, buisson), car le négatif d'arbre est véhicule et le négatif de plante est buisson\n",
    "- (voiture, véhicule) $\\rightarrow$ (plante, plante), car le négatif de voiture est plante et le négatif de véhicule est plante\n",
    "- (arbre, buisson) $\\rightarrow$ (véhicule, plante), car le négatif d'arbre est véhicule et le négatif de buisson est plante\n",
    "\n",
    "$T_S$ sera donc : \n",
    "- (véhicule, buisson)\n",
    "- (plante, plante)\n",
    "- (véhicule, plante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Implémentez la fonction `prepare_neighbors` qui renvoit la liste des voisins de chaque mot dans le lot. (4 points)\n",
    "\n",
    "Les voisins d'un mot $m$ sont tous les mots du lot qui apparaissent dans au moins une paire avec $m$. Utilisez les bons indices (indice dans la matrice d'embeddings et indice dans le lot). Le résultat est une liste de liste de voisins, où `neighbors[i]` est la liste des voisins du mot `i` dans le lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T19:20:13.727669700Z",
     "start_time": "2024-09-26T19:20:13.713620100Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_neighbors(index_pairs, unique_idx, index_to_idx):\n",
    "    \"\"\"\n",
    "    Prépare les voisins pour chaque mot dans les paires de mots.\n",
    "    \n",
    "    Args : \n",
    "        index_pairs     : torch.Tensor de seconde dimension 2\n",
    "        Tensor contenant les indices des embeddings des mots dans le vocabulaire. \n",
    "        Des indices qui sont reliés ensemble par une ligne dans ce tenseur ont \n",
    "        une relation sémantique entre eux (synonymes ou antonymes).\n",
    "\n",
    "        unique_idx      : set\n",
    "        Ensemble de tous les indices qui sont mentionnés dans la liste `index_pairs`.\n",
    "        \n",
    "        index_to_idx    : dict\n",
    "        Dictionnaire associant un indice mentionné dans `index_pairs` à son indice dans\n",
    "        la liste qui sera retournée. Par exemple, si dans ce dictionnaire, la clé 4 est\n",
    "        associée à la valeur 12, cela veut dire que les voisins du mot 4 dans le vocabulaire\n",
    "        seront retournés à l'indice 12 dans la liste de retour.\n",
    "\n",
    "    Returns:\n",
    "    Une liste où chaque élément est une liste des indices des voisins pour chaque mot (l'indice dans la liste correspond à l'indice unique).\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    neighbors = [[] for _ in range(len(unique_idx))]\n",
    "    word_pairs = index_pairs.tolist()\n",
    "    #print(word_pairs)\n",
    "    for word1, word2 in word_pairs:\n",
    "        idx1 = index_to_idx[word1]\n",
    "        idx2 = index_to_idx[word2]\n",
    "        \n",
    "        \n",
    "        neighbors[idx1].append(idx1)\n",
    "        neighbors[idx1].append(idx2)\n",
    "        \n",
    "        neighbors[idx2].append(idx2)\n",
    "        neighbors[idx2].append(idx1)\n",
    "\n",
    "    return neighbors\n",
    "    \n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T19:20:14.577111500Z",
     "start_time": "2024-09-26T19:20:14.561388500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx1 0 idx2 2\n",
      "idx1 2 idx2 3\n",
      "idx1 4 idx2 1\n",
      "[[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "\n",
    "index_pairs = torch.tensor([[0, 12], [12, 31], [53, 4]])\n",
    "unique_idx = {0, 4, 12, 31, 53}\n",
    "index_to_idx = {0: 0, 4: 1, 12: 2, 31: 3, 53: 4}\n",
    "print(prepare_neighbors(index_pairs, unique_idx, index_to_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse attendue\n",
    "\n",
    "`[[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Implémentez la fonction `select_negatives` qui renvoit un dictionnaire qui associe à chaque élément son négatif. (4 points)\n",
    "\n",
    "Pour chaque élément du lot, on cherche le voisin le plus proche qui n'est pas le voisin de l'autre élément de la paire.\n",
    "\n",
    "Utilisez un masque pour cacher, dans la matrice de similarité, les voisins.\n",
    "\n",
    "La fonction utilise un paramètre `synonym` qui indique si on travaille sur un lot de synonymes ou d'antonymes. En cas de synonymes, on cherche le voisin le plus proche qui n'est pas un voisin de l'autre élément de la paire. En cas d'antonymes, on cherche le voisin le plus éloigné qui n'est pas un voisin de l'autre élément de la paire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_negatives(indices, similarity_matrix, neighbors, synonym=True):\n",
    "    \"\"\"\n",
    "    Sélectionne les exemples négatifs à partir de la matrice de similarité et des voisins.\n",
    "\n",
    "    Args : \n",
    "        indices             : torch.Tensor (vocab_size)\n",
    "        Indices des mots présents dans le vocabulaires\n",
    "        \n",
    "        similarity_matrix   : torch.Tensor (vocab_size, vocab_size)\n",
    "        Matrice de similarité entre tous les mots présents dans le vocabulaire.\n",
    "        \n",
    "        neighbors           : list of lists\n",
    "        Liste des voisins de chaque mot. Par exemple, le premier élément de la liste\n",
    "        contiendra tous les voisins du mot 0 dans le vocabulaire.\n",
    "\n",
    "        synonym             : bool, optional (default=True)\n",
    "        Indique si l'on cherche des négatifs pour les synonymes (True) ou pour les antonymes (False).\n",
    "\n",
    "    Returns:\n",
    "    Dictionnaire mappant les mots avec leurs indices de négatifs {mot_index: négatif_index}.\n",
    "    \"\"\"\n",
    "    # TODO HINT: Utilisez un mask pour ignorer les voisins\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "\n",
    "indices = torch.tensor([0, 1, 2, 3, 4])\n",
    "neighbors = [[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]\n",
    "\n",
    "similarity_matrix = torch.tensor([\n",
    "    [ 1.0000, -0.4263, -0.7167, -0.9838, -0.5823],\n",
    "    [-0.4263,  1.0000, -0.1600,  0.5088, -0.3708],\n",
    "    [-0.7167, -0.1600,  1.0000,  0.7247,  0.5631],\n",
    "    [-0.9838,  0.5088,  0.7247,  1.0000,  0.4394],\n",
    "    [-0.5823, -0.3708,  0.5631,  0.4394,  1.0000]\n",
    "    ])\n",
    "\n",
    "print(select_negatives(indices, similarity_matrix, neighbors, synonym=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse attendue\n",
    "\n",
    "`{0: 1, 1: 3, 2: 4, 3: 1, 4: 2}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4 Implémentez la fonction `run_negative_extraction` qui prépare les paires de synonymes et d'antonymes et appelle `prepare_neighbors` et `select_negatives`. (4 points)\n",
    "\n",
    "Préparez les indices uniques des mots du batch, calculez la similarité des mots, et appelez `prepare_neighbors` et `select_negatives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_negative_extraction(model, index_pairs, synonym=True):\n",
    "    \"\"\"\n",
    "    Extrait les exemples négatifs pour un ensemble de paires de mots.\n",
    "\n",
    "    Args :\n",
    "        index_pairs : torch.Tensor de seconde dimension 2\n",
    "        Contient les indices des mots.\n",
    "\n",
    "        synonym     : bool, optional (default=True)\n",
    "        Indique si l'on cherche des négatifs pour les synonymes (True) ou pour les antonymes (False).\n",
    "\n",
    "    Returns:\n",
    "    Dictionnaire mappant les indices des mots avec leurs indices de négatifs {mot_index: négatif_index}.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fonctions de coût\n",
    "\n",
    "Pour chaque paire de synonymes $(x^l, x^r)$ *pour x left et x right* dans le lot $B_S$, nous trouvons un négatif $(t^l, t^r)$ ce qui constitue le lot $T_S$:\n",
    "- $ t^l $ est le mot dans le lot le plus proche de $ x^l $ mais qui n'est pas $ x^r $.\n",
    "- $ t^r $ est le mot dans le lot le plus proche de $ x^r $ mais qui n'est pas $ x^l $.\n",
    "\n",
    "De même, pour chaque paire d'antonymes $(x^l, x^r)$ dans le lot $B_A$, nous trouvons un négatif $(t^l, t^r)$ ce qui constitue le lot $T_A$:\n",
    "- $ t^l $ est le mot dans le lot le plus éloigné de $ x^l $ mais qui n'est pas $ x^r $.\n",
    "- $ t^r $ est le mot dans le lot le plus éloigné de $ x^r $ mais qui n'est pas $ x^l $.\n",
    "\n",
    "\n",
    "Comparer un mot à son synonyme (ou antonyme) et à son négatif permet d'entraîner le modèle sur des exemples difficiles qui forcent le modèle à apprendre des représentations plus robustes.\n",
    "\n",
    "Il y aura trois fonctions de coût :\n",
    "1. **Attraction** : Attire les synonymes plus proches les uns des autres.\n",
    "2. **Répulsion** : Repousse les antonymes plus loin les uns des autres.\n",
    "3. **Régularisation** : Évite que les plongements ne s'éloignent trop de ceux du modèle pré-entraîné.\n",
    "\n",
    "Les fonctions de coût sont définies comme suit, en sommant sur `i`, les paires de synonymes et d'antonymes dans les lots $B_S$ et $B_A$ :\n",
    "\n",
    "1. **Attraction** :\n",
    "$$ S(B_S, T_S) = \\sum_{i=1}^{|B_S|} \\left[ \\max \\left(0, \\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r \\right) + \\max \\left( 0, \\delta_{syn} + x_i^r t_i^r - x_i^l x_i^r \\right) \\right] $$\n",
    "\n",
    "1. **Répulsion** :\n",
    "$$ A(B_A, T_A) = \\sum_{i=1}^{|B_A|} \\left[ \\max \\left(0, \\delta_{ant} + x_i^l x_i^r - x_i^l t_i^l \\right) + \\max \\left( 0, \\delta_{ant} + x_i^r x_i^l - x_i^r t_i^r \\right) \\right] $$\n",
    "\n",
    "1. **Régularisation** :\n",
    "$$ R(B_S, B_A) = \\sum_{x_i \\in V(B_S \\cup B_A)} \\lambda_{reg} \\| \\hat{x}_i - x_i \\|^2 $$\n",
    "\n",
    "La fonction de coût totale est la somme de ces trois termes :\n",
    "$$ C(B_S, T_S, B_A, T_A) = S(B_S, T_S) + A(B_A, T_A) + R(B_S, B_A) $$\n",
    "\n",
    "$\\delta_{syn}$, $\\delta_{ant}$ et $\\lambda_{reg}$ sont des hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec l'exemple précédent, prenons\n",
    "> + $x_i^l$ : voiture\n",
    "> + $x_i^r$: véhicule\n",
    "> + $t_i^l$: plante\n",
    "> \n",
    "> On veut que voiture et véhicule aient un plus grand produit scalaire que voiture et plante, donc que $\\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r <0$, et donc que $S$ soit minimisé. De même pour la deuxième partie de l'équation, symmétrique, avec le 2nd élément du couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Implémentez la fonction `synonym_cost` qui calcule la fonction de coût d'attraction (sur les paires de synonymes). (5 points)\n",
    "\n",
    "$$ S(B_S, T_S) = \\sum_{i=1}^{|B_S|} \\left[ \\max \\left(0, \\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r \\right) + \\max \\left( 0, \\delta_{syn} + x_i^r t_i^r - x_i^l x_i^r \\right) \\right] $$\n",
    "\n",
    "Le membre de gauche pénalise si le mot de gauche est plus éloigné de son négatif que de son synonyme. De même, le membre de droite pénalise si le mot de gauche est plus éloigné de son négatif que de son synonyme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_cost(model, synonym_pairs, synonym_negatives):\n",
    "    \"\"\"\n",
    "    Calcule le coût d'attraction pour les paires de synonymes.\n",
    "\n",
    "    synonym_pairs: liste de tuples d'indices de paires de synonymes\n",
    "    synonym_negatives: dictionnaire de mots avec leurs négatifs {mot_index: négatif_index}\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total pour les paires de synonymes\n",
    "    \"\"\"\n",
    "    # HINT: Utiliser torch.relu pour max(0, x)\n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Implémentez la fonction `antonym_cost` qui calcule la fonction de coût de répulsion (sur les paires d'antonymes). (5 points)\n",
    "\n",
    "$$ A(B_A, T_A) = \\sum_{i=1}^{|B_A|} \\left[ \\max \\left(0, \\delta_{ant} + x_i^l x_i^r - x_i^l t_i^l \\right) + \\max \\left( 0, \\delta_{ant} + x_i^r x_i^l - x_i^r t_i^r \\right) \\right] $$\n",
    "\n",
    "Le membre de gauche pénalise si le mot de gauche est plus éloigné de son antonyme que de son négatif. De même, le membre de droite pénalise si le mot de gauche est plus éloigné de son antonyme que de son négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def antonym_cost(model, antonym_pairs, antonym_negatives):\n",
    "    \"\"\"\n",
    "    Calcule le coût de répulsion pour les paires d'antonymes.\n",
    "\n",
    "    antonym_pairs: liste de tuples d'indices de paires d'antonymes\n",
    "    antonym_negatives: dictionnaire de mots avec leurs négatifs {mot_index: négatif_index}\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total pour les paires d'antonymes\n",
    "    \"\"\"\n",
    "    # HINT: Utiliser torch.relu pour max(0, x)\n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Implémentez la fonction `regularization_cost` qui calcule la fonction de coût de régularisation. (4 points)\n",
    "\n",
    "$$ R(B_S, B_A) = \\sum_{x_i \\in V(B_S \\cup B_A)} \\lambda_{reg} \\| \\hat{x}_i - x_i \\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization_cost(model, synonym_pairs, antonym_pairs):\n",
    "    \"\"\"\n",
    "    Calcule le coût de régularisation pour les paires de synonymes et antonymes.\n",
    "\n",
    "    synonym_pairs: liste de tuples d'indices de paires de synonymes\n",
    "    antonym_pairs: liste de tuples d'indices de paires d'antonymes\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total de régularisation\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Mise en place\n",
    "##### 3.3.1 Implémentez la fonction `forward` qui utilise les fonctions définies plus tôt pour calculer le coût total. (4 points)\n",
    "\n",
    "La fonction prend en entrée un lot de synonymes et un lot d'antonymes, c'est-à-dire des paires de synonymes et des paires d'antonymes.\n",
    "\n",
    "Vous devez trouver les négatifs de tous les mots des lots au moyen de votre fonction  run_negative_extraction puis calculer la fonction de coût totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, synonym_pairs, antonym_pairs):\n",
    "    \"\"\"\n",
    "    Fonction forward pour calculer le coût total.\n",
    "\n",
    "    Args : \n",
    "        synonym_pairs   : \n",
    "        Liste de tuples d'indices de paires de synonymes\n",
    "\n",
    "        antonym_pairs   : \n",
    "        Liste de tuples d'indices de paires d'antonymes\n",
    "\n",
    "    Returns:\n",
    "    Tenseur contenant le coût total (attraction, répulsion et régularisation)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Évaluation (3 points)\n",
    "\n",
    "Utilisez la fonction `spearman_rank_correlation` pour compléter la fonction d'évaluation `evaluate` qui exécute le modèle sur le jeu d'évaluation et calcule la corrélation de Spearman entre les scores prédits et réels.\n",
    "\n",
    "Utilisez `torch.no_grad()` pour éviter de stocker les gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data, word_to_index):\n",
    "    \"\"\"\n",
    "    Calclue les prédictions du modèle sur le jeu d'évaluation puis la corrélation de Spearman entre les scores prédits et réels.\n",
    "\n",
    "    model: modèle de plongements de mots\n",
    "    eval_data: pd.DataFrame\n",
    "    word_to_index: dict\n",
    "\n",
    "    Returns:\n",
    "    float, la corrélation de Spearman entre les scores prédits par le modèle et réels.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "    word_pairs = list(zip(eval_data['word 1'], eval_data['word 2']))\n",
    "    eval_indices = [(word_to_index[w1], word_to_index[w2]) for w1, w2 in word_pairs if w1 in word_to_index and w2 in word_to_index]\n",
    "    # TODO\n",
    "    # END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entraînement de zéro (16 Points)\n",
    "\n",
    "Nous allons maintenant entraîner le modèle de zéro, sans utiliser les plongements GloVe pré-entraînés. Ensuite, dans la partie 5, nous entraînerons le modèle en l'initialisant avec les plongements GloVe pré-entraînés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres, optimiseur et DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "train_syn_tensor = data_to_tensor(train_synonyms, glove_word_to_index)\n",
    "train_ant_tensor = data_to_tensor(train_antonyms, glove_word_to_index)\n",
    "\n",
    "syn_data_loader = DataLoader(train_syn_tensor, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "ant_data_loader = DataLoader(train_ant_tensor, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Complétez la cellule suivante pour créer le modèle de zéro `model_zero`, à partir d'une matrice de plongements aléatoire. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zero = None\n",
    "random_seed = 0\n",
    "random_generator = torch.Generator(device=device).manual_seed(random_seed)\n",
    "\n",
    "embeddings_size = glove_embeddings.size()\n",
    "random_init_embeddings = torch.randn(embeddings_size, generator=random_generator, device=device)\n",
    "\n",
    "# TODO\n",
    "model_zero = None\n",
    "# END TODO\n",
    "\n",
    "optimizer = optim.Adam(model_zero.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Entraînez le modèle sur le jeu des paires de synonymes et d'antonymes. (6 points)\n",
    "\n",
    "N'oubliez pas que l'entraînement se fait sur les synonymes et antonymes et que l'évaluation se fait sur SimLex-999.\n",
    "\n",
    "À défaut d'avoir un jeu de validation, on observe les résultats sur le corpus de test : SimLex-999. Ceci n’est fait qu’à titre illustratif pour voir l'évolution de l'apprentissage. Il ne faut pas faire de choix pour l’entrainement à partir des résultats sur le corpus de test.\n",
    "\n",
    "*Note : Les jeux de synonymes et d'antonymes n'ont pas la même taille. Une époque (epoch) correspond à une itération sur le jeu de données le plus petit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_corr = evaluate(model_zero, evaluation_simlex, glove_word_to_index)\n",
    "print(f'Before training, Spearman Correlation: {spearman_corr:.4f}')\n",
    "\n",
    "# TODO\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Courbes d'entraînement du modèle de zéro (4 points)\n",
    "\n",
    "Affichez la perte moyenne sur le jeu d'entraînement et la corrélation de Spearman sur le jeu de validation à chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Évaluation du modèle de zéro et comparaison avec GloVe (5 points)\n",
    "\n",
    "Comparez le modèle de zéro après l'entraînement à GloVe (résultat de la partie 2.4) en termes de corrélation de Spearman sur le jeu de validation. Quelle méthode est la plus performante ? Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Intérêt de GloVe (18 Points)\n",
    "\n",
    "Dans la section précédente, nous avons entraîné un modèle de zéro.\n",
    "\n",
    "Nous allons maintenant évaluer si initialiser le modèle avec les plongements de GloVe permet d'améliorer les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Initialisation avec GloVe (6 points)\n",
    "\n",
    "Entraînez le modèle `model_fine_tuned`, mais cette fois en ititialisant directement avec les plongements du modèle pré-entraîné GloVe.\n",
    "\n",
    "On utilisera Adam comme optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_fine_tuned = None\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_corr = evaluate(model_fine_tuned, evaluation_simlex, glove_word_to_index)\n",
    "print(f'Before training, Spearman Correlation: {spearman_corr:.4f}')\n",
    "\n",
    "# TODO\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Courbes d'entraînement (4 points)\n",
    "\n",
    "Affichez la perte moyenne sur le jeu d'entraînement et la corrélation de Spearman sur le jeu de validation à chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Vérification sur un exemple (3 points)\n",
    "\n",
    "Avec le modèle `model_fine_tuned`, calculez la similarité cosinus entre 'fast' et 'slow' et entre 'fast' et 'rapid'. Commentez les résultats en les comparant avec ceux de la partie 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Analyse, comparaison, conclusion (5 points)\n",
    "\n",
    "Comparez les performances des trois modèles (GloVe, zéro, fine-tuned). Quelle méthode est la plus performante ? Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrables\n",
    "Vous devez remettre votre notebook sur Moodle et Gradescope en ipynb et pdf. Pour Gradescope vous devez associer les numéros de questions avec vos réponses dans le pdf grâce à l'outil que fournit Gradescope.\n",
    "\n",
    "\n",
    "## Évaluation \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code et obtention des sorties attendues\n",
    "2. Réponses correctes aux questions d'analyse\n",
    "3. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "4. Commentaires clairs et informatifs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
