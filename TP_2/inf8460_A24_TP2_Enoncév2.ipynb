{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INF8460 – Traitement automatique de la langue naturelle - Automne 2024\n",
    "\n",
    "## TP2: Plongements de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identification de l'équipe:\n",
    "\n",
    "### Groupe de laboratoire:\n",
    "\n",
    "### Equipe numéro :\n",
    "\n",
    "### Membres:\n",
    "\n",
    "- membre 1 (% de contribution, nature de la contribution)\n",
    "- membre 2 (% de contribution, nature de la contribution)\n",
    "- membre 3 (% de contribution, nature de la contribution)\n",
    "\n",
    "* nature de la contribution: Décrivez brièvement ce qui a été fait par chaque membre de l’équipe. Tous les membres sont censés contribuer au développement. Bien que chaque membre puisse effectuer différentes tâches, vous devez vous efforcer d’obtenir une répartition égale du travail. Soyez précis ! N'indiquez pas seulement : travail réparti équitablement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectif du TP\n",
    "\n",
    "L'objectif de ce TP est d'entraîner un modèle de plongements lexicaux qui intègre des notions de synonymie et d'antonymie en utilisant des réseaux de neurones. \n",
    "\n",
    "Contrairement aux plongements vus en cours comme GloVe ou Word2Vec, qui positionnent souvent les mots à proximité équivalente de leurs synonymes et antonymes, nous chercherons à faire distinguer à nos modèles les synonymes et antonymes, en rapprochant les mots de leurs synonymes et en les éloignant de leurs antonymes.\n",
    "\n",
    "### Jeux de données\n",
    "\n",
    "**Paires de synonymes et antonymes** (entraînement) : Les fichiers `train_synonyms.txt` et `train_antonymms.txt` contiennent les paires de synonymes et d'antonymes qui serviront à l'entraînement de nos modèles.\n",
    "- train_synonyms: ~640k paires de synonymes\n",
    "- train_antonyms: ~12k paires d'antonymes\n",
    "\n",
    "**SimLex-999** (test) : Le fichier `simlex_english.txt` contient 1000 paires de mots et leur similarité entre 0 et 10. Des antonymes auront une similarité de 0 et des mots proches auront une similarité plus élevée. Par exemples :\n",
    "\n",
    "- *nice* & *cruel* -> 0\n",
    "- *violent* & *angry* -> 5.9\n",
    "- *essential* & *necessary* -> 9.8\n",
    "\n",
    "### Développement du TP\n",
    "\n",
    "Le TP suivra les étapes suivantes:\n",
    "\n",
    "- Partie 1 : Familiarisation avec GloVe, modèle de plongements de mots pré-entraîné\n",
    "- Partie 2 : Évaluation de GloVe sur SimLex-999\n",
    "- Partie 3 : Mise en place de la méthode d'entraînement\n",
    "- Partie 4 : Entraînement de zéro (baseline)\n",
    "- Partie 5 : Entraînement utilisant GloVe pré-entraîné et conclusion\n",
    "\n",
    "Le TP est noté sur 89 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies autorisées\n",
    "\n",
    "+ numpy\n",
    "+ pandas\n",
    "+ torch\n",
    "+ matplotlib\n",
    "\n",
    "Si vous voulez utiliser une autre librairie, veuillez demander à votre chargé de lab.\n",
    "\n",
    "### Imports\n",
    "\n",
    "Les imports effectués dans la cellule suivante devraient être suffisants pour faire tout ce TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:33:53.552257500Z",
     "start_time": "2024-10-02T17:33:50.963112500Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GloVe (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons utiliser le modèle pré-entraîné GloVe qui crée des plongements lexicaux de mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Chargement du modèle\n",
    "\n",
    "La cellule suivante permet charger le modèle GloVe pré-entraîné. Le chargement du modèle peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:33:53.559881400Z",
     "start_time": "2024-10-02T17:33:53.555970700Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Décommenter pour télécharger les GloVe embeddings à partir de https://nlp.stanford.edu/projects/glove/\n",
    "# !wget http://nlp.stanford.edu/data/glove.42B.300d.zip -P /content\n",
    "# !unzip /content/glove.42B.300d.zip -d /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:33:53.560413600Z",
     "start_time": "2024-10-02T17:33:53.560413600Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model_path = 'glove.42B.300d.txt'\n",
    "\n",
    "data_root = 'data'\n",
    "\n",
    "train_synonyms_path = f\"{data_root}/train_synonyms.txt\"\n",
    "train_antonyms_path = f\"{data_root}/train_antonyms.txt\"\n",
    "\n",
    "eval_simlex = f'{data_root}/simlex_english.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:33:53.565421100Z",
     "start_time": "2024-10-02T17:33:53.560413600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_word_vectors(filepath, vocab=None):\n",
    "    \"\"\"\n",
    "    Télécharge le modèle pré-entraîné de plongements de mots en pytorch\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    index = 0\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            if vocab is None or word in vocab:\n",
    "                embedding = np.array(split_line[1:], dtype=np.float32)\n",
    "                word_to_index[word] = index\n",
    "                embeddings.append(embedding)\n",
    "                index += 1\n",
    "    \n",
    "    embeddings = np.stack(embeddings)\n",
    "    embeddings = torch.from_numpy(embeddings)\n",
    "    return word_to_index, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:29.954781400Z",
     "start_time": "2024-10-02T17:33:53.565421100Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index, embeddings = load_word_vectors(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:30.009357500Z",
     "start_time": "2024-10-02T17:35:30.001689400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{',': 0,\n 'the': 1,\n '.': 2,\n 'and': 3,\n 'to': 4,\n 'of': 5,\n 'a': 6,\n 'in': 7,\n '\"': 8,\n 'is': 9,\n 'for': 10,\n ':': 11,\n 'i': 12,\n ')': 13,\n 'that': 14,\n '(': 15,\n 'you': 16,\n 'it': 17,\n 'on': 18,\n '-': 19,\n 'with': 20,\n \"'s\": 21,\n 'this': 22,\n 'by': 23,\n 'are': 24,\n 'at': 25,\n 'as': 26,\n 'be': 27,\n 'from': 28,\n 'have': 29,\n 'was': 30,\n 'or': 31,\n 'your': 32,\n 'not': 33,\n '...': 34,\n 'we': 35,\n '!': 36,\n 'but': 37,\n '?': 38,\n 'all': 39,\n 'will': 40,\n 'an': 41,\n 'my': 42,\n 'can': 43,\n 'they': 44,\n \"n't\": 45,\n 'do': 46,\n 'he': 47,\n 'more': 48,\n 'if': 49,\n 'one': 50,\n 'has': 51,\n '|': 52,\n 'so': 53,\n 'about': 54,\n 'new': 55,\n 'what': 56,\n 'his': 57,\n 'there': 58,\n 'up': 59,\n 'out': 60,\n ';': 61,\n 'their': 62,\n 'our': 63,\n \"'\": 64,\n 'like': 65,\n 'when': 66,\n '$': 67,\n 'just': 68,\n 'time': 69,\n '&': 70,\n 'me': 71,\n 'which': 72,\n 'who': 73,\n 'no': 74,\n 'would': 75,\n '/': 76,\n '1': 77,\n 'some': 78,\n 'get': 79,\n '[': 80,\n ']': 81,\n 'also': 82,\n 'other': 83,\n 'how': 84,\n 'may': 85,\n 'had': 86,\n 'am': 87,\n 'been': 88,\n '2': 89,\n 'her': 90,\n 'were': 91,\n 'them': 92,\n 'people': 93,\n 'she': 94,\n 'any': 95,\n 'now': 96,\n 'only': 97,\n 'pm': 98,\n 'first': 99,\n 'than': 100,\n 'good': 101,\n '>': 102,\n 'into': 103,\n 'its': 104,\n 'these': 105,\n 'us': 106,\n 'see': 107,\n 'here': 108,\n 'make': 109,\n 'home': 110,\n '3': 111,\n 'very': 112,\n 'over': 113,\n 'most': 114,\n 'then': 115,\n 'know': 116,\n 'said': 117,\n 'after': 118,\n 'well': 119,\n 'use': 120,\n 'two': 121,\n '%': 122,\n 'did': 123,\n 'could': 124,\n 'day': 125,\n 'great': 126,\n 'free': 127,\n 'many': 128,\n 'back': 129,\n 'way': 130,\n 'work': 131,\n 'because': 132,\n \"'m\": 133,\n 'best': 134,\n 'should': 135,\n 'even': 136,\n 'year': 137,\n 'think': 138,\n 'years': 139,\n 'much': 140,\n 'does': 141,\n 'where': 142,\n '4': 143,\n 'go': 144,\n 'love': 145,\n 'need': 146,\n 'last': 147,\n 'find': 148,\n '2012': 149,\n '=': 150,\n 'world': 151,\n 'really': 152,\n 'information': 153,\n 'through': 154,\n 'want': 155,\n '#': 156,\n '5': 157,\n 'him': 158,\n 'right': 159,\n 'take': 160,\n 'views': 161,\n 'such': 162,\n 'made': 163,\n 'those': 164,\n 'business': 165,\n 'life': 166,\n '2011': 167,\n 'before': 168,\n 'being': 169,\n '--': 170,\n 'off': 171,\n 'used': 172,\n 'help': 173,\n '10': 174,\n 'while': 175,\n 'posted': 176,\n \"'re\": 177,\n 'too': 178,\n 'still': 179,\n \"'ve\": 180,\n 'down': 181,\n '2010': 182,\n 'going': 183,\n 'video': 184,\n 'part': 185,\n 'online': 186,\n 'each': 187,\n 'little': 188,\n 'high': 189,\n '+': 190,\n 'look': 191,\n 'around': 192,\n 'same': 193,\n 'game': 194,\n 'read': 195,\n 'service': 196,\n '2009': 197,\n 'long': 198,\n 'why': 199,\n 'school': 200,\n 'state': 201,\n '6': 202,\n 'post': 203,\n 'city': 204,\n 'own': 205,\n 'every': 206,\n 'system': 207,\n 'next': 208,\n \"'ll\": 209,\n 'news': 210,\n 'site': 211,\n 'say': 212,\n 'got': 213,\n 'both': 214,\n 'under': 215,\n 'top': 216,\n 'set': 217,\n 'another': 218,\n 'since': 219,\n 'things': 220,\n 'available': 221,\n 'show': 222,\n 'place': 223,\n 'using': 224,\n 'better': 225,\n 'page': 226,\n 'never': 227,\n 'please': 228,\n 'come': 229,\n 'big': 230,\n 'today': 231,\n 's': 232,\n 'family': 233,\n 'music': 234,\n 'services': 235,\n 'full': 236,\n 'something': 237,\n 'number': 238,\n 'company': 239,\n 'few': 240,\n '7': 241,\n 'price': 242,\n 'ca': 243,\n 'between': 244,\n 'days': 245,\n 'book': 246,\n '0': 247,\n 'name': 248,\n 'view': 249,\n 'found': 250,\n 'ago': 251,\n 'real': 252,\n 'without': 253,\n 'old': 254,\n 'search': 255,\n 'always': 256,\n 'black': 257,\n 'house': 258,\n '2008': 259,\n 'three': 260,\n '8': 261,\n 'man': 262,\n 'must': 263,\n 'during': 264,\n 'again': 265,\n 'end': 266,\n '12': 267,\n 'different': 268,\n 'lot': 269,\n 'health': 270,\n 'looking': 271,\n 'week': 272,\n 'sure': 273,\n 'group': 274,\n 'give': 275,\n '20': 276,\n 'team': 277,\n 'power': 278,\n 'times': 279,\n 'let': 280,\n 'car': 281,\n 'area': 282,\n 'buy': 283,\n 'money': 284,\n 'water': 285,\n 'keep': 286,\n 'support': 287,\n 'web': 288,\n 'data': 289,\n 'live': 290,\n 'comments': 291,\n 'small': 292,\n '.....': 293,\n 'play': 294,\n 'including': 295,\n 'x': 296,\n 'thing': 297,\n 'list': 298,\n 'put': 299,\n 'children': 300,\n 'design': 301,\n 'women': 302,\n 'might': 303,\n 'white': 304,\n 'public': 305,\n 'point': 306,\n 'order': 307,\n '2013': 308,\n 'call': 309,\n 'second': 310,\n 'local': 311,\n 'says': 312,\n 'case': 313,\n 'care': 314,\n 'download': 315,\n 'reblogged': 316,\n 'program': 317,\n '....': 318,\n 'night': 319,\n '15': 320,\n 'blog': 321,\n 'away': 322,\n 'start': 323,\n 'products': 324,\n 'review': 325,\n 'against': 326,\n 'left': 327,\n 'however': 328,\n 'center': 329,\n 'feel': 330,\n 'having': 331,\n '11': 332,\n 'website': 333,\n 'line': 334,\n 'thanks': 335,\n 'food': 336,\n 'ever': 337,\n '9': 338,\n 'open': 339,\n 'university': 340,\n 'room': 341,\n 'though': 342,\n '30': 343,\n 'god': 344,\n 'product': 345,\n 'national': 346,\n 'change': 347,\n 'making': 348,\n 'experience': 349,\n 'add': 350,\n '2007': 351,\n 'internet': 352,\n 'hotel': 353,\n 'check': 354,\n 'job': 355,\n 'getting': 356,\n 'date': 357,\n \"'d\": 358,\n 'quality': 359,\n 'once': 360,\n 'course': 361,\n 'american': 362,\n 'hard': 363,\n 'within': 364,\n 'contact': 365,\n 'until': 366,\n 'try': 367,\n 'sale': 368,\n 'games': 369,\n 'side': 370,\n 'able': 371,\n 'county': 372,\n 'phone': 373,\n 'research': 374,\n 'community': 375,\n 'software': 376,\n 'working': 377,\n 'march': 378,\n 'art': 379,\n 'yet': 380,\n 'months': 381,\n 'body': 382,\n 'easy': 383,\n 'control': 384,\n 'story': 385,\n 'done': 386,\n 'light': 387,\n 'de': 388,\n 'click': 389,\n 'country': 390,\n 'provide': 391,\n 'market': 392,\n 'results': 393,\n 'large': 394,\n 'person': 395,\n 'actually': 396,\n 'less': 397,\n 'office': 398,\n 'enough': 399,\n 'men': 400,\n 'government': 401,\n 'june': 402,\n 'following': 403,\n 'special': 404,\n 'international': 405,\n 'states': 406,\n 'report': 407,\n 'makes': 408,\n 'per': 409,\n 'important': 410,\n 'united': 411,\n 'series': 412,\n 'april': 413,\n 'doing': 414,\n 'size': 415,\n 'air': 416,\n 'hours': 417,\n 'red': 418,\n 'john': 419,\n 'south': 420,\n 'project': 421,\n 'thought': 422,\n 'nice': 423,\n 'management': 424,\n 'based': 425,\n 'run': 426,\n 'development': 427,\n '100': 428,\n 'type': 429,\n 'history': 430,\n 'least': 431,\n 'friends': 432,\n '18': 433,\n '14': 434,\n '13': 435,\n 'email': 436,\n 'party': 437,\n 'general': 438,\n 'north': 439,\n '16': 440,\n 'law': 441,\n 'bad': 442,\n 'far': 443,\n '25': 444,\n 'head': 445,\n 'problem': 446,\n 'already': 447,\n 'called': 448,\n 'process': 449,\n 'january': 450,\n 'fun': 451,\n 'york': 452,\n 'someone': 453,\n 'hope': 454,\n 'students': 455,\n 'watch': 456,\n 'hot': 457,\n 'photo': 458,\n 'level': 459,\n 'movie': 460,\n 'comment': 461,\n 'july': 462,\n 'media': 463,\n 'together': 464,\n 'needs': 465,\n '!!': 466,\n 'others': 467,\n '24': 468,\n 'include': 469,\n 'young': 470,\n 'season': 471,\n 'members': 472,\n 'version': 473,\n 'started': 474,\n 'offer': 475,\n 'went': 476,\n 'form': 477,\n 'anything': 478,\n 'several': 479,\n 'file': 480,\n 'bit': 481,\n 'road': 482,\n 'tell': 483,\n '@': 484,\n 'link': 485,\n 'article': 486,\n 'pretty': 487,\n 'often': 488,\n 'four': 489,\n 'february': 490,\n 'park': 491,\n 'tv': 492,\n 'came': 493,\n 'hand': 494,\n 'yes': 495,\n 'everything': 496,\n 'green': 497,\n 'share': 498,\n 'access': 499,\n 'social': 500,\n 'fact': 501,\n 'reviews': 502,\n 'personal': 503,\n 'visit': 504,\n 'single': 505,\n 'along': 506,\n 'become': 507,\n '~': 508,\n 'points': 509,\n 'plan': 510,\n 'added': 511,\n 'october': 512,\n 'reply': 513,\n 'believe': 514,\n 'technology': 515,\n '2006': 516,\n 'possible': 517,\n 'photos': 518,\n 'street': 519,\n 'everyone': 520,\n 'past': 521,\n 'anyone': 522,\n 'below': 523,\n 'features': 524,\n 'means': 525,\n 'west': 526,\n 'windows': 527,\n 'current': 528,\n 'related': 529,\n 'september': 530,\n 'details': 531,\n 'november': 532,\n 'code': 533,\n 'sex': 534,\n 'tags': 535,\n 'college': 536,\n 'nothing': 537,\n 'class': 538,\n 'event': 539,\n 'card': 540,\n '17': 541,\n 'seen': 542,\n 'december': 543,\n 're': 544,\n 'minutes': 545,\n 'august': 546,\n 'value': 547,\n 'training': 548,\n 'beautiful': 549,\n 'early': 550,\n 'took': 551,\n 'security': 552,\n 'true': 553,\n 'education': 554,\n 'comes': 555,\n 'kind': 556,\n 'source': 557,\n 'song': 558,\n 'board': 559,\n 'film': 560,\n 'learn': 561,\n 'low': 562,\n 'happy': 563,\n 'question': 564,\n 'front': 565,\n 'kids': 566,\n 'property': 567,\n 'reading': 568,\n 'probably': 569,\n 'short': 570,\n 'above': 571,\n 'create': 572,\n 'computer': 573,\n 'whole': 574,\n 'blue': 575,\n 'either': 576,\n 'given': 577,\n 'example': 578,\n 'heart': 579,\n '£': 580,\n 'near': 581,\n 'parts': 582,\n 'member': 583,\n 'future': 584,\n 'whether': 585,\n 'building': 586,\n 'insurance': 587,\n 'girl': 588,\n 'c': 589,\n '21': 590,\n 'energy': 591,\n '19': 592,\n 'image': 593,\n 'million': 594,\n 'child': 595,\n 'face': 596,\n 'maybe': 597,\n 'pay': 598,\n 'mind': 599,\n 'thank': 600,\n 'close': 601,\n 'books': 602,\n '22': 603,\n 'complete': 604,\n 'network': 605,\n 'main': 606,\n 'u.s.': 607,\n 'trying': 608,\n 'war': 609,\n 'location': 610,\n 'later': 611,\n 'user': 612,\n 'offers': 613,\n 'month': 614,\n 'credit': 615,\n 'president': 616,\n 'save': 617,\n 'idea': 618,\n 'human': 619,\n 'jobs': 620,\n 'words': 621,\n 'works': 622,\n 'travel': 623,\n 'perfect': 624,\n 'total': 625,\n 'cost': 626,\n 'store': 627,\n 'looks': 628,\n 'questions': 629,\n 'almost': 630,\n 'original': 631,\n 'model': 632,\n 'style': 633,\n 'systems': 634,\n 'medical': 635,\n 'space': 636,\n 'action': 637,\n 'sales': 638,\n 'taking': 639,\n 'quite': 640,\n 'age': 641,\n '50': 642,\n 'club': 643,\n 'living': 644,\n 'known': 645,\n 'stop': 646,\n 'posts': 647,\n 'oh': 648,\n '\\\\': 649,\n 'drive': 650,\n 'talk': 651,\n 'box': 652,\n 'events': 653,\n 'color': 654,\n 'content': 655,\n 'study': 656,\n 'companies': 657,\n 'via': 658,\n 'baby': 659,\n '!!!': 660,\n 'church': 661,\n 'b': 662,\n 'natural': 663,\n 'issue': 664,\n 'coming': 665,\n 'test': 666,\n 'major': 667,\n 'five': 668,\n 'performance': 669,\n 'industry': 670,\n 'mean': 671,\n 'america': 672,\n 'key': 673,\n 'uk': 674,\n 'enjoy': 675,\n 'collection': 676,\n 'range': 677,\n 'rate': 678,\n 'marketing': 679,\n 'told': 680,\n 'simple': 681,\n '23': 682,\n 'seems': 683,\n 'digital': 684,\n 'ask': 685,\n 'rights': 686,\n 'court': 687,\n 'due': 688,\n 'sound': 689,\n 'friend': 690,\n 'send': 691,\n 'across': 692,\n 'word': 693,\n '2005': 694,\n 'remember': 695,\n 'leave': 696,\n 'according': 697,\n 'professional': 698,\n 'soon': 699,\n 'wanted': 700,\n 'problems': 701,\n 'miles': 702,\n 'else': 703,\n 'private': 704,\n 'includes': 705,\n 'english': 706,\n 'field': 707,\n 'rather': 708,\n 'cover': 709,\n 'beach': 710,\n 'issues': 711,\n 'stay': 712,\n 'taken': 713,\n 'message': 714,\n 'girls': 715,\n 'reason': 716,\n '>>': 717,\n 'pictures': 718,\n 'address': 719,\n 'financial': 720,\n 'turn': 721,\n 'guide': 722,\n 'although': 723,\n 'especially': 724,\n 'move': 725,\n 'oil': 726,\n 'prices': 727,\n 'half': 728,\n 'star': 729,\n 'simply': 730,\n 'press': 731,\n 'estate': 732,\n 'continue': 733,\n 'running': 734,\n 'section': 735,\n 'woman': 736,\n 'weeks': 737,\n 'policy': 738,\n 'latest': 739,\n 'return': 740,\n '26': 741,\n 'la': 742,\n 'department': 743,\n 'shows': 744,\n 'stock': 745,\n '28': 746,\n 'title': 747,\n 'summer': 748,\n 'provides': 749,\n 'picture': 750,\n 'deal': 751,\n 'note': 752,\n 'm': 753,\n 'win': 754,\n 'paper': 755,\n 'town': 756,\n 'meet': 757,\n 'mobile': 758,\n 'items': 759,\n 'hair': 760,\n 'understand': 761,\n ':)': 762,\n 'friday': 763,\n 'further': 764,\n 'weight': 765,\n 'follow': 766,\n 'application': 767,\n 't': 768,\n 'account': 769,\n 'asked': 770,\n 'plus': 771,\n 'staff': 772,\n 'item': 773,\n 'death': 774,\n 'bring': 775,\n 'sports': 776,\n 'player': 777,\n 'lost': 778,\n 'google': 779,\n '_': 780,\n 'published': 781,\n 'gold': 782,\n 'london': 783,\n 'gets': 784,\n 'act': 785,\n 'among': 786,\n 'shop': 787,\n 'east': 788,\n 'upon': 789,\n 'guy': 790,\n '27': 791,\n 'wedding': 792,\n 'subject': 793,\n 'd': 794,\n 'instead': 795,\n 'morning': 796,\n 'videos': 797,\n 'usually': 798,\n 'clear': 799,\n 'science': 800,\n 'couple': 801,\n 'playing': 802,\n 'standard': 803,\n 'present': 804,\n 'recent': 805,\n 'san': 806,\n 'david': 807,\n 'lyrics': 808,\n 'release': 809,\n 'currently': 810,\n 'table': 811,\n 'etc.': 812,\n 'daily': 813,\n 'record': 814,\n 'stuff': 815,\n 'matter': 816,\n 'required': 817,\n 'fire': 818,\n 'inside': 819,\n 'needed': 820,\n 'sun': 821,\n 'writing': 822,\n 'favorite': 823,\n 'final': 824,\n 'various': 825,\n 'behind': 826,\n 'third': 827,\n 'dog': 828,\n 'common': 829,\n 'provided': 830,\n 'u': 831,\n 'saw': 832,\n 'cool': 833,\n 'guys': 834,\n 'california': 835,\n 'popular': 836,\n 'tax': 837,\n 'interest': 838,\n 'located': 839,\n 'meeting': 840,\n 'rock': 841,\n 'takes': 842,\n 'cheap': 843,\n 'fast': 844,\n 'choose': 845,\n 'position': 846,\n 'links': 847,\n 'similar': 848,\n 'name@domain.com': 849,\n 'hit': 850,\n 'forum': 851,\n 'write': 852,\n 'cut': 853,\n '29': 854,\n 'n': 855,\n 'written': 856,\n 'users': 857,\n 'areas': 858,\n 'info': 859,\n 'sites': 860,\n 'sunday': 861,\n 'created': 862,\n 'result': 863,\n 'amazing': 864,\n 'yourself': 865,\n 'lead': 866,\n 'treatment': 867,\n 'saturday': 868,\n 'cannot': 869,\n 'wo': 870,\n 'terms': 871,\n 'pages': 872,\n 'bill': 873,\n 'land': 874,\n 'e': 875,\n 'fine': 876,\n 'cause': 877,\n 'designed': 878,\n 'resources': 879,\n 'images': 880,\n 'rest': 881,\n 'goes': 882,\n 'wrong': 883,\n 'outside': 884,\n 'band': 885,\n 'ready': 886,\n 'sometimes': 887,\n 'author': 888,\n 'amount': 889,\n 'text': 890,\n 'sign': 891,\n '2004': 892,\n 'customer': 893,\n 'mr.': 894,\n 'police': 895,\n 'changes': 896,\n 'players': 897,\n 'join': 898,\n 'equipment': 899,\n 'longer': 900,\n 'usa': 901,\n 'recently': 902,\n 'finally': 903,\n 'central': 904,\n 'student': 905,\n 'myself': 906,\n 'bank': 907,\n 'choice': 908,\n 'previous': 909,\n 'percent': 910,\n 'programs': 911,\n 'hotels': 912,\n 'map': 913,\n 'step': 914,\n 'w': 915,\n 'washington': 916,\n 'interesting': 917,\n 'strong': 918,\n 'ways': 919,\n 'late': 920,\n 'customers': 921,\n 'director': 922,\n 'global': 923,\n 'period': 924,\n 'mark': 925,\n 'category': 926,\n '{': 927,\n 'welcome': 928,\n 'mother': 929,\n 'bar': 930,\n 'hear': 931,\n 'homes': 932,\n 'held': 933,\n 'shall': 934,\n 'movies': 935,\n 'received': 936,\n 'king': 937,\n 'unique': 938,\n 'forward': 939,\n 'description': 940,\n 'door': 941,\n 'answer': 942,\n 'jan': 943,\n 'christmas': 944,\n 'tools': 945,\n 'james': 946,\n 'worth': 947,\n 'certain': 948,\n 'additional': 949,\n 'fall': 950,\n 'safety': 951,\n 'wall': 952,\n 'heard': 953,\n 'gift': 954,\n 'bed': 955,\n 'material': 956,\n 'excellent': 957,\n 'st.': 958,\n 'practice': 959,\n 'conference': 960,\n 'inc.': 961,\n 'language': 962,\n 'brand': 963,\n 'speed': 964,\n 'wish': 965,\n 'wait': 966,\n 'obama': 967,\n 'included': 968,\n 'build': 969,\n 'sense': 970,\n 'monday': 971,\n 'track': 972,\n 'lol': 973,\n 'learning': 974,\n 'saying': 975,\n 'skin': 976,\n 'loss': 977,\n 'quote': 978,\n 'lake': 979,\n 'huge': 980,\n 'production': 981,\n 'thinking': 982,\n 'six': 983,\n 'album': 984,\n 'brown': 985,\n 'radio': 986,\n 'super': 987,\n 'china': 988,\n 'likely': 989,\n 'society': 990,\n 'eyes': 991,\n 'update': 992,\n 'options': 993,\n '40': 994,\n 'son': 995,\n 'screen': 996,\n 'engine': 997,\n 'wrote': 998,\n 'paul': 999,\n ...}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index\n",
    "# len(word_to_index)\n",
    "# type(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:30.111370600Z",
     "start_time": "2024-10-02T17:35:30.009357500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1838, -0.1212, -0.1199,  ..., -0.0390,  0.1827,  0.1465],\n        [-0.2084, -0.1493, -0.0175,  ..., -0.5407,  0.2120, -0.0094],\n        [ 0.1088,  0.0022,  0.2221,  ..., -0.2970,  0.1594, -0.1490],\n        ...,\n        [ 0.2736,  0.0413, -0.1227,  ..., -0.3318,  0.0379,  0.0564],\n        [-0.0524,  0.3214,  0.2324,  ..., -0.0813,  0.0481, -0.0872],\n        [-0.1197,  0.1602, -0.2492,  ..., -0.0909,  0.2783,  0.1137]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings\n",
    "# print(embeddings.shape)\n",
    "# type(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Implémentez la fonction `cosine_similarity` avec pytorch et sans utiliser `torch.nn.CosineSimilarity` (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:30.114910500Z",
     "start_time": "2024-10-02T17:35:30.111370600Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Calcule la matrice de similarité cosinus entre deux matrices\n",
    "\n",
    "    Args : \n",
    "        a   : torch.Tensor, shape=(n, d)\n",
    "        b   : torch.Tensor, shape=(m, d)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, shape=(n, m)\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalisation\n",
    "    a_norm = a / a.norm(dim=1, keepdim=True)\n",
    "    b_norm = b / b.norm(dim=1, keepdim=True)\n",
    "\n",
    "    return torch.mm(a_norm, b_norm.T) # Produit matriciel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Complétez la fonction `n_closest_vect` qui retourne les n mots les plus proches d'un mot donné. (5 points)\n",
    "\n",
    "`n_closest_vect` prendra en entrée la matrice des plongements `embeddings`, le dictionnaire de correspondance entre les mots et les indices `word_to_index`, le plongement d'un mot `word` et le nombre `n` de mots attendus. La fonction devra retourner la liste des mots dont les plongements sont les plus proches du vecteur de référence et leur similarité cosinus.\n",
    "\n",
    "C'est-à-dire les n mots avec lesquels le mot a la plus grande similarité cosinus. Utilisez la fonction `cosine_similarity` que vous venez d'implémenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:30.127260600Z",
     "start_time": "2024-10-02T17:35:30.114910500Z"
    }
   },
   "outputs": [],
   "source": [
    "def n_closest_vect(embeddings, word_to_index, word, n=5):\n",
    "    \"\"\"\n",
    "    Trouve les n mots les plus proches du vecteur donné et leur similarité\n",
    "\n",
    "    Args : \n",
    "        embeddings      : torch.Tensor, shape=(vocab_size, embedding_dim)\n",
    "        Matrice de plongement de tous les mots\n",
    "\n",
    "        word_to_index   : dict\n",
    "        Dictionnaire qui relie un mot à son index dans le vocabulaire\n",
    "\n",
    "        word            : torch.Tensor, shape=(embedding_dim,)\n",
    "        Plongement du mot dont on cherche les n mots les plus proches\n",
    "\n",
    "        n               : int, number of closest words to return\n",
    "        Nombre de mots à retourner\n",
    "\n",
    "    Returns:\n",
    "    Liste de tuple contenant les n mots les plus similaires avec leur coefficient\n",
    "    de similarité\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcul de la similarité entre word et tous les autres mots\n",
    "    similarities = cosine_similarity(word.unsqueeze(0), embeddings)\n",
    "\n",
    "    # Récupération des n similarités les plus grandes\n",
    "    top_n_similarities, top_n_indices = torch.topk(similarities.squeeze(0), n)\n",
    "\n",
    "    # Construction de la liste de tuples des n mots les plus similaires\n",
    "    closest_words = []\n",
    "    for i in range(n):\n",
    "        similar_word = list(word_to_index.keys())[top_n_indices[i].item()]\n",
    "        similarity = top_n_similarities[i].item()\n",
    "        closest_words.append((similar_word, similarity))\n",
    "\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:32.050319600Z",
     "start_time": "2024-10-02T17:35:30.122249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('morning', 0.9999999403953552), ('afternoon', 0.8665475249290466), ('evening', 0.7880069613456726), ('yesterday', 0.7614548802375793), ('sunday', 0.7548925280570984)]\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "print(n_closest_vect(embeddings, word_to_index, embeddings[word_to_index['morning']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue :\n",
    "```\n",
    "[('morning', 1.0), ('afternoon', 0.8665473461151123), ('evening', 0.7880070209503174), ('yesterday', 0.7614548206329346), ('sunday', 0.7548925876617432)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Quelle est la similarité cosinus entre 'fast' et 'slow' ? Entre 'fast' et 'rapid' ? Commentez les résultats et expliquez leur origine. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:32.060065700Z",
     "start_time": "2024-10-02T17:35:32.050319600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité cosinus 'fast' / 'slow' :  0.7113726139068604\n",
      "Similarité cosinus 'fast' / 'rapid' :  0.6445335149765015\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la similarité cosinus entre 'fast' et 'slow'\n",
    "similarity_fast_slow = cosine_similarity(\n",
    "    embeddings[word_to_index['fast']].unsqueeze(0), \n",
    "    embeddings[word_to_index['slow']].unsqueeze(0)\n",
    ").item()\n",
    "\n",
    "# Calcul de la similarité cosinus entre 'fast' et 'rapid'\n",
    "similarity_fast_rapid = cosine_similarity(\n",
    "    embeddings[word_to_index['fast']].unsqueeze(0), \n",
    "    embeddings[word_to_index['rapid']].unsqueeze(0)\n",
    ").item()\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Similarité cosinus 'fast' / 'slow' : \", similarity_fast_slow)\n",
    "print(\"Similarité cosinus 'fast' / 'rapid' : \", similarity_fast_rapid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Évaluation (12 Points)\n",
    "\n",
    "#### Données\n",
    "\n",
    "Les cellules qui suivent permettent de télécharger les données et de se restreindre au vocabulaire qui nous sera utile, pour éviter de charger des plongements inutiles.\n",
    "\n",
    "Comme décrit dans l'introduction, nous avons 3 fichiers de données:\n",
    "- Des paires de synonymes pour l'entraînement (`train_synonyms.txt`) \n",
    "- Des paires d'antonymes pour l'entraînement (`train_antonymms.txt`)\n",
    "- Des paires de mots avec leur similarité pour l'évaluation (`simlex_english.txt`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:32.075455700Z",
     "start_time": "2024-10-02T17:35:32.060065700Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Télécharge les paires de synonymes et antonymes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            pair = line.strip().split()\n",
    "            assert pair[0].startswith('en_') and pair[1].startswith('en_')\n",
    "            data.append((pair[0][3:], pair[1][3:]))\n",
    "    return data\n",
    "\n",
    "def data_to_tensor(data, word_to_index):\n",
    "    indices = [word_to_index[word] for pair in data for word in pair if word in word_to_index]\n",
    "    return torch.tensor(indices).view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:57.747142100Z",
     "start_time": "2024-10-02T17:35:32.068947400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Données d'entraînement\n",
    "train_synonyms = load_data(train_synonyms_path)\n",
    "train_antonyms = load_data(train_antonyms_path)\n",
    "\n",
    "# Données d'évaluation\n",
    "evaluation_simlex = pd.read_csv(eval_simlex, sep='\\t') # pd dataframe with columns 'word 1', 'word 2', 'score'\n",
    "\n",
    "# On se restreint au vocabulaire qu'on va utiliser pour éviter de charger des embeddings inutiles\n",
    "vocab = set([word for pair in train_synonyms + train_antonyms for word in pair])\n",
    "eval_vocab = set(evaluation_simlex['word 1']).union(set(evaluation_simlex['word 2']))\n",
    "vocab.update(eval_vocab)\n",
    "\n",
    "glove_word_to_index, glove_embeddings = load_word_vectors(pretrained_model_path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Observation du jeu de données SimLex-999. (2 points)\n",
    "\n",
    "Affichez l'histogramme de fréquence des scores de similarité dans le jeu de données SimLex-999. Utilisez `bins=40`. Votre axe des x doit représenter le score de similarité et votre axe des y doit représenter la fréquence. Comment interpréter un score de 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.076060300Z",
     "start_time": "2024-10-02T17:35:57.772446100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0oAAAIiCAYAAAD2CjhuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVnklEQVR4nO3df3yN9f/H8ecZ+21+zjaLjeTHJIRSU0NCRElFaX4rJdUsJQkTTSjpF9KnD8qHVFQ+feqD5Ec15Ud+FGtUbH5MPkeYX2Oc6/uHdr7OtY0552znbHvcb7dzq/M+1/W+Xtd1rnN2nq7rel8WwzAMAQAAAADsfDxdAAAAAAB4G4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAALis2bNnKzg4WD/++KOnSwGAYkFQAorA3LlzZbFYtHHjxnxf79q1q2rXru3QVrt2bfXv3/+KlpOSkqKkpCQdPXrUuULhFnv27JHFYtHcuXM9XUqZ0r9//zyfo6LWtm1btW3btsj7tFgsSkpKcuty8ttPC/sdsmXLFiUmJurDDz9Uq1atnK4h97txz549TvdRlA4fPqxRo0apUaNGCg4OVqVKldSwYUP16dNH27Zts09XFOtxub8bxW39+vXq1KmTQkJCVKFCBbVr107ff/99nukMw9Abb7yhhg0byt/fXzVq1NBjjz2mI0eO5Jk2MzNT/fv3V1hYmAICAtSkSRO99957xbE6gFPKe7oAABd8+umnqlix4hXNk5KSovHjx6t///6qXLly0RQGwG7GjBklos/81KhRQ+vWrVPdunXtbYX5DsnKytL999+vadOmqVu3bsVSqyecOHFCN910k06cOKFnnnlGTZs21enTp7Vz504tWbJEW7ZsUZMmTSRJd955p9atW6caNWp4uOqisWHDBsXFxenGG2/UBx98IMMwNGXKFLVv316rVq3SzTffbJ92xIgRmj59ukaMGKHbb79dO3bs0NixY7VhwwatW7dOvr6+kqRjx47plltu0dmzZzVlyhTVqFFDCxcu1ODBg3Xs2DElJiZ6anWBAhGUAC9x/fXXe7qEK5aTkyOLxaLy5fkqKYtOnz6tgIAAWSwWT5dSbBo1alQi+rzY+fPnde7cOfn7++umm2664vkrVqyoXbt2FUFl3uXjjz/Wb7/9pm+++Ubt2rVzeC0xMVE2m83+vHr16qpevXpxl1hsxowZo8qVK+u///2vgoKCJEm33367rr76ao0YMcJ+ZGn//v16/fXX9fjjj2vy5MmSpA4dOigsLEy9e/fW3Llz9fDDD0uSZs6cqT/++EMbN25UixYtJEmdOnVSZmamxo4dq4EDB/IPfvA6nHoHeAnzqXc2m00TJ05UgwYNFBgYqMqVK6tJkyZ6/fXXJUlJSUl65plnJEl16tSRxWKRxWLR6tWr7fNPmTLFfjpEWFiY+vbtq3379jks1zAMJScnKzo6WgEBAWrZsqVWrFiR53Sg1atXy2Kx6IMPPtDTTz+tq666Sv7+/vrtt9/0v//9T0OHDlWjRo1UoUIFhYWF6bbbbtO3337rsKzcU3+mTp2qyZMnq3bt2goMDFTbtm21c+dO5eTk6LnnnlNkZKQqVaqke+65R4cOHcqznbp27aovvvhC119/vQIDAxUTE6MvvvhC0oXTV2JiYhQcHKwbb7wx39NYNm7cqLvuuktVq1ZVQECArr/+en300UeFep8OHDignj17KiQkRJUqVVKvXr108ODBfKctzHJOnTqlESNGqE6dOgoICFDVqlXVsmVLLVy48JJ1FHa+H3/8Ud26dVO1atUUEBCgunXrKiEhwWGa7777Tu3bt1dISIiCgoIUGxur//znPw7T5J4WtHz5cg0cOFDVq1dXUFCQzpw5I0latGiRbr75ZgUHB6tChQrq1KmTNm/e7NDHH3/8oQceeECRkZHy9/dXeHi42rdvry1btlxyXXOX36BBA/n7+ysmJkbvv/9+vtOdPXtWEydOtO/31atX14ABA/S///3vsssoTH3mz4U79unCnM53pZ+xKVOmaOLEiapTp478/f21atWqPKfeXe47RCrc+1qQH374Qa1bt1ZAQIAiIyM1atQo5eTk5Jlu0aJF6tixo2rUqGH/PD/33HM6efKkw3T9+/dXhQoV9Ntvv6lLly6qUKGCatWqpaefftq+H+aaOXOmmjZtqgoVKigkJEQNGzbU888/f8l6Dx8+LEkFHiXy8fn/n0z5nXrXtm1bNW7cWOvWrVNsbKwCAwNVu3ZtzZkzR5L0n//8R82bN1dQUJCuu+46/fe//71kPQXZtWuXevfurbCwMPvn4e2337a/np2dreuvv17XXHONjh07Zm8/ePCgIiIi1LZtW50/f/6Sy/j+++/Vtm1be0iSpJCQEMXFxSklJUWZmZmSLrzH58+fV5cuXRzm79q1qyRp8eLFDn2Gh4fbQ9LF0548edLp7QEUJf4ZGChCuf+Sa2YYxmXnnTJlipKSkvTCCy8oLi5OOTk5+vXXX+3XEgwePFh//fWX3nzzTS1ZssT+xz33X6cfe+wxzZ49W8OGDVPXrl21Z88ejRkzRqtXr9ZPP/2k0NBQSdLo0aM1adIkPfLII+rRo4f27t2rwYMHKycnR/Xr189T16hRo3TzzTdr1qxZ8vHxUVhYmP1H6Lhx4xQREaETJ07o008/Vdu2bbVy5co8PwLffvttNWnSRG+//baOHj2qp59+Wt26dVOrVq3k6+urf/7zn0pPT9eIESM0ePBgLV261GH+rVu3atSoURo9erQqVaqk8ePHq0ePHho1apRWrlyp5ORkWSwWjRw5Ul27dtXu3bsVGBgoSVq1apXuuOMOtWrVSrNmzVKlSpX04YcfqlevXjp16tQlrxM7ffq0br/9dh04cECTJk1S/fr19Z///Ee9evXKM21hl5OYmKgPPvhAEydO1PXXX6+TJ0/ql19+sf9oK0hh5lu2bJm6deummJgYTZs2TVFRUdqzZ4+WL19un2bNmjXq0KGD/VoBf39/zZgxQ926ddPChQvzrNvAgQN155136oMPPtDJkyfl6+ur5ORkvfDCCxowYIBeeOEFnT17VlOnTtWtt96q9evX2/fJLl266Pz585oyZYqioqJktVqVkpJy2etj5s6dqwEDBujuu+/Wq6++qmPHjikpKUlnzpxx+PFqs9l0991369tvv9Wzzz6r2NhYpaena9y4cWrbtq02btxo3w/y42x9kuv79OX89ddfkgr/GXvjjTdUv359vfLKK6pYsaLq1auXp8/LfYcU9n3Nz44dO9S+fXvVrl1bc+fOVVBQkGbMmKEFCxbkmXbXrl3q0qWLEhISFBwcrF9//VWTJ0/W+vXr9c033zhMm5OTo7vuukuDBg3S008/rbVr12rChAmqVKmSxo4dK0n68MMPNXToUD3xxBN65ZVX5OPjo99++007duy45DbOPZ2sb9++ev7553XrrbeqWrVql5zH7ODBgxowYICeffZZ1axZU2+++aYGDhyovXv36pNPPtHzzz+vSpUq6cUXX1T37t31xx9/KDIystD979ixQ7GxsYqKitKrr76qiIgILVu2TE8++aSsVqvGjRungIAAffTRR2rRooUGDhyoxYsXy2az6aGHHpJhGFq4cKHKlSt3yeWcPXtW/v7+edpz237++WfVqFFDZ8+edWjP5evrK4vF4nBd1+X63LZtmx544IFCbwugWBgA3G7OnDmGpEs+oqOjHeaJjo42+vXrZ3/etWtXo1mzZpdcztSpUw1Jxu7dux3aU1NTDUnG0KFDHdp//PFHQ5Lx/PPPG4ZhGH/99Zfh7+9v9OrVy2G6devWGZKMNm3a2NtWrVplSDLi4uIuu/7nzp0zcnJyjPbt2xv33HOPvX337t2GJKNp06bG+fPn7e3Tp083JBl33XWXQz8JCQmGJOPYsWP2tujoaCMwMNDYt2+fvW3Lli2GJKNGjRrGyZMn7e2fffaZIclYunSpva1hw4bG9ddfb+Tk5Dgsq2vXrkaNGjUc6jKbOXOmIcn4/PPPHdoffvhhQ5IxZ86cK15O48aNje7duxe4zIIUZr66desadevWNU6fPl3gNDfddJMRFhZmHD9+3N527tw5o3HjxkbNmjUNm81mGMb/79N9+/Z1mD8jI8MoX7688cQTTzi0Hz9+3IiIiDB69uxpGIZhWK1WQ5Ixffr0K1rP8+fPG5GRkUbz5s3ttRiGYezZs8fw9fV1+BwtXLjQkGQsXrzYoY8NGzYYkowZM2YUuJzC1temTRuHz4U79mlzn4ZhGJKMcePGFVjH5T5jdevWNc6ePeswT+5rF++nBX2HFPZ9LUivXr2MwMBA4+DBgw41N2zYMN/l5bLZbEZOTo6xZs0aQ5KxdetW+2v9+vUzJBkfffSRwzxdunQxGjRoYH8+bNgwo3LlypesryAvvvii4efnZ/+erlOnjvHoo4861GEY//95uHg92rRpY0gyNm7caG87fPiwUa5cOSMwMNDYv3+/vT33O+uNN97I0+eGDRsKrK9Tp05GzZo1Hfaf3HUOCAgw/vrrL3vbokWL7Pv02LFjDR8fH2P58uWF2g7NmjUz6tev77BP5+TkGFdffbUhyViwYIHDekyYMMFh/pUrVxqSDD8/P3tbQkKC4ePjY6SnpztM26dPH0OS8cgjjxSqNqA4ceodUITef/99bdiwIc/jlltuuey8N954o7Zu3aqhQ4dq2bJlysrKKvRyV61aJUl5jo7ceOONiomJ0cqVKyVdOG3izJkz6tmzp8N0N910U4Gjid177735ts+aNUvNmzdXQECAypcvL19fX61cuVKpqal5pu3SpYvDkYCYmBhJFy6Qvlhue0ZGhkN7s2bNdNVVV+WZznyqSG57enq6JOm3337Tr7/+qoceekiSdO7cOfujS5cuyszMVFpaWr7rJ13YriEhIbrrrrsc2nv37u3w/EqWc+ONN+qrr77Sc889p9WrV+v06dMFLv9il5tv586d+v333zVo0CAFBATk28fJkyf1448/6r777lOFChXs7eXKlVOfPn20b9++PNvD/P4vW7ZM586dU9++fR3WMyAgQG3atLGfxlW1alXVrVtXU6dO1bRp07R582aHaz4KkpaWpgMHDqh3794O10JFR0crNjbWYdovvvhClStXVrdu3RxqadasmSIiIhxOKTNztr5cru7ThXEln7G77rrLfhG9Mwr7vhZk1apVat++vcLDw+1t5cqVy/fo6x9//KHevXsrIiJC5cqVk6+vr9q0aSNJedbNYrHkGVCiSZMm9s+4dOGzcfToUT344IP6/PPPZbVaC73eY8aMUUZGhv75z39qyJAhqlChgmbNmqUWLVpc9nRY6cJpexefWla1alWFhYWpWbNmDkeOzN9NhZGdna2VK1fqnnvuUVBQUJ7vlezsbP3www/26Xv27KnHHntMzzzzjCZOnKjnn39eHTp0sL9uGIZDHxef/fDEE09o586dGjZsmPbv36+9e/fq0Ucftdebu683bdpUcXFxmjp1qj7++GMdPXpUKSkpevTRR1WuXDmHz8QjjzwiX19fPfTQQ9q+fbsOHz6st99+W4sWLXLoE/Am7JVAEYqJiVHLli3zPCpVqnTZeUeNGqVXXnlFP/zwgzp37qxq1aqpffv2hRo69lLn2kdGRtpfz/3vxT9mcuXXVlCf06ZN02OPPaZWrVpp8eLF+uGHH7Rhwwbdcccd+f7wr1q1qsNzPz+/S7ZnZ2e7Zf4///xT0oVRmnx9fR0eQ4cOlaRL/qg6fPhwvtslIiLC4fmVLOeNN97QyJEj9dlnn6ldu3aqWrWqunfvftmL5y83X+7pkDVr1iywjyNHjsgwjAL3k9x1vph52tx1veGGG/Ks66JFi+zrabFYtHLlSnXq1ElTpkxR8+bNVb16dT355JM6fvx4gTXmLt+8jfNr+/PPP3X06FH5+fnlqeXgwYOXfG+drS+Xq/v05VzpZ8zV0dgK+74W5PDhw4V6z06cOKFbb71VP/74oyZOnKjVq1drw4YNWrJkiSTlWbegoKA8wd/f399he/bp08d+quO9996rsLAwtWrVSitWrCjUuoeHh2vAgAGaNWuWtm3bpjVr1sjPz09PPfXUZec1v9/ShffcHfvB4cOHde7cOb355pt53pPca4TM78vAgQOVk5Oj8uXL68knn3R4bc2aNXn6yb3mauDAgXr55Zf1wQcfqGbNmoqKitKOHTs0YsQISXL4h6qPP/5YrVu3Vs+ePVWlShW1a9dOPXr0yPcftD799FOlp6ercePGCg0N1eTJk/Xqq6/m6RPwFlyjBHip8uXLKzExUYmJiTp69Ki+/vprPf/88+rUqZP27t3rcOTELPe8+szMzDw/lA8cOGC/Pil3utwfRRc7ePBgvkeV8hvhbP78+Wrbtq1mzpzp0F6YH5jFKXe9R40apR49euQ7TYMGDQqcv1q1alq/fn2edvNgDleynODgYI0fP17jx4/Xn3/+aT9K1K1bN/36668F1nK5+XJH5DIP3nGxKlWqyMfHx35h9sUOHDjgsC65zO9/7uuffPKJoqOjC1yWdOEoUO49U3bu3KmPPvpISUlJOnv2rGbNmpXvPLn7aH4DZuS33atVq1bgReEhISFur6+4XOlnzNWRCK/kfc1PtWrVCvWeffPNNzpw4IBWr15tP4okyeV7ww0YMEADBgzQyZMntXbtWo0bN05du3bVzp07r3h94uLi1LFjR3322Wc6dOiQwsLCXKrNWVWqVLEf7X388cfznaZOnTr2/z958qT69Omj+vXr688//9TgwYP1+eef219v0aKFNmzY4DD/xUe9Ro4cqYSEBO3atUshISGKjo7WkCFDFBwc7HDULCwsTF9++aUOHTqkgwcPKjo6WoGBgZoxY4buu+8+h/47d+6s9PR0/fbbbzp37pzq169vH+AmLi7O+Y0DFBGCElACVK5cWffdd5/279+vhIQE7dmzR40aNbJfBGv+V9fbbrtN0oUfVzfccIO9fcOGDUpNTdXo0aMlSa1atZK/v78WLVrk8IP+hx9+UHp6eqFv5mmxWPJcpLtt2zatW7dOtWrVuuL1LSoNGjRQvXr1tHXrViUnJ1/x/O3atdNHH32kpUuXOpx+Z75A3dnlhIeHq3///tq6daumT5+uU6dOXTIQX2q++vXrq27duvrnP/+pxMTEfC+iDg4OVqtWrbRkyRK98sor9oEObDab5s+fr5o1a+Y7oMfFOnXqpPLly+v3338v8LTM/NSvX18vvPCCFi9erJ9++qnA6Ro0aGC/30piYqI9AKSnpyslJcXhh13Xrl314Ycf6vz58y7dFPVK6isuRfUZK+g7xNn3NVe7du20dOlS/fnnn/ajsOfPn7efZpUr9/00r9s777xzxcvMT3BwsDp37qyzZ8+qe/fu2r59e4FB6c8//1T16tXznAJ2/vx57dq1S0FBQR4dvjooKEjt2rXT5s2b1aRJE/tRqYI8+uijysjI0Pr16/Xrr7/qvvvu02uvvabhw4dLuvAPBy1btrxkH/7+/mrcuLGkC6eLLlq0SA8//HC+g6KEhYXZQ+Qbb7yhkydPatiwYXmms1gs9sFFzp49q9dff13NmjUjKMErEZQAL9WtWzc1btxYLVu2VPXq1ZWenq7p06crOjra/kfmuuuukyS9/vrr6tevn3x9fdWgQQM1aNBAjzzyiN588035+Pioc+fO9lHvatWqZf9DWbVqVSUmJmrSpEmqUqWK7rnnHu3bt0/jx49XjRo1Cn3OeNeuXTVhwgSNGzdObdq0UVpaml588UXVqVMn31H/POmdd95R586d1alTJ/Xv319XXXWV/vrrL6Wmpuqnn37Sxx9/XOC8ffv21Wuvvaa+ffvqpZdeUr169fTll19q2bJlTi+nVatW6tq1q5o0aaIqVaooNTVVH3zwgW6++eZLhqTCzPf222+rW7duuummmzR8+HBFRUUpIyNDy5Yt07/+9S9J0qRJk9ShQwe1a9dOI0aMkJ+fn2bMmKFffvlFCxcuvOyRidq1a+vFF1/U6NGj9ccff+iOO+5QlSpV9Oeff2r9+vX2I1/btm3TsGHDdP/996tevXry8/PTN998o23btum5554rsH8fHx9NmDBBgwcP1j333KOHH35YR48eVVJSUp7TuB544AH961//UpcuXfTUU0/pxhtvlK+vr/bt26dVq1bp7rvv1j333JPvcpytr7gU1WesoO+Qwr6vBXnhhRe0dOlS3XbbbRo7dqyCgoL09ttv5xnyOzY2VlWqVNGjjz6qcePGydfXV//617+0detWp9cp94d869atVaNGDR08eFCTJk1SpUqVHP7hyOyDDz7QO++8o969e+uGG25QpUqVtG/fPv3jH//Q9u3bNXbs2MuGE3f45ptvHIYdz9WlSxe9/vrruuWWW3TrrbfqscceU+3atXX8+HH99ttv+ve//20fJfAf//iH5s+frzlz5ujaa6/Vtddeq2HDhmnkyJFq3bq1brzxxkvW8Msvv2jx4sVq2bKl/P39tXXrVr388suqV6+eJkyY4DDtu+++K0mqW7eujh49qq+++krvvfeekpOT1bx5c4dpn3jiCbVt21bVqlXTH3/8oTfeeEP79u3TmjVrXNhiQBHy9GgSQGl0udGL7rzzzsuOevfqq68asbGxRmhoqOHn52dERUUZgwYNMvbs2eMw36hRo4zIyEjDx8fHkGSsWrXKMIwLo4VNnjzZqF+/vuHr62uEhoYa8fHxxt69ex3mt9lsxsSJE42aNWsafn5+RpMmTYwvvvjCaNq0qcNoWrmj3n388cd51ufMmTPGiBEjjKuuusoICAgwmjdvbnz22WdGv379HNYzd9StqVOnOsxfUN/5bcfo6GjjzjvvzFODJOPxxx93aCtoeVu3bjV69uxphIWFGb6+vkZERIRx2223GbNmzcrTr9m+ffuMe++916hQoYIREhJi3HvvvUZKSkqe0cQKu5znnnvOaNmypVGlShXD39/fuPrqq43hw4cbVqv1knUUdr5169YZnTt3NipVqmT4+/sbdevWNYYPH+4wzbfffmvcdtttRnBwsBEYGGjcdNNNxr///W+HaS63T3/22WdGu3btjIoVKxr+/v5GdHS0cd999xlff/21YRiG8eeffxr9+/c3GjZsaAQHBxsVKlQwmjRpYrz22mvGuXPnLrmuhmEY//jHP4x69eoZfn5+Rv369Y1//vOfefYvw7gwMtcrr7xiNG3a1AgICDAqVKhgNGzY0BgyZIixa9euAvsvbH0FjXrnyj5dmFHvXP2MXfyaeT8t6DvEMC7/vl7K999/b9x0002Gv7+/ERERYTzzzDPG7Nmz84wWl5KSYtx8881GUFCQUb16dWPw4MHGTz/9lKfWfv36GcHBwXmWM27cOOPinzPz5s0z2rVrZ4SHhxt+fn5GZGSk0bNnT2Pbtm2XrHfHjh3G008/bbRs2dKoXr26Ub58eaNKlSpGmzZtjA8++MBh2oJGvbv22mvz9FvY76zLjZaau6zdu3cbAwcONK666irD19fXqF69uhEbG2tMnDjRMAzD2LZtmxEYGOjw98QwDCM7O9to0aKFUbt2bePIkSOX3BZpaWlGXFycUbVqVcPPz8+45pprjBdeeME4ceJEnmnfeecdIyYmxggKCjIqVKhg3HrrrcZnn32Wb7933323UaNGDfv3Yf/+/fP8TQO8icUwCnFDFwBlyu7du9WwYUONGzfusjdpBAAAKI0ISkAZt3XrVi1cuFCxsbGqWLGi0tLSNGXKFGVlZemXX34pcPQ7AACA0oxrlIAyLjg4WBs3btR7772no0ePqlKlSmrbtq1eeuklQhIAACizOKIEAAAAACbccBYAAAAATAhKAAAAAGBCUAIAAAAAkzIxmIPNZtOBAwcUEhJy2ZsnAgAAACi9DMPQ8ePHFRkZKR+fgo8blYmgdODAAdWqVcvTZQAAAADwEnv37lXNmjULfL1MBKWQkBBJFzZGxYoVPVwNAAAAAE/JyspSrVq17BmhIGUiKOWeblexYkWCEgAAAIDLXpLDYA4AAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAk/KeLqAsysjIkNVqdamP0NBQRUVFuakiAAAAABcjKBWzjIwMNWgQo+zsUy71ExAQpLS0VMISAAAAUAQISsXMarX+HZLmS4pxspdUZWfHy2q1EpQAAACAIkBQ8pgYSc09XQQAAACAfDCYAwAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgIlHg9K5c+f0wgsvqE6dOgoMDNTVV1+tF198UTabzT6NYRhKSkpSZGSkAgMD1bZtW23fvt2DVQMAAAAo7TwalCZPnqxZs2bprbfeUmpqqqZMmaKpU6fqzTfftE8zZcoUTZs2TW+99ZY2bNigiIgIdejQQcePH/dg5QAAAABKM48GpXXr1unuu+/WnXfeqdq1a+u+++5Tx44dtXHjRkkXjiZNnz5do0ePVo8ePdS4cWPNmzdPp06d0oIFCzxZOgAAAIBSrLwnF37LLbdo1qxZ2rlzp+rXr6+tW7fqu+++0/Tp0yVJu3fv1sGDB9WxY0f7PP7+/mrTpo1SUlI0ZMiQfPs9c+aMzpw5Y3+elZUlScrJyVFOTk7RrVAh2Gw2BQYGSrJJcrYWm6RA2Ww2j68PAAAAUJIU9vezR4PSyJEjdezYMTVs2FDlypXT+fPn9dJLL+nBBx+UJB08eFCSFB4e7jBfeHi40tPTC+x30qRJGj9+fJ725cuXKygoyI1r4JyFCxdK2v/3w+letH//fu3f70ofAAAAQNly6tSpQk3n0aC0aNEizZ8/XwsWLNC1116rLVu2KCEhQZGRkerXr599OovF4jCfYRh52i42atQoJSYm2p9nZWWpVq1a6tixoypWrOj+FbkCW7duVVxcnKS1kpo624ukOK1du1ZNmzrbBwAAAFD25J5tdjkeDUrPPPOMnnvuOT3wwAOSpOuuu07p6emaNGmS+vXrp4iICEkXjizVqFHDPt+hQ4fyHGW6mL+/v/z9/fO0+/r6ytfX181rcWV8fHx0+vRpXbg8zNlafCSdlo+Pj8fXBwAAAChJCvv72aODOZw6dUo+Po4llCtXzj48eJ06dRQREaEVK1bYXz979qzWrFmj2NjYYq0VAAAAQNnh0SNK3bp100svvaSoqChde+212rx5s6ZNm6aBAwdKunDKXUJCgpKTk1WvXj3Vq1dPycnJCgoKUu/evT1ZOgAAAIBSzKNB6c0339SYMWM0dOhQHTp0SJGRkRoyZIjGjh1rn+bZZ5/V6dOnNXToUB05ckStWrXS8uXLFRIS4sHKAQAAAJRmFsMwDE8XUdSysrJUqVIlHTt2zOODOfz0009q0aKFpE2Smjvbi6QW2rRpk5o3d7YPAAAAoOwpbDbw6DVKAAAAAOCNCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACASXlPFwDkysjIkNVqdbmf0NBQRUVFuaEiAAAAlFUEJXiFjIwMNWgQo+zsUy73FRAQpLS0VMISAAAAnEZQglewWq1/h6T5kmJc6ClV2dnxslqtBCUAAAA4jaAELxMjqbmniwAAAEAZx2AOAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAxONBaf/+/YqPj1e1atUUFBSkZs2aadOmTfbXDcNQUlKSIiMjFRgYqLZt22r79u0erBgAAABAaefRoHTkyBG1bt1avr6++uqrr7Rjxw69+uqrqly5sn2aKVOmaNq0aXrrrbe0YcMGRUREqEOHDjp+/LjnCgcAAABQqpX35MInT56sWrVqac6cOfa22rVr2//fMAxNnz5do0ePVo8ePSRJ8+bNU3h4uBYsWKAhQ4YUd8kAAAAAygCPBqWlS5eqU6dOuv/++7VmzRpdddVVGjp0qB5++GFJ0u7du3Xw4EF17NjRPo+/v7/atGmjlJSUAoPSmTNndObMGfvzrKwsSVJOTo5ycnKKcI0uz2azKTAwUJJNkrO12CQFymazeXx93MU920UqjdsGAAAA7lPY34gWwzCMIq6lQAEBAZKkxMRE3X///Vq/fr0SEhL0zjvvqG/fvkpJSVHr1q21f/9+RUZG2ud75JFHlJ6ermXLluXbb1JSksaPH5+nfcGCBQoKCiqalQEAAADg9U6dOqXevXvr2LFjqlixYoHTefSIks1mU8uWLZWcnCxJuv7667V9+3bNnDlTffv2tU9nsVgc5jMMI0/bxUaNGqXExET786ysLNWqVUsdO3a85MYoDlu3blVcXJyktZKaOtuLpDitXbtWTZs624d3cc92kUrjtgEAAID75J5tdjkeDUo1atRQo0aNHNpiYmK0ePFiSVJERIQk6eDBg6pRo4Z9mkOHDik8PLzAfv39/eXv75+n3dfXV76+vu4o3Wk+Pj46ffq0Loyj4WwtPpJOy8fHx+Pr4y7u2S5Sadw2AAAAcJ/C/kb06Kh3rVu3VlpamkPbzp07FR0dLUmqU6eOIiIitGLFCvvrZ8+e1Zo1axQbG1ustQIAAAAoOzx6RGn48OGKjY1VcnKyevbsqfXr12v27NmaPXu2pAun3CUkJCg5OVn16tVTvXr1lJycrKCgIPXu3duTpQMAAAAoxTwalG644QZ9+umnGjVqlF588UXVqVNH06dP10MPPWSf5tlnn9Xp06c1dOhQHTlyRK1atdLy5csVEhLiwcoBAAAAlGYeDUqS1LVrV3Xt2rXA1y0Wi5KSkpSUlFR8RQEAAAAo0zx6jRIAAAAAeCOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADAhKAEAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYuByUfvvtNy1btkynT5+WJBmG4XJRAAAAAOBJTgelw4cP6/bbb1f9+vXVpUsXZWZmSpIGDx6sp59+2m0FAgAAAEBxczooDR8+XOXLl1dGRoaCgoLs7b169dJ///tftxQHAAAAAJ5Q3tkZly9frmXLlqlmzZoO7fXq1VN6errLhQEAAACApzh9ROnkyZMOR5JyWa1W+fv7u1QUAAAAAHiS00EpLi5O77//vv25xWKRzWbT1KlT1a5dO7cUBwAAAACe4PSpd1OnTlXbtm21ceNGnT17Vs8++6y2b9+uv/76S99//707awQAAACAYuX0EaVGjRpp27ZtuvHGG9WhQwedPHlSPXr00ObNm1W3bl131ggAAAAAxcrpI0qSFBERofHjx7urFgAAAADwCk4fUZozZ44+/vjjPO0ff/yx5s2b51JRAAAAAOBJTgell19+WaGhoXnaw8LClJyc7FJRAAAAAOBJTgel9PR01alTJ097dHS0MjIyXCoKAAAAADzJ6aAUFhambdu25WnfunWrqlWr5lJRAAAAAOBJTgelBx54QE8++aRWrVql8+fP6/z58/rmm2/01FNP6YEHHnBnjQAAAABQrJwe9W7ixIlKT09X+/btVb78hW5sNpv69u3LNUoAAAAASjSng5Kfn58WLVqkCRMmaOvWrQoMDNR1112n6Ohod9YHAAAAAMXOpfsoSVL9+vVVv359d9QCoAAZGRmyWq0u9REaGqqoqCg3VQQAAFC6OR2Uzp8/r7lz52rlypU6dOiQbDabw+vffPONy8UBuBCSGjSIUXb2KZf6CQgIUlpaKmEJAACgEJwOSk899ZTmzp2rO++8U40bN5bFYnFnXQD+ZrVa/w5J8yXFONlLqrKz42W1WglKAAAAheB0UPrwww/10UcfqUuXLu6sB0CBYiQ193QRAAAAZYLTw4P7+fnpmmuucWctAAAAAOAVnA5KTz/9tF5//XUZhuHOegAAAADA45w+9e67777TqlWr9NVXX+naa6+Vr6+vw+tLlixxuTgAAAAA8ASng1LlypV1zz33uLMWAAAAAPAKTgelOXPmuLMOAAAAAPAaTl+jJEnnzp3T119/rXfeeUfHjx+XJB04cEAnTpxwS3EAAAAA4AlOH1FKT0/XHXfcoYyMDJ05c0YdOnRQSEiIpkyZouzsbM2aNcuddQIAAABAsXH6iNJTTz2lli1b6siRIwoMDLS333PPPVq5cqVbigMAAAAAT3Bp1Lvvv/9efn5+Du3R0dHav3+/y4UBAAAAgKc4fUTJZrPp/Pnzedr37dunkJAQl4oCAAAAAE9yOih16NBB06dPtz+3WCw6ceKExo0bpy5durijNgAAAADwCKdPvXvttdfUrl07NWrUSNnZ2erdu7d27dql0NBQLVy40J01AgAAAECxcjooRUZGasuWLVq4cKF++ukn2Ww2DRo0SA899JDD4A4AAAAAUNI4HZQkKTAwUAMHDtTAgQPdVQ8AAAAAeJzTQen999+/5Ot9+/Z1tmsAAAAA8Cing9JTTz3l8DwnJ0enTp2Sn5+fgoKCCEoAAAAASiynR707cuSIw+PEiRNKS0vTLbfcwmAOAAAAAEo0p4NSfurVq6eXX345z9EmAAAAAChJ3BqUJKlcuXI6cOCAu7sFAAAAgGLj9DVKS5cudXhuGIYyMzP11ltvqXXr1i4XBgAAAACe4nRQ6t69u8Nzi8Wi6tWr67bbbtOrr77qal0AAAAA4DFOByWbzebOOgAAAADAa7j9GiUAAAAAKOmcPqKUmJhY6GmnTZvm7GIAAAAAoNg5HZQ2b96sn376SefOnVODBg0kSTt37lS5cuXUvHlz+3QWi8X1KgEAAACgGDkdlLp166aQkBDNmzdPVapUkXThJrQDBgzQrbfeqqefftptRQIAAABAcXL6GqVXX31VkyZNsockSapSpYomTpzIqHcAAAAASjSng1JWVpb+/PPPPO2HDh3S8ePHXSoKAAAAADzJ6aB0zz33aMCAAfrkk0+0b98+7du3T5988okGDRqkHj16uLNGAAAAAChWTl+jNGvWLI0YMULx8fHKycm50Fn58ho0aJCmTp3qtgIBAAAAoLg5HZSCgoI0Y8YMTZ06Vb///rsMw9A111yj4OBgd9YHeERGRoasVqvL/YSGhioqKsoNFblHamqqy3142zoBAAAUBaeDUq7MzExlZmYqLi5OgYGBMgyDIcFRomVkZKhBgxhlZ59yua+AgCClpaV6QbDIlOSj+Ph4l3vynnUCAAAoOoUOSjabTT4+/39J0+HDh9WzZ0+tWrVKFotFu3bt0tVXX63BgwercuXKjHyHEstqtf4dkuZLinGhp1RlZ8fLarV6Qag4Ksmm0rVOAAAARafQQWnatGlq1KiRunTpIkkaPny4fH19lZGRoZiY///h1atXLw0fPpyghFIgRlLzy05VspTGdQIAAHC/QgelDh066L777lNmZqYGDRqk5cuXa9myZapZs6bDdPXq1VN6errbCwUAAACA4lLo4cGbNm2q9evX69///rck6eTJkwoKCsozndVqlb+/v/sqBAAAAIBidkX3UapSpYo+++wzSVJcXJzef/99+2sWi0U2m01Tp05Vu3bt3FokAAAAABQnp0e9mzp1qtq2bauNGzfq7NmzevbZZ7V9+3b99ddf+v77791ZIwAAAAAUqys6onSxRo0aadu2bbrxxhvVoUMHnTx5Uj169NDmzZtVt25dd9YIAAAAAMXKqSNKOTk56tixo9555x2NHz/e3TUBAAAAgEc5dUTJ19dXv/zyCzeWBQAAAFAqOX3qXd++ffXee++5sxYAAAAA8ApOB6WzZ89q5syZatGihYYMGaLExESHhzMmTZoki8WihIQEe5thGEpKSlJkZKQCAwPVtm1bbd++3dmyAQAAAOCyrvgapT/++EO1a9fWL7/8oubNm0uSdu7c6TCNM6fkbdiwQbNnz1aTJk0c2qdMmaJp06Zp7ty5ql+/viZOnKgOHTooLS1NISEhV7wcAAAAALicKw5K9erVU2ZmplatWiVJ6tWrl9544w2Fh4c7XcSJEyf00EMP6d1339XEiRPt7YZhaPr06Ro9erR69OghSZo3b57Cw8O1YMECDRkyxOllAgAAAEBBrjgoGYbh8Pyrr77SyZMnXSri8ccf15133qnbb7/dISjt3r1bBw8eVMeOHe1t/v7+atOmjVJSUgoMSmfOnNGZM2fsz7OysiRdGK0vJyfHpVpdZbPZFBgYKMkmydlabJICZbPZPL4+7uKe7SK5Y9t4Uy3urcd71gkAAMBTCvsbxukbzuYyB6cr9eGHH+qnn37Shg0b8rx28OBBScpztCo8PFzp6ekF9jlp0qR8hy1fvny5goKCXKrXHRYuXChp/98Pp3vR/v37tX+/K314F/dsF8kd28abanFPPRUkedc6AQAAeMKpU6cKNd0VByWLxZLnGiRnhwnfu3evnnrqKS1fvlwBAQGXXObFDMO45DJHjRrlMKBEVlaWatWqpY4dO6pixYpO1eouW7duVVxcnKS1kpo624ukOK1du1ZNmzrbh3dxz3aR3LFtvKkW99XzkaSHXexDKo37HgAAKFtyzza7HKdOvevfv7/8/f0lSdnZ2Xr00UcVHBzsMN2SJUsu29emTZt06NAhtWjRwt52/vx5rV27Vm+99ZbS0tIkXTiyVKNGDfs0hw4duuQ1Uf7+/vb6Lubr6ytfX9/L1lWUfHx8dPr0aV0YcNDZWnwknZaPj4/H18dd3LNdJHdsG2+qxb31eM86AQAAeEphf8NccVDq16+fw/P4+Pgr7cKuffv2+vnnnx3aBgwYoIYNG2rkyJG6+uqrFRERoRUrVuj666+XdGFY8jVr1mjy5MlOLxcAAAAALuWKg9KcOXPctvCQkBA1btzYoS04OFjVqlWztyckJCg5OVn16tVTvXr1lJycrKCgIPXu3dttdQAofhkZGbJarS71ERoaqqioKDdVBAAA8P9cHsyhqD377LM6ffq0hg4dqiNHjqhVq1Zavnw591ACSrCMjAw1aBCj7OzCXUxZkICAIKWlpRKWAACA23ldUFq9erXDc4vFoqSkJCUlJXmkHgDuZ7Va/w5J8yXFONlLqrKz42W1WglKAADA7bwuKAEoS2IkNfd0EQAAAHn4eLoAAAAAAPA2BCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4YHR6mUmprqkXlR/NzxfoWGhnIvJgAA4ICghFImU5KP4uPjPV0Iipz73uuAgCClpaUSlgAAgB1BCaXMUUk2SfN14WamzvhS0hh3FYQic1Suv9eSlKrs7HhZrVaCEgAAsCMooZSKkdTcyXk59a5kceW9BgAAyB+DOQAAAACACUeU4BYZGRmyWq1Oz88ACgAAAPAmBCW4LCMjQw0axCg7+5SnSwEAAADcgqAEl1mt1r9DEgMoAAAAoHQgKMGNGEABAAAApQODOQAAAACACUeUAKAUcnWAFUkKDQ3l3lIAgDKLoAQApYy7BlgJCAhSWloqYQkAUCYRlACglHHPACupys6Ol9VqJSgBAMokghIAlFquDLACAEDZxmAOAAAAAGDCESWgiKWmujb0uavzAwAA4MoRlIAikynJR/Hx8Z4uBAAAAFeIoAQUmaOSbHLtgnpJ+lLSGHcUBAAAgEIiKAFFztUL6jn1DgAAoLgxmAMAAAAAmHBECQDg9TIyMmS1Wl3uJzQ0lPtCAQAKhaAEAPBqGRkZatAg5u+b6LomICBIaWmphCUAwGURlAAAXs1qtf4dklwdGCVV2dnxslqtBCUAwGURlAAAJYSrA6MAAFB4DOYAAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYMKodwAgKTU11aX5uZEpAAClC0EJQBmXKclH8fHxLvXCjUwBAChdCEoAyrijkmxy7Wam3MgUAIDShqAEAJK4mSkAALgYgzkAAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACUEJAAAAAEwISgAAAABgUt7TBQAoeVJTUz06PwAAQFEjKAG4ApmSfBQfH+/pQgAAAIoUQQnAFTgqySZpvqQYF/r5UtIYdxQEAABQJAhKAJwQI6m5C/Nz6h0AAPBuDOYAAAAAACYcUQIAL5KRkSGr1epSHwyWAQCA6whKAOAlMjIy1KBBjLKzT3m6FAAAyjyCEgB4CavV+ndIYrAMAAA8jaAEAF6HwTIAAPA0BnMAAAAAABOOKJVxXDgOuI+rnwU+S8XDHds5NDRUUVFRbqgGAOCtCEplGBeOA+6SKclH8fHxni4El+S+9ykgIEhpaamEJQAoxQhKZRgXjgPuclSSTXyWvN1Rued9SlV2drysVitBCQBKMYISxIXjgLvwWSoZXH2fAABlAYM5AAAAAIAJR5QAAAVi4AMAQFlFUAIA5IOBDwAAZRtBCQCQj6Ni4AMAQFlGUAIAXAIDHwAAyiYGcwAAAAAAE44oAQAAt8jIyJDVanW5HwYAAeANCEoAAMBlGRkZatAg5u8bmbuGAUAAeAOCEgAAcJnVav07JDEACIDSgaAEAADciAFAAJQODOYAAAAAACYEJQAAAAAwISgBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACASXlPFwDnpaamenR+ACjLXP0ODQ0NVVRUlJuqAQC4G0GpRMqU5KP4+HhPFwIAZZB7voMDAoKUlpZKWAIAL+XRoDRp0iQtWbJEv/76qwIDAxUbG6vJkyerQYMG9mkMw9D48eM1e/ZsHTlyRK1atdLbb7+ta6+91oOVe9pRSTZJ8yXFuNDPl5LGuKMgAChDjsr17+BUZWfHy2q1EpQAwEt5NCitWbNGjz/+uG644QadO3dOo0ePVseOHbVjxw4FBwdLkqZMmaJp06Zp7ty5ql+/viZOnKgOHTooLS1NISEhnizfC8RIau7C/Jx6BwDOc/U7GADgzTwalP773/86PJ8zZ47CwsK0adMmxcXFyTAMTZ8+XaNHj1aPHj0kSfPmzVN4eLgWLFigIUOGeKJsAAAAAKWcV12jdOzYMUlS1apVJUm7d+/WwYMH1bFjR/s0/v7+atOmjVJSUgoMSmfOnNGZM2fsz7OysiRJOTk5ysnJKaryC8VmsykwMFAXTttwpRZ39OFt/VBL0fbjTbW4qx9qKdp+3NGHTVKgUlNTZbPZnOohLS3NTd+bkvds3wvbxWazefzvkru47+9b6ds2ALxLYb9bLIZhGEVcS6EYhqG7775bR44c0bfffitJSklJUevWrbV//35FRkbap33kkUeUnp6uZcuW5dtXUlKSxo8fn6d9wYIFCgoKKpoVAAAAAOD1Tp06pd69e+vYsWOqWLFigdN5zRGlYcOGadu2bfruu+/yvGaxWByeG4aRp+1io0aNUmJiov15VlaWatWqpY4dO15yYxSHrVu3Ki4uTtJaSU2d7OUjSQ+72Ie39UMtRduPN9Xirn6opWj7cXct70pqcJlpC7Jc0kQ31uIN/WyVFKe1a9eqaVNXavEe7vn7JpXGbQPAu+SebXY5XhGUnnjiCS1dulRr165VzZo17e0RERGSpIMHD6pGjRr29kOHDik8PLzA/vz9/eXv75+n3dfXV76+vm6s/Mr5+Pjo9OnTunCvX1dqcUcf3tYPtRRtP95Ui7v6oZai7cedtbgy8EGam2vxhn58JJ2Wj4+Px/8uuYv7/r6Vvm0DwLsU9rvFp4jruCTDMDRs2DAtWbJE33zzjerUqePwep06dRQREaEVK1bY286ePas1a9YoNja2uMsFAAAAUEZ49IjS448/rgULFujzzz9XSEiIDh48KEmqVKmSAgMDZbFYlJCQoOTkZNWrV0/16tVTcnKygoKC1Lt3b0+WDgCAy1JTXb9NQ2hoKPdiAoAi4NGgNHPmTElS27ZtHdrnzJmj/v37S5KeffZZnT59WkOHDrXfcHb58uXcQwkAUIJlSvJRfHy8yz0FBAQpLS2VsAQAbubRoFSYAfcsFouSkpKUlJRU9AUBAFAsjurCMNjzdeH6LWelKjs7XlarlaAEAG7mFYM5AABQNrkyyAUAoCh5dDAHAAAAAPBGHFECAAAASqGMjAxZrVaX+ymrg8YQlAAAAIBSJiMjQw0axCg7+5TLfZXVQWMISgAAAEApY7Va/w5JDBrjLIISAAAAUGoxaIyzGMwBAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmDA8OAEAZl5GRIavV6lIfqampbqoGALwDQQkAgDIsIyNDDRrE/H1jSgBALoISAABlmNVq/TskzdeFG1M660tJY9xTFAB4AYISAADQhZDU3IX5OfUOQOnCYA4AAAAAYMIRJQAASjhXBlIorYMwuGOACkkKDQ1VVFSUGypCWeGOfY/9zjsQlAAAKLEyJfkoPj7e04V4FXcOUBEQEKS0tFR+tKJQ3LXvsd95B4ISAAAl1lFJNrk2EEPpG4TBfQNUpCo7O15Wq5UfrCgU9+x77HfegqAEAECJ58pADKXz1LsLXB2gAnAW+15pwGAOAAAAAGDCESUAAIBLcMeAF1ycD5Q8BCUAAIB8uW+wDC7OB0oeghIAAEC+jsr1wTIkLs4HSiaCEgAAwCVxYT5QFjGYAwAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGBCUAIAAAAAE4ISAAAAAJgQlAAAAADApLynCwAAAEDZlJGRIavV6nI/oaGhioqKckNFwP8jKAEAAKDYZWRkqEGDGGVnn3K5r4CAIKWlpRKW4FYEJQAAABQ7q9X6d0iaLynGhZ5SlZ0dL6vVSlCCWxGUAAAA4EExkpp7ugggDwZzAAAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAAAAADAhKAEAAACACcODAwAA4IpkZGTIarW61EdqaqqbqgGKBkEJAAAAhZaRkaEGDWL+vlksUHoRlAAAAFBoVqv175A0XxduFuusLyWNcU9RQBEgKAEAAMAJMZKauzA/p97BuzGYAwAAAACYcEQJAACgBHDHAAqSFBoaqqioKDdUBJRuBCUAAAAv584BFAICgpSWlkpYAi6DoAQAAODl3DeAQqqys+NltVoJSsBlEJQAAABKDFcHUABQWAzmAAAAAAAmHFECAAAoY1JTnR+a25V5UXK5+r6XxEFECEoAAABlRqYkH8XHx3u6EJQY7tlnSuIgIgQlAACAMuOoJJtcGxTiS0lj3FUQvN5Rub7PlMxBRAhKAAAAZY4rg0Jw6l3ZVPYGEmEwBwAAAAAw4YgSAADwOgw2gLLO1f2Yz4HrCEoAAMCLMNgAyjo+A96CoAQAALzIUTHYAMq2o3L9MyDxOXAdQQkAAHghBhtAWefq4Al8DlzFYA4AAAAAYMIRJQAAgGLAABVAyUJQAgAAKFJcnA+URAQlAACAInVUDFABlDwEJQAAgGLBABVAScJgDgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMGF4cAAAAJR4qamuDaEeGhqqqKgoN1WD0oCgBAAAgBIsU5KP4uPjXeolICBIaWmphCXYEZQAAABQgh2VZJM0Xxdu6uuMVGVnx8tqtRKUYEdQAgAAQCkQI6m5p4tAKcJgDgAAAABgwhElAAAAQK4PCOHq/PAuBCUAAACUce4ZEAKlC0EJAAAAZdxRuT4ghCR9KWmMOwqCFyAoAQAAAJJcHxCCU+9KEwZzAAAAAAATghIAAAAAmBCUAAAAAMCkxASlGTNmqE6dOgoICFCLFi307bfferokAAAAAKVUiQhKixYtUkJCgkaPHq3Nmzfr1ltvVefOnZWRkeHp0gAAAACUQiUiKE2bNk2DBg3S4MGDFRMTo+nTp6tWrVqaOXOmp0sDAAAAUAp5/fDgZ8+e1aZNm/Tcc885tHfs2FEpKSn5znPmzBmdOXPG/vzYsWOSpL/++ks5OTlFV2whZGVlKSAgQNImSVlO9pImydU+vK0fainafrypFnf1Qy1F2w+1FG0/1FK0/XhTLe7qh1qKth9qKdp+dkkKUFZWlg4fPuxCLe5x/PhxSZJhGJee0PBy+/fvNyQZ33//vUP7Sy+9ZNSvXz/fecaNG2dI4sGDBw8ePHjw4MGDB498H3v37r1kDvH6I0q5LBaLw3PDMPK05Ro1apQSExPtz202m/766y9Vq1atwHmKS1ZWlmrVqqW9e/eqYsWKHq0FJQP7DK4U+wyuBPsLrhT7DK6Ut+0zhmHo+PHjioyMvOR0Xh+UQkNDVa5cOR08eNCh/dChQwoPD893Hn9/f/n7+zu0Va5cuahKdErFihW9YkdBycE+gyvFPoMrwf6CK8U+gyvlTftMpUqVLjuN1w/m4OfnpxYtWmjFihUO7StWrFBsbKyHqgIAAABQmnn9ESVJSkxMVJ8+fdSyZUvdfPPNmj17tjIyMvToo496ujQAAAAApVCJCEq9evXS4cOH9eKLLyozM1ONGzfWl19+qejoaE+XdsX8/f01bty4PKcGAgVhn8GVYp/BlWB/wZVin8GVKqn7jMUwLjcuHgAAAACULV5/jRIAAAAAFDeCEgAAAACYEJQAAAAAwISgBAAAAAAmBKViNGPGDNWpU0cBAQFq0aKFvv32W0+XBC81adIk3XDDDQoJCVFYWJi6d++utLQ0T5eFEmTSpEmyWCxKSEjwdCnwYvv371d8fLyqVaumoKAgNWvWTJs2bfJ0WfBS586d0wsvvKA6deooMDBQV199tV588UXZbDZPlwYvsXbtWnXr1k2RkZGyWCz67LPPHF43DENJSUmKjIxUYGCg2rZtq+3bt3um2EIgKBWTRYsWKSEhQaNHj9bmzZt16623qnPnzsrIyPB0afBCa9as0eOPP64ffvhBK1as0Llz59SxY0edPHnS06WhBNiwYYNmz56tJk2aeLoUeLEjR46odevW8vX11VdffaUdO3bo1VdfVeXKlT1dGrzU5MmTNWvWLL311ltKTU3VlClTNHXqVL355pueLg1e4uTJk2ratKneeuutfF+fMmWKpk2bprfeeksbNmxQRESEOnTooOPHjxdzpYXD8ODFpFWrVmrevLlmzpxpb4uJiVH37t01adIkD1aGkuB///ufwsLCtGbNGsXFxXm6HHixEydOqHnz5poxY4YmTpyoZs2aafr06Z4uC17oueee0/fff8/ZDSi0rl27Kjw8XO+995697d5771VQUJA++OADD1YGb2SxWPTpp5+qe/fuki4cTYqMjFRCQoJGjhwpSTpz5ozCw8M1efJkDRkyxIPV5o8jSsXg7Nmz2rRpkzp27OjQ3rFjR6WkpHioKpQkx44dkyRVrVrVw5XA2z3++OO68847dfvtt3u6FHi5pUuXqmXLlrr//vsVFham66+/Xu+++66ny4IXu+WWW7Ry5Urt3LlTkrR161Z999136tKli4crQ0mwe/duHTx40OH3sL+/v9q0aeO1v4fLe7qAssBqter8+fMKDw93aA8PD9fBgwc9VBVKCsMwlJiYqFtuuUWNGzf2dDnwYh9++KF++uknbdiwwdOloAT4448/NHPmTCUmJur555/X+vXr9eSTT8rf3199+/b1dHnwQiNHjtSxY8fUsGFDlStXTufPn9dLL72kBx980NOloQTI/c2b3+/h9PR0T5R0WQSlYmSxWByeG4aRpw0wGzZsmLZt26bvvvvO06XAi+3du1dPPfWUli9froCAAE+XgxLAZrOpZcuWSk5OliRdf/312r59u2bOnElQQr4WLVqk+fPna8GCBbr22mu1ZcsWJSQkKDIyUv369fN0eSghStLvYYJSMQgNDVW5cuXyHD06dOhQnlQNXOyJJ57Q0qVLtXbtWtWsWdPT5cCLbdq0SYcOHVKLFi3sbefPn9fatWv11ltv6cyZMypXrpwHK4S3qVGjhho1auTQFhMTo8WLF3uoIni7Z555Rs8995weeOABSdJ1112n9PR0TZo0iaCEy4qIiJB04chSjRo17O3e/HuYa5SKgZ+fn1q0aKEVK1Y4tK9YsUKxsbEeqgrezDAMDRs2TEuWLNE333yjOnXqeLokeLn27dvr559/1pYtW+yPli1b6qGHHtKWLVsIScijdevWeW47sHPnTkVHR3uoIni7U6dOycfH8adjuXLlGB4chVKnTh1FREQ4/B4+e/as1qxZ47W/hzmiVEwSExPVp08ftWzZUjfffLNmz56tjIwMPfroo54uDV7o8ccf14IFC/T5558rJCTEfjSyUqVKCgwM9HB18EYhISF5rmELDg5WtWrVuLYN+Ro+fLhiY2OVnJysnj17av369Zo9e7Zmz57t6dLgpbp166aXXnpJUVFRuvbaa7V582ZNmzZNAwcO9HRp8BInTpzQb7/9Zn++e/dubdmyRVWrVlVUVJQSEhKUnJysevXqqV69ekpOTlZQUJB69+7twaoLxvDgxWjGjBmaMmWKMjMz1bhxY7322msM9Yx8FXSu7pw5c9S/f//iLQYlVtu2bRkeHJf0xRdfaNSoUdq1a5fq1KmjxMREPfzww54uC17q+PHjGjNmjD799FMdOnRIkZGRevDBBzV27Fj5+fl5ujx4gdWrV6tdu3Z52vv166e5c+fKMAyNHz9e77zzjo4cOaJWrVrp7bff9tp/0CMoAQAAAIAJ1ygBAAAAgAlBCQAAAABMCEoAAAAAYEJQAgAAAAATghIAAAAAmBCUAAAAAMCEoAQAAAAAJgQlAABKsddff13r1q3zdBkAUOIQlAAAJUZSUpKaNWtWZP33799f3bt3d6mP1atXy2Kx6OjRo5KkuXPnqnLlyi7XtmfPHlksFm3ZsqXQ80ybNk1LlixR8+bNXV4+AJQ1BCUAKEMOHTqkIUOGKCoqSv7+/oqIiFCnTp044vC3119/XXPnznWpj9jYWGVmZqpSpUruKepvtWrVUmZmpho3biwpbyAz++GHH/TBBx/o888/l7+/v1trAYCyoLynCwAAFJ97771XOTk5mjdvnq6++mr9+eefWrlypf76668iW+bZs2fl5+dXZP27kzvCjZ+fnyIiItxQzf/L3YZX0u9NN92kzZs3u7UOAChLOKIEAGXE0aNH9d1332ny5Mlq166doqOjdeONN2rUqFG68847HaZ75JFHFB4eroCAADVu3FhffPGF/fXFixfr2muvlb+/v2rXrq1XX33VYTm1a9fWxIkT1b9/f1WqVEkPP/ywJCklJUVxcXEKDAxUrVq19OSTT+rkyZOXrPnll19WeHi4QkJCNGjQIGVnZ+eZZs6cOYqJiVFAQIAaNmyoGTNmXLLPTz75RNddd50CAwNVrVo13X777fY6zKfetW3bVk888YQSEhJUpUoVhYeHa/bs2Tp58qQGDBigkJAQ1a1bV1999ZV9nssd6fn999919913Kzw8XBUqVNANN9ygr7/++rLb8OJT7/bs2aN27dpJkqpUqSKLxaL+/ftLkgzD0JQpU3T11VcrMDBQTZs21SeffHLJbQIAyIugBABlRIUKFVShQgV99tlnOnPmTL7T2Gw2de7cWSkpKZo/f7527Nihl19+WeXKlZMkbdq0ST179tQDDzygn3/+WUlJSRozZkye09WmTp2qxo0ba9OmTRozZox+/vlnderUST169NC2bdu0aNEifffddxo2bFiB9X700UcaN26cXnrpJW3cuFE1atTIE4LeffddjR49Wi+99JJSU1OVnJysMWPGaN68efn2mZmZqQcffFADBw5UamqqVq9erR49esgwjALrmDdvnkJDQ7V+/Xo98cQTeuyxx3T//fcrNjZWP/30kzp16qQ+ffro1KlTBfZxsRMnTqhLly76+uuvtXnzZnXq1EndunVTRkbGJbfhxWrVqqXFixdLktLS0pSZmanXX39dkvTCCy9ozpw5mjlzprZv367hw4crPj5ea9asKVR9AIC/GQCAMuOTTz4xqlSpYgQEBBixsbHGqFGjjK1bt9pfX7ZsmeHj42OkpaXlO3/v3r2NDh06OLQ988wzRqNGjezPo6Ojje7duztM06dPH+ORRx5xaPv2228NHx8f4/Tp0/ku6+abbzYeffRRh7ZWrVoZTZs2tT+vVauWsWDBAodpJkyYYNx888359rlp0yZDkrFnz558X+/Xr59x991325+3adPGuOWWW+zPz507ZwQHBxt9+vSxt2VmZhqSjHXr1hmGYRirVq0yJBlHjhwxDMMw5syZY1SqVCnf5eVq1KiR8eabb9qf57cNd+/ebUgyNm/enO9yDMMwTpw4YQQEBBgpKSkO8w4aNMh48MEHL1kDAMARR5QAoAy59957deDAAS1dulSdOnXS6tWr1bx5c/sRoS1btqhmzZqqX79+vvOnpqaqdevWDm2tW7fWrl27dP78eXtby5YtHabZtGmT5s6daz+qVaFCBXXq1Ek2m027d+8ucFk333yzQ9vFz//3v/9p7969GjRokEO/EydO1O+//55vn02bNlX79u113XXX6f7779e7776rI0eO5L+x/takSRP7/5crV07VqlXTddddZ28LDw+XdGGgjMI4efKknn32WTVq1EiVK1dWhQoV9Ouvv+Y5omTehoWxY8cOZWdnq0OHDg7b5P333y9wmwAA8sdgDgBQxgQEBKhDhw7q0KGDxo4dq8GDB2vcuHHq37+/AgMDLzmvYRiyWCx52syCg4MdnttsNg0ZMkRPPvlknmmjoqKcWIsLfUoXTr9r1aqVw2u5pwqalStXTitWrFBKSoqWL1+uN998U6NHj9aPP/6oOnXq5DuPr6+vw3OLxeLQlrs9cuu5nGeeeUbLli3TK6+8omuuuUaBgYG67777dPbsWYfpzNuwMHJr+M9//qOrrrrK4TVGvgOAK0NQAoAyrlGjRvrss88kXTh6sm/fPu3cuTPfo0qNGjXSd99959CWkpKi+vXrFxhOJKl58+bavn27rrnmmkLXFRMTox9++EF9+/a1t/3www/2/w8PD9dVV12lP/74Qw899FCh+7VYLGrdurVat26tsWPHKjo6Wp9++qkSExML3Ycrvv32W/Xv31/33HOPpAvXLO3Zs+eK+8kdSfDiI3mNGjWSv7+/MjIy1KZNG7fUCwBlFUEJAMqIw4cP6/7779fAgQPVpEkThYSEaOPGjZoyZYruvvtuSVKbNm0UFxene++9V9OmTdM111yjX3/9VRaLRXfccYeefvpp3XDDDZowYYJ69eqldevW6a233rrsSHMjR47UTTfdpMcff1wPP/ywgoODlZqaqhUrVujNN9/Md56nnnpK/fr1U8uWLXXLLbfoX//6l7Zv366rr77aPk1SUpKefPJJVaxYUZ07d9aZM2e0ceNGHTlyJN/g8+OPP2rlypXq2LGjwsLC9OOPP+p///ufYmJiXNiyV+aaa67RkiVL1K1bN1ksFo0ZM6bQR6MuFh0dLYvFoi+++EJdunRRYGCgQkJCNGLECA0fPlw2m0233HKLsrKylJKSogoVKqhfv35FsEYAUDoRlACgjKhQoYJatWql1157Tb///rtycnJUq1YtPfzww3r++eft0y1evFgjRozQgw8+qJMnT+qaa67Ryy+/LOnCkaGPPvpIY8eO1YQJE1SjRg29+OKL9qGpC9KkSROtWbNGo0eP1q233irDMFS3bl316tWrwHl69eql33//XSNHjlR2drbuvfdePfbYY1q2bJl9msGDBysoKEhTp07Vs88+q+DgYF133XVKSEjIt8+KFStq7dq1mj59urKyshQdHa1XX31VnTt3LvyGdNFrr72mgQMHKjY2VqGhoRo5cqSysrKuuJ+rrrpK48eP13PPPacBAwaob9++mjt3riZMmKCwsDBNmjRJf/zxhypXrqzmzZs7vMcAgMuzGPmdXA4AAAAAZRij3gEAAACACUEJAAAAAEwISgAAAABgQlACAAAAABOCEgAAAACYEJQAAAAAwISgBAAAAAAmBCUAAAAAMCEoAQAAAIAJQQkAAAAATAhKAAAAAGDyf55qpZUP+PhdAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(evaluation_simlex['score'], bins=40, color='blue', edgecolor='black')\n",
    "plt.title('Histogramme des scores de similarité dans SimLex-999')\n",
    "plt.xlabel('Score de similarité')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Corrélation de Spearman\n",
    "\n",
    "\n",
    "La corrélation de Spearman est une mesure de la relation monotone entre deux variables $x$ et $y$. Elle est comprise entre -1 et 1. Plutôt que de comparer les valeurs brutes des variables, elle compare leurs rangs. Elle est calculée comme suit :\n",
    "$$\\rho (x, y) = 1 - \\frac{6 \\sum_{i=1}^n (r_{x_i} - r_{y_i})^2}{n(n^2 - 1)}$$\n",
    "où $r_{x_i}$ est le rang de la i-ème valeur de la variable x, $r_{y_i}$ est le rang de la i-ème valeur de la variable y, $n$ est le nombre total de paires d'observations $(x, y)$.\n",
    "\n",
    "Les rangs sont attribués en ordonnant les valeurs de chaque variable du plus petit au plus grand. La plus petite valeur reçoit le rang 1, la suivante le rang 2, et ainsi de suite.\n",
    "\n",
    "##### Expliquez pourquoi on utilise la corrélation de Spearman plutôt que la corrélation entre les valeurs des variables. (3 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Implémentation de la corrélation de Spearman (2 points)\n",
    "\n",
    "Complétez la fonction suivante pour calculer la corrélation de Spearman entre deux listes de valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.076060300Z",
     "start_time": "2024-10-02T17:35:58.070259700Z"
    }
   },
   "outputs": [],
   "source": [
    "def spearman_rank_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calcule la corrélation de Spearman entre deux listes de valeurs.\n",
    "\n",
    "    Args:\n",
    "        x   : list of float\n",
    "        y   : list of float\n",
    "\n",
    "    Returns:\n",
    "    La corrélation de Spearman entre les deux listes (float). \n",
    "    \"\"\"\n",
    "\n",
    "    # Transformation de x en DataFrame\n",
    "    x_df = pd.DataFrame(x, columns=['value'])\n",
    "    x_df['rank'] = x_df['value'].rank() # Obtenir le rang\n",
    "   \n",
    "    # Transformation de y en DataFrame\n",
    "    y_df = pd.DataFrame(y, columns=['value'])\n",
    "    y_df['rank'] = y_df['value'].rank() # Obtenir le rang\n",
    "    \n",
    "    # Calcul de la somme des différences des rangs au carré\n",
    "    sum_diff_ranks = 0\n",
    "    n  = len(x_df)\n",
    "    for i in range(n) : \n",
    "        sum_diff_ranks += (x_df.iloc[i]['rank'] - y_df.iloc[i]['rank'])**2\n",
    "        \n",
    "    # Calcul de la corrélation de Spearman\n",
    "    spearman_corr = 1 - 6*(sum_diff_ranks)/(n * (n**2 -1))\n",
    "    \n",
    "    return spearman_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Évaluation du modèle GloVe (2 points)\n",
    "\n",
    "\n",
    "Retrouvez les plongements du modèle GloVe de tous les mots du jeu de données SimLex-999, puis calculez la similarité cosinus entre les paires.\n",
    "\n",
    "Calculez ensuite la corrélation de Spearman entre les scores de simlex et les similarités cosinus obtenues et affichez-la."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.254935700Z",
     "start_time": "2024-10-02T17:35:58.084568300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation de Spearman: 0.29\n"
     ]
    }
   ],
   "source": [
    "pairs = evaluation_simlex[['word 1', 'word 2', 'score']].values.tolist()\n",
    "\n",
    "similarities = []\n",
    "simlex_scores = []\n",
    "\n",
    "for word1, word2, score in pairs: # Pour chaque paire de Simlex\n",
    "    if word1 in glove_word_to_index and word2 in glove_word_to_index:\n",
    "        embedding1 = glove_embeddings[glove_word_to_index[word1]] # Récupérer la représentation de word 1\n",
    "        embedding2 = glove_embeddings[glove_word_to_index[word2]] # Récupérer la représentation de word 2\n",
    "        \n",
    "        cosine_sim = cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item() # Calcul de la similarité cosinus entre les deux paires\n",
    "        similarities.append(cosine_sim) # Stocker la similarité\n",
    "        simlex_scores.append(score) #Stocker le score\n",
    "\n",
    "\n",
    "spearman_corr = spearman_rank_correlation(similarities, simlex_scores) # Calcul de la corrélation de Spearman\n",
    "\n",
    "print(f\"Correlation de Spearman: {spearman_corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résultat attendu: 0.29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Interprétation du résultat (3 points)\n",
    "\n",
    "Qu'est ce que ce nombre représente et que peut-on en conclure sur la qualité des plongements GloVe (2 conclusions) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Description de la méthode contrastive (33 Points)\n",
    "\n",
    "Nous allons maintenant implémenter une méthode contrastive de plongements de mots. Elle vise à améliorer les plongements lexicaux de mots en tenant compte des synonymes et antonymes. \n",
    "\n",
    "Notre modèle se basera simplement sur une matrice de plongements de mots, qui associe à chaque mot un vecteur de plongement.\n",
    "\n",
    "L'idée est d'entraîner ce modèle à rapprocher les plongements de synonymes et d'éloigner ceux d'antonymes.\n",
    "\n",
    "La cellule suivante définit le modèle et ses attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.254935700Z",
     "start_time": "2024-10-02T17:35:58.196026600Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveWordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, embeddings, device='cpu', margin_plus=0.6, margin_minus=0., regularization=1e-9):\n",
    "        super(ContrastiveWordEmbeddingModel, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "        # Hyperparamètres pour les fonctions de coût\n",
    "        self.margin_plus = margin_plus\n",
    "        self.margin_minus = margin_minus\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        # Initialisation des plongements de mots\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings.detach().clone(), freeze=False).to(self.device)\n",
    "        self.original_embeddings = nn.Embedding.from_pretrained(embeddings.detach().clone(), freeze=True).to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:50:23.469102500Z",
     "start_time": "2024-10-02T17:50:23.453346300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Création des négatifs\n",
    "\n",
    "Pendant l'entraînement, au lieu de traiter tout le jeu d'entraînement d'un coup, nous allons avoir des lots (batchs) de paires de synonymes $B_S$ et d'antonymes $B_A$.\n",
    "\n",
    "Dans un lot de synonymes, on définit le négatif d'un mot comme le mot du lot le plus proche qui n'est pas dans la même paire. Intuitivement, c'est le mot que le modèle devrait confondre le plus avec le synonyme. Similairement, dans un lot d'antonymes, on définit le négatif d'un mot comme le mot du lot le plus éloigné qui n'est pas dans la même paire.\n",
    "\n",
    "On répète ce processus pour chaque mot de chaque paire de synonymes et d'antonymes.\n",
    "\n",
    "*Attention, un mot peut apparaître plusieurs fois dans un lot avec des synonymes ou antonymes différents, et il ne peut être le négatif d'aucun de ses synonymes, ou antonymes.*\n",
    "\n",
    "##### 3.1.1 Exemple pour illustrer l'implémentation\n",
    "\n",
    "Prenons un exemple avec un lot $B_S$ de synonymes de taille 3. On veut construire le lot de négatifs $T_S$\n",
    "\n",
    "$B_S$:\n",
    "- (arbre, plante)\n",
    "- (voiture, véhicule)\n",
    "- (arbre, buisson)\n",
    "\n",
    "On a 5 mots uniques dans le lot: arbre, plante, voiture, véhicule, buisson. Supposons que la matrice de similarité cosinus soit la suivante :\n",
    "\n",
    "|       | arbre | plante | voiture | véhicule | buisson |\n",
    "|-------|-------|--------|--------|----------|---------|\n",
    "| arbre | 1     | 0.8    | 0.1    | 0.2      | 0.9     |\n",
    "| plante| 0.8   | 1      | 0.3    | 0.4      | 0.7     |\n",
    "| voiture| 0.1  | 0.3    | 1      | 0.9      | 0.2     |\n",
    "| véhicule| 0.2 | 0.4    | 0.9    | 1        | 0.3     |\n",
    "| buisson| 0.9  | 0.7    | 0.2    | 0.3      | 1       |\n",
    "\n",
    "\n",
    "On commence par calculer les voisins de chaque mot du lot $B_S$. Le voisin d'un mot $m$ est défini comme tout mot qui apparait dans au moins une paire avec $m$ dans $B_S$. Un mot est aussi considéré comme son propre voisin.\n",
    "\n",
    "- voisins de arbre : arbre, plante, buisson\n",
    "- voisins de plante : plante, arbre\n",
    "- voisins de voiture : voiture, véhicule\n",
    "- voisins de véhicule : véhicule, voiture\n",
    "- voisins de buisson : buisson, arbre\n",
    "\n",
    "Après avoir masqué les voisins, la matrice est :\n",
    "\n",
    "|       | arbre | plante | voiture | véhicule | buisson |\n",
    "|-------|-------|--------|--------|----------|---------|\n",
    "| arbre | -inf  | -inf   | 0.1    | 0.2      | -inf    |\n",
    "| plante| -inf  | -inf   | 0.3    | 0.4      | 0.7     |\n",
    "| voiture| 0.1  | 0.3    | -inf   | -inf     | 0.2     |\n",
    "| véhicule| 0.2 | 0.4    | -inf   | -inf     | 0.3     |\n",
    "| buisson| -inf | 0.7    | 0.2    | 0.3      | -inf    |\n",
    "\n",
    "Pour calculer les négatifs, on prend le maximum de chaque ligne (donc le mot le plus similaire qui n'est pas un voisin) :\n",
    "\n",
    "Ici,\n",
    "- le négatif d'arbre est véhicule\n",
    "- le négatif de plante est buisson\n",
    "- le négatif de voiture est plante\n",
    "- le négatif de véhicule est plante\n",
    "- le négatif de buisson est plante\n",
    "\n",
    "En reprenant le batch $B_S$:\n",
    "- (arbre, plante)\n",
    "- (voiture, véhicule)\n",
    "- (arbre, buisson)\n",
    "\n",
    "$T_S$ sera composé de paires composées du négatif de chaque élément de $B_S$ :\n",
    "\n",
    "$B_S$ -> $T_S$\n",
    "- (arbre, plante) $\\rightarrow$ (véhicule, buisson), car le négatif d'arbre est véhicule et le négatif de plante est buisson\n",
    "- (voiture, véhicule) $\\rightarrow$ (plante, plante), car le négatif de voiture est plante et le négatif de véhicule est plante\n",
    "- (arbre, buisson) $\\rightarrow$ (véhicule, plante), car le négatif d'arbre est véhicule et le négatif de buisson est plante\n",
    "\n",
    "$T_S$ sera donc : \n",
    "- (véhicule, buisson)\n",
    "- (plante, plante)\n",
    "- (véhicule, plante)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Implémentez la fonction `prepare_neighbors` qui renvoit la liste des voisins de chaque mot dans le lot. (4 points)\n",
    "\n",
    "Les voisins d'un mot $m$ sont tous les mots du lot qui apparaissent dans au moins une paire avec $m$. Utilisez les bons indices (indice dans la matrice d'embeddings et indice dans le lot). Le résultat est une liste de liste de voisins, où `neighbors[i]` est la liste des voisins du mot `i` dans le lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.254935700Z",
     "start_time": "2024-10-02T17:35:58.201949600Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_neighbors(index_pairs, unique_idx, index_to_idx):\n",
    "    \"\"\"\n",
    "    Prépare les voisins pour chaque mot dans les paires de mots.\n",
    "    \n",
    "    Args : \n",
    "        index_pairs     : torch.Tensor de seconde dimension 2\n",
    "        Tensor contenant les indices des embeddings des mots dans le vocabulaire. \n",
    "        Des indices qui sont reliés ensemble par une ligne dans ce tenseur ont \n",
    "        une relation sémantique entre eux (synonymes ou antonymes).\n",
    "\n",
    "        unique_idx      : set\n",
    "        Ensemble de tous les indices qui sont mentionnés dans la liste `index_pairs`.\n",
    "        \n",
    "        index_to_idx    : dict\n",
    "        Dictionnaire associant un indice mentionné dans `index_pairs` à son indice dans\n",
    "        la liste qui sera retournée. Par exemple, si dans ce dictionnaire, la clé 4 est\n",
    "        associée à la valeur 12, cela veut dire que les voisins du mot 4 dans le vocabulaire\n",
    "        seront retournés à l'indice 12 dans la liste de retour.\n",
    "\n",
    "    Returns:\n",
    "    Une liste où chaque élément est une liste des indices des voisins pour chaque mot (l'indice dans la liste correspond à l'indice unique).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation de la liste\n",
    "    neighbors = [[] for _ in range(len(unique_idx))]\n",
    "    \n",
    "    word_pairs = index_pairs.tolist()\n",
    "    \n",
    "    for word1, word2 in word_pairs: # pour chaque paire word1/word2\n",
    "        idx1 = index_to_idx[word1]\n",
    "        idx2 = index_to_idx[word2]\n",
    "        \n",
    "        neighbors[idx1].append(idx1) # word1 est son propre voisin\n",
    "        neighbors[idx1].append(idx2) # word2 est voisin de word1\n",
    "        \n",
    "        neighbors[idx2].append(idx2) # word2 est son propre voisin\n",
    "        neighbors[idx2].append(idx1) # word1 est voisin de word2\n",
    "\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.254935700Z",
     "start_time": "2024-10-02T17:35:58.209858100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "index_pairs = torch.tensor([[0, 12], [12, 31], [53, 4]])\n",
    "unique_idx = {0, 4, 12, 31, 53}\n",
    "index_to_idx = {0: 0, 4: 1, 12: 2, 31: 3, 53: 4}\n",
    "print(prepare_neighbors(index_pairs, unique_idx, index_to_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse attendue\n",
    "\n",
    "`[[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Implémentez la fonction `select_negatives` qui renvoit un dictionnaire qui associe à chaque élément son négatif. (4 points)\n",
    "\n",
    "Pour chaque élément du lot, on cherche le voisin le plus proche qui n'est pas le voisin de l'autre élément de la paire.\n",
    "\n",
    "Utilisez un masque pour cacher, dans la matrice de similarité, les voisins.\n",
    "\n",
    "La fonction utilise un paramètre `synonym` qui indique si on travaille sur un lot de synonymes ou d'antonymes. En cas de synonymes, on cherche le voisin le plus proche qui n'est pas un voisin de l'autre élément de la paire. En cas d'antonymes, on cherche le voisin le plus éloigné qui n'est pas un voisin de l'autre élément de la paire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.259160200Z",
     "start_time": "2024-10-02T17:35:58.217347500Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_negatives(indices, similarity_matrix, neighbors, synonym=True):\n",
    "    \"\"\"\n",
    "    Sélectionne les exemples négatifs à partir de la matrice de similarité et des voisins.\n",
    "\n",
    "    Args : \n",
    "        indices             : torch.Tensor (vocab_size)\n",
    "        Indices des mots présents dans le vocabulaires\n",
    "        \n",
    "        similarity_matrix   : torch.Tensor (vocab_size, vocab_size)\n",
    "        Matrice de similarité entre tous les mots présents dans le vocabulaire.\n",
    "        \n",
    "        neighbors           : list of lists\n",
    "        Liste des voisins de chaque mot. Par exemple, le premier élément de la liste\n",
    "        contiendra tous les voisins du mot 0 dans le vocabulaire.\n",
    "\n",
    "        synonym             : bool, optional (default=True)\n",
    "        Indique si l'on cherche des négatifs pour les synonymes (True) ou pour les antonymes (False).\n",
    "\n",
    "    Returns:\n",
    "    Dictionnaire mappant les mots avec leurs indices de négatifs {mot_index: négatif_index}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Masquage des voisins dans la matrice de similarité\n",
    "\n",
    "    # Attention : On considère pour le masquage que le mappage des valeurs dans neighbours correspond aux indices de la matrice de similarité\n",
    "    # (e.g la première liste de neigbours correspond aux voisins du mot représeenté par la première ligne de la matrice de similarité)\n",
    "    # C'est le cas si :\n",
    "    # -> index_to_idx utilisé dans prepare_neighbors est de la forme {x0: 0, x1: 1, ..., xn: n} \n",
    "    # -> {x0, x1, ..., xn} est la liste ordonnée des indices \"indices\" donné à cosine_similarity (soit similarity_matrix = cosine_similarity(model.embeddings(indices), model.embeddings(indices))\n",
    "    \n",
    "    mask_value = float('-inf') if synonym else float('+inf')\n",
    "\n",
    "    for index, neighbors_list in enumerate(neighbors):\n",
    "        for neighbour in neighbors_list:\n",
    "            similarity_matrix[index][neighbour] = mask_value\n",
    "\n",
    "\n",
    "    # Création du dictionnaire\n",
    "\n",
    "    # Attention : On considère pour le création du dictionnaire que l'ordre des valeurs dans indices correspond à l'ordre de la matrice de similarité\n",
    "    # (e.g le premier élément de indices a sa représentation dans la première ligne de la matrice de similarité)\n",
    "    # C'est le cas si :\n",
    "    # -> la liste indices \"indices\" est donnée à cosine_similarity (soit similarity_matrix = cosine_similarity(model.embeddings(indices), model.embeddings(indices))\n",
    "\n",
    "    result_dict = {\n",
    "        index : (similarity_matrix[i].argmax() if synonym else similarity_matrix[i].argmin()).item() # Association de chaque indice à l'indice du voisin le plus proche (synonyme) ou le plus éloigné (antonyme)\n",
    "        for i, index in enumerate(indices.tolist())\n",
    "    }\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.259160200Z",
     "start_time": "2024-10-02T17:35:58.225614800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 3, 2: 4, 3: 1, 4: 2}\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "indices = torch.tensor([0, 1, 2, 3, 4])\n",
    "neighbors = [[0, 2], [1, 4], [2, 0, 2, 3], [3, 2], [4, 1]]\n",
    "\n",
    "similarity_matrix = torch.tensor([\n",
    "    [ 1.0000, -0.4263, -0.7167, -0.9838, -0.5823],\n",
    "    [-0.4263,  1.0000, -0.1600,  0.5088, -0.3708],\n",
    "    [-0.7167, -0.1600,  1.0000,  0.7247,  0.5631],\n",
    "    [-0.9838,  0.5088,  0.7247,  1.0000,  0.4394],\n",
    "    [-0.5823, -0.3708,  0.5631,  0.4394,  1.0000]\n",
    "    ])\n",
    "\n",
    "print(select_negatives(indices, similarity_matrix, neighbors, synonym=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse attendue\n",
    "\n",
    "`{0: 1, 1: 3, 2: 4, 3: 1, 4: 2}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4 Implémentez la fonction `run_negative_extraction` qui prépare les paires de synonymes et d'antonymes et appelle `prepare_neighbors` et `select_negatives`. (4 points)\n",
    "\n",
    "Préparez les indices uniques des mots du batch, calculez la similarité des mots, et appelez `prepare_neighbors` et `select_negatives`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:08:41.267744400Z",
     "start_time": "2024-10-02T18:08:41.251847400Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_negative_extraction(model, index_pairs, synonym=True):\n",
    "    \"\"\"\n",
    "    Extrait les exemples négatifs pour un ensemble de paires de mots.\n",
    "\n",
    "    Args :\n",
    "        index_pairs : torch.Tensor de seconde dimension 2\n",
    "        Contient les indices des mots.\n",
    "\n",
    "        synonym     : bool, optional (default=True)\n",
    "        Indique si l'on cherche des négatifs pour les synonymes (True) ou pour les antonymes (False).\n",
    "\n",
    "    Returns:\n",
    "    Dictionnaire mappant les indices des mots avec leurs indices de négatifs {mot_index: négatif_index}.\n",
    "    \"\"\"\n",
    "    \n",
    "    device = model.device\n",
    "\n",
    "    # Move index_pairs to the correct device\n",
    "    index_pairs = index_pairs.to(device)\n",
    "    \n",
    "\n",
    "    # Préparation des paramètres de prepare_neighbors\n",
    "    unique_ids = set([idx.item() for row in index_pairs for idx in row]) # liste des indices uniques\n",
    "    index_to_ids = {idx: i for i, idx in enumerate(sorted(list(unique_ids)))} # dictionnaire de mappage vers les indices de la matrice de similarité (clés, qui correspondent aux ids des mots, triées par ordre croissant)\n",
    "\n",
    "\n",
    "    # Préparation des paramètres de select_negatives\n",
    "    indices = torch.tensor(sorted(list(unique_ids)), device = device) # liste d'indices (valeurs, qui correspondent aux ids des mots, triées par ordre croissant)\n",
    "    embeddings = model.embeddings(indices)\n",
    "    similarity_matrix = cosine_similarity(embeddings, embeddings) # matrice de similarité (les lignes et les colonnes donc triées par ordre croissant des ids des mots)\n",
    "\n",
    "    # Appels des deux fonctions\n",
    "    neighbors = prepare_neighbors(index_pairs, unique_ids, index_to_ids)\n",
    "    negatives = select_negatives(indices, similarity_matrix, neighbors, synonym)\n",
    "\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Fonctions de coût\n",
    "\n",
    "Pour chaque paire de synonymes $(x^l, x^r)$ *pour x left et x right* dans le lot $B_S$, nous trouvons un négatif $(t^l, t^r)$ ce qui constitue le lot $T_S$:\n",
    "- $ t^l $ est le mot dans le lot le plus proche de $ x^l $ mais qui n'est pas $ x^r $.\n",
    "- $ t^r $ est le mot dans le lot le plus proche de $ x^r $ mais qui n'est pas $ x^l $.\n",
    "\n",
    "De même, pour chaque paire d'antonymes $(x^l, x^r)$ dans le lot $B_A$, nous trouvons un négatif $(t^l, t^r)$ ce qui constitue le lot $T_A$:\n",
    "- $ t^l $ est le mot dans le lot le plus éloigné de $ x^l $ mais qui n'est pas $ x^r $.\n",
    "- $ t^r $ est le mot dans le lot le plus éloigné de $ x^r $ mais qui n'est pas $ x^l $.\n",
    "\n",
    "\n",
    "Comparer un mot à son synonyme (ou antonyme) et à son négatif permet d'entraîner le modèle sur des exemples difficiles qui forcent le modèle à apprendre des représentations plus robustes.\n",
    "\n",
    "Il y aura trois fonctions de coût :\n",
    "1. **Attraction** : Attire les synonymes plus proches les uns des autres.\n",
    "2. **Répulsion** : Repousse les antonymes plus loin les uns des autres.\n",
    "3. **Régularisation** : Évite que les plongements ne s'éloignent trop de ceux du modèle pré-entraîné.\n",
    "\n",
    "Les fonctions de coût sont définies comme suit, en sommant sur `i`, les paires de synonymes et d'antonymes dans les lots $B_S$ et $B_A$ :\n",
    "\n",
    "1. **Attraction** :\n",
    "$$ S(B_S, T_S) = \\sum_{i=1}^{|B_S|} \\left[ \\max \\left(0, \\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r \\right) + \\max \\left( 0, \\delta_{syn} + x_i^r t_i^r - x_i^l x_i^r \\right) \\right] $$\n",
    "\n",
    "1. **Répulsion** :\n",
    "$$ A(B_A, T_A) = \\sum_{i=1}^{|B_A|} \\left[ \\max \\left(0, \\delta_{ant} + x_i^l x_i^r - x_i^l t_i^l \\right) + \\max \\left( 0, \\delta_{ant} + x_i^r x_i^l - x_i^r t_i^r \\right) \\right] $$\n",
    "\n",
    "1. **Régularisation** :\n",
    "$$ R(B_S, B_A) = \\sum_{x_i \\in V(B_S \\cup B_A)} \\lambda_{reg} \\| \\hat{x}_i - x_i \\|^2 $$\n",
    "\n",
    "La fonction de coût totale est la somme de ces trois termes :\n",
    "$$ C(B_S, T_S, B_A, T_A) = S(B_S, T_S) + A(B_A, T_A) + R(B_S, B_A) $$\n",
    "\n",
    "$\\delta_{syn}$, $\\delta_{ant}$ et $\\lambda_{reg}$ sont des hyperparamètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec l'exemple précédent, prenons\n",
    "> + $x_i^l$ : voiture\n",
    "> + $x_i^r$: véhicule\n",
    "> + $t_i^l$: plante\n",
    "> \n",
    "> On veut que voiture et véhicule aient un plus grand produit scalaire que voiture et plante, donc que $\\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r <0$, et donc que $S$ soit minimisé. De même pour la deuxième partie de l'équation, symmétrique, avec le 2nd élément du couple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Implémentez la fonction `synonym_cost` qui calcule la fonction de coût d'attraction (sur les paires de synonymes). (5 points)\n",
    "\n",
    "$$ S(B_S, T_S) = \\sum_{i=1}^{|B_S|} \\left[ \\max \\left(0, \\delta_{syn} + x_i^l t_i^l - x_i^l x_i^r \\right) + \\max \\left( 0, \\delta_{syn} + x_i^r t_i^r - x_i^l x_i^r \\right) \\right] $$\n",
    "\n",
    "Le membre de gauche pénalise si le mot de gauche est plus éloigné de son négatif que de son synonyme. De même, le membre de droite pénalise si le mot de gauche est plus éloigné de son négatif que de son synonyme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:20:24.350418Z",
     "start_time": "2024-10-02T18:20:24.348755300Z"
    }
   },
   "outputs": [],
   "source": [
    "def synonym_cost(model, synonym_pairs, synonym_negatives):\n",
    "    \"\"\"\n",
    "    Calcule le coût d'attraction pour les paires de synonymes.\n",
    "\n",
    "    synonym_pairs: liste de tuples d'indices de paires de synonymes\n",
    "    synonym_negatives: dictionnaire de mots avec leurs négatifs {mot_index: négatif_index}\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total pour les paires de synonymes\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "     # Initialisation du coût à zéro\n",
    "    cost = torch.tensor(0.0, device=model.device) \n",
    "    for pair in synonym_pairs :\n",
    "        x_l = model.embeddings(pair[0])\n",
    "        x_r = model.embeddings(pair[1])\n",
    "        t_l = model.embeddings(torch.tensor(synonym_negatives[pair[0].item()] , device= model.device))\n",
    "        t_r = model.embeddings(torch.tensor(synonym_negatives[pair[1].item()] , device= model.device))\n",
    "        \n",
    "        cost += torch.relu(torch.dot(x_l, t_l) - torch.dot(x_l,x_r) + model.margin_plus) + torch.relu(torch.dot(x_r, t_r) - torch.dot(x_l,x_r) + model.margin_plus)\n",
    "        \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Implémentez la fonction `antonym_cost` qui calcule la fonction de coût de répulsion (sur les paires d'antonymes). (5 points)\n",
    "\n",
    "$$ A(B_A, T_A) = \\sum_{i=1}^{|B_A|} \\left[ \\max \\left(0, \\delta_{ant} + x_i^l x_i^r - x_i^l t_i^l \\right) + \\max \\left( 0, \\delta_{ant} + x_i^r x_i^l - x_i^r t_i^r \\right) \\right] $$\n",
    "\n",
    "Le membre de gauche pénalise si le mot de gauche est plus éloigné de son antonyme que de son négatif. De même, le membre de droite pénalise si le mot de gauche est plus éloigné de son antonyme que de son négatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:20:26.587307Z",
     "start_time": "2024-10-02T18:20:26.578900700Z"
    }
   },
   "outputs": [],
   "source": [
    "def antonym_cost(model, antonym_pairs, antonym_negatives):\n",
    "    \"\"\"\n",
    "    Calcule le coût de répulsion pour les paires d'antonymes.\n",
    "\n",
    "    antonym_pairs: liste de tuples d'indices de paires d'antonymes\n",
    "    antonym_negatives: dictionnaire de mots avec leurs négatifs {mot_index: négatif_index}\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total pour les paires d'antonymes\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation du cout à zéro\n",
    "    cost = torch.tensor(0.0, device=model.device) \n",
    "    for pair in antonym_pairs :\n",
    "        x_l = model.embeddings(pair[0])\n",
    "        x_r = model.embeddings(pair[1])\n",
    "        t_l = model.embeddings(torch.tensor(antonym_negatives[pair[0].item()] , device= model.device))\n",
    "        t_r = model.embeddings(torch.tensor(antonym_negatives[pair[1].item()] , device= model.device))\n",
    "        \n",
    "        cost += torch.relu(torch.dot(x_l,x_r) -torch.dot(x_l, t_l)+ model.margin_minus) + torch.relu(torch.dot(x_l,x_r)  -torch.dot(x_r, t_r) + model.margin_minus)\n",
    "        \n",
    "    return cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Implémentez la fonction `regularization_cost` qui calcule la fonction de coût de régularisation. (4 points)\n",
    "\n",
    "$$ R(B_S, B_A) = \\sum_{x_i \\in V(B_S \\cup B_A)} \\lambda_{reg} \\| \\hat{x}_i - x_i \\|^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:21:20.502703200Z",
     "start_time": "2024-10-02T18:21:20.472005300Z"
    }
   },
   "outputs": [],
   "source": [
    "def regularization_cost(model, synonym_pairs, antonym_pairs):\n",
    "    \"\"\"\n",
    "    Calcule le coût de régularisation pour les paires de synonymes et antonymes.\n",
    "\n",
    "    synonym_pairs: liste de tuples d'indices de paires de synonymes\n",
    "    antonym_pairs: liste de tuples d'indices de paires d'antonymes\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor, coût total de régularisation\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation du cout à zéro\n",
    "    cost = torch.tensor(0.0, device=model.device) \n",
    "    \n",
    "    # Récupérer l'ensemble des indices\n",
    "    indexes = set([syn_index for pair in synonym_pairs for syn_index in pair] + [ant_index for pair  in antonym_pairs for ant_index in pair])\n",
    "    for i in indexes :\n",
    "\n",
    "        # Calcul des paramètres\n",
    "        x_hat = model.original_embeddings(i)\n",
    "        x = model.embeddings(i)\n",
    "\n",
    "        # Application de la régularisation\n",
    "        cost += model.regularization * torch.norm(x_hat - x, p=2)**2\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Mise en place\n",
    "##### 3.3.1 Implémentez la fonction `forward` qui utilise les fonctions définies plus tôt pour calculer le coût total. (4 points)\n",
    "\n",
    "La fonction prend en entrée un lot de synonymes et un lot d'antonymes, c'est-à-dire des paires de synonymes et des paires d'antonymes.\n",
    "\n",
    "Vous devez trouver les négatifs de tous les mots des lots au moyen de votre fonction  run_negative_extraction puis calculer la fonction de coût totale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:15:51.640104300Z",
     "start_time": "2024-10-02T18:15:51.608724700Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(model, synonym_pairs, antonym_pairs):\n",
    "    \"\"\"\n",
    "    Fonction forward pour calculer le coût total.\n",
    "\n",
    "    Args : \n",
    "        synonym_pairs   : \n",
    "        Liste de tuples d'indices de paires de synonymes\n",
    "\n",
    "        antonym_pairs   : \n",
    "        Liste de tuples d'indices de paires d'antonymes\n",
    "\n",
    "    Returns:\n",
    "    Tenseur contenant le coût total (attraction, répulsion et régularisation)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcul de tous les négatifs\n",
    "    neg_syn_dict = run_negative_extraction(model, synonym_pairs)\n",
    "    neg_ant_dict = run_negative_extraction(model, antonym_pairs, synonym=False)\n",
    "\n",
    "    # Calcul du cout total\n",
    "    cost  = synonym_cost(model, synonym_pairs, neg_syn_dict) + antonym_cost(model, antonym_pairs, neg_ant_dict) + regularization_cost(model, synonym_pairs, antonym_pairs)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Évaluation (3 points)\n",
    "\n",
    "Utilisez la fonction `spearman_rank_correlation` pour compléter la fonction d'évaluation `evaluate` qui exécute le modèle sur le jeu d'évaluation et calcule la corrélation de Spearman entre les scores prédits et réels.\n",
    "\n",
    "Utilisez `torch.no_grad()` pour éviter de stocker les gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:35:58.337402200Z",
     "start_time": "2024-10-02T17:35:58.259676Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, eval_data, word_to_index):\n",
    "    \"\"\"\n",
    "    Calclue les prédictions du modèle sur le jeu d'évaluation puis la corrélation de Spearman entre les scores prédits et réels.\n",
    "\n",
    "    model: modèle de plongements de mots\n",
    "    eval_data: pd.DataFrame\n",
    "    word_to_index: dict\n",
    "\n",
    "    Returns:\n",
    "    float, la corrélation de Spearman entre les scores prédits par le modèle et réels.\n",
    "    \"\"\"\n",
    "    # Mettre le modèle en mode évaluation\n",
    "    model.eval()\n",
    "    device = model.device\n",
    "\n",
    "    # Obtenir les indices des paires de mots de eval_data\n",
    "    word_pairs = list(zip(eval_data['word 1'], eval_data['word 2']))\n",
    "    eval_indices = [(word_to_index[w1], word_to_index[w2]) for w1, w2 in word_pairs if w1 in word_to_index and w2 in word_to_index]\n",
    "\n",
    "    # Scores réels\n",
    "    real_scores = eval_data['score'].values\n",
    "\n",
    "    # Scores prédits\n",
    "    predicted_scores = []\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for id_word1, id_word2 in eval_indices:\n",
    "            word1_embedding = model.embeddings(torch.tensor(id_word1, device=device)) # Récupérer la représentation de word 1 issue du modèle\n",
    "            word2_embedding = model.embeddings(torch.tensor(id_word2, device=device)) # Récupérer la représentation de word 2 issue du modèle\n",
    "            cosine_sim = cosine_similarity(word1_embedding.unsqueeze(0), word2_embedding.unsqueeze(0)).item() # Calcul de la similarité cosinus entre les deux paires\n",
    "            predicted_scores.append(cosine_sim) # Stocker la similarité\n",
    "            \n",
    "    return spearman_rank_correlation(real_scores, predicted_scores) # Calculer la corrélation de Spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:52:41.209190Z",
     "start_time": "2024-10-02T17:52:40.433091300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.29163879410472593"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ContrastiveWordEmbeddingModel(glove_embeddings, device='cuda')\n",
    "evaluate(model, evaluation_simlex, glove_word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:52:47.410419100Z",
     "start_time": "2024-10-02T17:52:47.394802800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Entraînement de zéro (16 Points)\n",
    "\n",
    "Nous allons maintenant entraîner le modèle de zéro, sans utiliser les plongements GloVe pré-entraînés. Ensuite, dans la partie 5, nous entraînerons le modèle en l'initialisant avec les plongements GloVe pré-entraînés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T18:15:57.260642100Z",
     "start_time": "2024-10-02T18:15:56.706595400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9730, 14810],\n",
      "        [27410,  6684],\n",
      "        [ 5027,  3734],\n",
      "        ...,\n",
      "        [   61,  7257],\n",
      "        [22695,  7062],\n",
      "        [  799, 10038]])\n",
      "tensor([[29492, 22090],\n",
      "        [26217,  4303],\n",
      "        [12561,  1466],\n",
      "        ...,\n",
      "        [ 2120,  5228],\n",
      "        [ 6579, 16620],\n",
      "        [30950,   573]])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamètres, optimiseur et DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-3\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "train_syn_tensor = data_to_tensor(train_synonyms, glove_word_to_index)\n",
    "train_ant_tensor = data_to_tensor(train_antonyms, glove_word_to_index)\n",
    "\n",
    "print(train_syn_tensor)\n",
    "print(train_ant_tensor)\n",
    "\n",
    "\n",
    "syn_data_loader = DataLoader(train_syn_tensor, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "ant_data_loader = DataLoader(train_ant_tensor, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "#print(len(syn_data_loader))\n",
    "#print(len(ant_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Complétez la cellule suivante pour créer le modèle de zéro `model_zero`, à partir d'une matrice de plongements aléatoire. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:53:15.391099700Z",
     "start_time": "2024-10-02T17:53:15.359515500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_zero = None\n",
    "random_seed = 0\n",
    "random_generator = torch.Generator(device=device).manual_seed(random_seed)\n",
    "\n",
    "embeddings_size = glove_embeddings.size()\n",
    "random_init_embeddings = torch.randn(embeddings_size, generator=random_generator, device=device)\n",
    "\n",
    "model_zero = ContrastiveWordEmbeddingModel(random_init_embeddings, device='cuda')\n",
    "\n",
    "optimizer = optim.Adam(model_zero.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU: True\n"
     ]
    }
   ],
   "source": [
    "is_on_gpu = next(model.parameters()).is_cuda\n",
    "print(f\"Model is on GPU: {is_on_gpu}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:54:21.395035700Z",
     "start_time": "2024-10-02T17:54:21.383025Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is using GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if is_on_gpu:\n",
    "    current_device = torch.cuda.current_device()  # Get the current GPU device index\n",
    "    device_name = torch.cuda.get_device_name(current_device)  # Get the name of the GPU\n",
    "    print(f\"Model is using GPU: {device_name}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T17:54:36.381947300Z",
     "start_time": "2024-10-02T17:54:36.334575200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Entraînez le modèle sur le jeu des paires de synonymes et d'antonymes. (6 points)\n",
    "\n",
    "N'oubliez pas que l'entraînement se fait sur les synonymes et antonymes et que l'évaluation se fait sur SimLex-999.\n",
    "\n",
    "À défaut d'avoir un jeu de validation, on observe les résultats sur le corpus de test : SimLex-999. Ceci n’est fait qu’à titre illustratif pour voir l'évolution de l'apprentissage. Il ne faut pas faire de choix pour l’entrainement à partir des résultats sur le corpus de test.\n",
    "\n",
    "*Note : Les jeux de synonymes et d'antonymes n'ont pas la même taille. Une époque (epoch) correspond à une itération sur le jeu de données le plus petit.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-02T18:21:35.289979900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, Spearman Correlation: 0.0493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number : 0\n",
      "batch number : 1\n",
      "batch number : 2\n",
      "batch number : 3\n",
      "batch number : 4\n",
      "batch number : 5\n",
      "batch number : 6\n",
      "batch number : 7\n",
      "batch number : 8\n",
      "batch number : 9\n",
      "batch number : 10\n",
      "batch number : 11\n",
      "batch number : 12\n",
      "batch number : 13\n",
      "batch number : 14\n",
      "batch number : 15\n",
      "batch number : 16\n",
      "batch number : 17\n",
      "batch number : 18\n",
      "batch number : 19\n",
      "batch number : 20\n",
      "batch number : 21\n",
      "batch number : 22\n",
      "batch number : 23\n",
      "batch number : 24\n",
      "batch number : 25\n",
      "batch number : 26\n",
      "batch number : 27\n",
      "batch number : 28\n",
      "batch number : 29\n",
      "batch number : 30\n",
      "batch number : 31\n",
      "batch number : 32\n",
      "batch number : 33\n",
      "batch number : 34\n",
      "batch number : 35\n",
      "batch number : 36\n",
      "batch number : 37\n",
      "batch number : 38\n",
      "batch number : 39\n",
      "batch number : 40\n",
      "batch number : 41\n",
      "batch number : 42\n",
      "batch number : 43\n",
      "batch number : 44\n",
      "batch number : 45\n",
      "batch number : 46\n",
      "batch number : 47\n",
      "batch number : 48\n",
      "batch number : 49\n",
      "batch number : 50\n",
      "batch number : 51\n",
      "batch number : 52\n",
      "batch number : 53\n",
      "batch number : 54\n",
      "batch number : 55\n",
      "batch number : 56\n",
      "batch number : 57\n",
      "batch number : 58\n",
      "batch number : 59\n",
      "batch number : 60\n",
      "batch number : 61\n",
      "batch number : 62\n",
      "batch number : 63\n",
      "batch number : 64\n",
      "batch number : 65\n",
      "batch number : 66\n",
      "batch number : 67\n",
      "batch number : 68\n",
      "batch number : 69\n",
      "batch number : 70\n",
      "batch number : 71\n",
      "batch number : 72\n",
      "batch number : 73\n",
      "batch number : 74\n",
      "batch number : 75\n",
      "batch number : 76\n",
      "batch number : 77\n",
      "batch number : 78\n",
      "batch number : 79\n",
      "batch number : 80\n",
      "batch number : 81\n",
      "batch number : 82\n",
      "batch number : 83\n",
      "batch number : 84\n",
      "batch number : 85\n",
      "batch number : 86\n",
      "batch number : 87\n",
      "batch number : 88\n",
      "batch number : 89\n",
      "batch number : 90\n",
      "batch number : 91\n",
      "batch number : 92\n",
      "batch number : 93\n",
      "batch number : 94\n",
      "batch number : 95\n",
      "batch number : 96\n",
      "batch number : 97\n",
      "batch number : 98\n",
      "batch number : 99\n",
      "batch number : 100\n",
      "batch number : 101\n",
      "batch number : 102\n",
      "batch number : 103\n",
      "batch number : 104\n",
      "batch number : 105\n",
      "batch number : 106\n",
      "batch number : 107\n",
      "batch number : 108\n",
      "batch number : 109\n",
      "batch number : 110\n",
      "batch number : 111\n",
      "batch number : 112\n",
      "batch number : 113\n",
      "batch number : 114\n",
      "batch number : 115\n",
      "batch number : 116\n",
      "batch number : 117\n",
      "batch number : 118\n",
      "batch number : 119\n",
      "batch number : 120\n",
      "batch number : 121\n",
      "batch number : 122\n",
      "batch number : 123\n",
      "batch number : 124\n",
      "batch number : 125\n",
      "batch number : 126\n",
      "batch number : 127\n",
      "batch number : 128\n",
      "batch number : 129\n",
      "batch number : 130\n",
      "batch number : 131\n",
      "batch number : 132\n",
      "batch number : 133\n",
      "batch number : 134\n",
      "batch number : 135\n",
      "batch number : 136\n",
      "batch number : 137\n",
      "batch number : 138\n",
      "batch number : 139\n",
      "batch number : 140\n",
      "batch number : 141\n",
      "batch number : 142\n",
      "batch number : 143\n",
      "batch number : 144\n",
      "batch number : 145\n",
      "batch number : 146\n",
      "batch number : 147\n",
      "batch number : 148\n",
      "batch number : 149\n",
      "batch number : 150\n",
      "batch number : 151\n",
      "batch number : 152\n",
      "batch number : 153\n",
      "batch number : 154\n",
      "batch number : 155\n",
      "batch number : 156\n",
      "batch number : 157\n",
      "batch number : 158\n",
      "batch number : 159\n",
      "batch number : 160\n",
      "batch number : 161\n",
      "batch number : 162\n",
      "batch number : 163\n",
      "batch number : 164\n",
      "batch number : 165\n",
      "batch number : 166\n",
      "batch number : 167\n",
      "batch number : 168\n",
      "batch number : 169\n",
      "batch number : 170\n",
      "batch number : 171\n",
      "batch number : 172\n",
      "batch number : 173\n",
      "batch number : 174\n",
      "batch number : 175\n",
      "batch number : 176\n",
      "batch number : 177\n",
      "batch number : 178\n",
      "batch number : 179\n",
      "batch number : 180\n",
      "batch number : 181\n",
      "batch number : 182\n",
      "batch number : 183\n",
      "batch number : 184\n",
      "batch number : 185\n",
      "Epoch 1/20, Loss: 457322.4590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:28<28:10, 88.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Spearman Correlation: 0.0963\n",
      "batch number : 0\n",
      "batch number : 1\n",
      "batch number : 2\n",
      "batch number : 3\n",
      "batch number : 4\n",
      "batch number : 5\n",
      "batch number : 6\n",
      "batch number : 7\n",
      "batch number : 8\n",
      "batch number : 9\n",
      "batch number : 10\n",
      "batch number : 11\n",
      "batch number : 12\n",
      "batch number : 13\n",
      "batch number : 14\n",
      "batch number : 15\n",
      "batch number : 16\n",
      "batch number : 17\n",
      "batch number : 18\n",
      "batch number : 19\n",
      "batch number : 20\n",
      "batch number : 21\n",
      "batch number : 22\n",
      "batch number : 23\n",
      "batch number : 24\n",
      "batch number : 25\n",
      "batch number : 26\n",
      "batch number : 27\n",
      "batch number : 28\n",
      "batch number : 29\n",
      "batch number : 30\n",
      "batch number : 31\n",
      "batch number : 32\n",
      "batch number : 33\n",
      "batch number : 34\n",
      "batch number : 35\n",
      "batch number : 36\n",
      "batch number : 37\n",
      "batch number : 38\n",
      "batch number : 39\n",
      "batch number : 40\n",
      "batch number : 41\n",
      "batch number : 42\n",
      "batch number : 43\n",
      "batch number : 44\n",
      "batch number : 45\n",
      "batch number : 46\n",
      "batch number : 47\n",
      "batch number : 48\n",
      "batch number : 49\n",
      "batch number : 50\n",
      "batch number : 51\n",
      "batch number : 52\n",
      "batch number : 53\n",
      "batch number : 54\n",
      "batch number : 55\n",
      "batch number : 56\n",
      "batch number : 57\n",
      "batch number : 58\n",
      "batch number : 59\n",
      "batch number : 60\n",
      "batch number : 61\n",
      "batch number : 62\n",
      "batch number : 63\n",
      "batch number : 64\n",
      "batch number : 65\n",
      "batch number : 66\n",
      "batch number : 67\n",
      "batch number : 68\n",
      "batch number : 69\n",
      "batch number : 70\n",
      "batch number : 71\n",
      "batch number : 72\n",
      "batch number : 73\n",
      "batch number : 74\n",
      "batch number : 75\n",
      "batch number : 76\n",
      "batch number : 77\n",
      "batch number : 78\n",
      "batch number : 79\n",
      "batch number : 80\n",
      "batch number : 81\n",
      "batch number : 82\n",
      "batch number : 83\n",
      "batch number : 84\n",
      "batch number : 85\n",
      "batch number : 86\n",
      "batch number : 87\n",
      "batch number : 88\n",
      "batch number : 89\n",
      "batch number : 90\n",
      "batch number : 91\n",
      "batch number : 92\n",
      "batch number : 93\n",
      "batch number : 94\n",
      "batch number : 95\n",
      "batch number : 96\n",
      "batch number : 97\n",
      "batch number : 98\n",
      "batch number : 99\n",
      "batch number : 100\n",
      "batch number : 101\n",
      "batch number : 102\n",
      "batch number : 103\n",
      "batch number : 104\n",
      "batch number : 105\n",
      "batch number : 106\n",
      "batch number : 107\n",
      "batch number : 108\n",
      "batch number : 109\n",
      "batch number : 110\n",
      "batch number : 111\n",
      "batch number : 112\n",
      "batch number : 113\n",
      "batch number : 114\n",
      "batch number : 115\n",
      "batch number : 116\n",
      "batch number : 117\n",
      "batch number : 118\n",
      "batch number : 119\n",
      "batch number : 120\n",
      "batch number : 121\n",
      "batch number : 122\n",
      "batch number : 123\n",
      "batch number : 124\n",
      "batch number : 125\n",
      "batch number : 126\n",
      "batch number : 127\n",
      "batch number : 128\n",
      "batch number : 129\n",
      "batch number : 130\n",
      "batch number : 131\n",
      "batch number : 132\n",
      "batch number : 133\n",
      "batch number : 134\n",
      "batch number : 135\n",
      "batch number : 136\n",
      "batch number : 137\n",
      "batch number : 138\n",
      "batch number : 139\n",
      "batch number : 140\n",
      "batch number : 141\n",
      "batch number : 142\n",
      "batch number : 143\n",
      "batch number : 144\n",
      "batch number : 145\n",
      "batch number : 146\n",
      "batch number : 147\n",
      "batch number : 148\n",
      "batch number : 149\n",
      "batch number : 150\n",
      "batch number : 151\n",
      "batch number : 152\n",
      "batch number : 153\n",
      "batch number : 154\n",
      "batch number : 155\n",
      "batch number : 156\n",
      "batch number : 157\n",
      "batch number : 158\n",
      "batch number : 159\n",
      "batch number : 160\n",
      "batch number : 161\n",
      "batch number : 162\n",
      "batch number : 163\n",
      "batch number : 164\n",
      "batch number : 165\n",
      "batch number : 166\n",
      "batch number : 167\n",
      "batch number : 168\n",
      "batch number : 169\n",
      "batch number : 170\n",
      "batch number : 171\n",
      "batch number : 172\n",
      "batch number : 173\n",
      "batch number : 174\n",
      "batch number : 175\n",
      "batch number : 176\n",
      "batch number : 177\n",
      "batch number : 178\n",
      "batch number : 179\n",
      "batch number : 180\n",
      "batch number : 181\n",
      "batch number : 182\n",
      "batch number : 183\n",
      "batch number : 184\n",
      "batch number : 185\n",
      "Epoch 2/20, Loss: 323246.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [02:57<26:32, 88.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Spearman Correlation: 0.1251\n",
      "batch number : 0\n",
      "batch number : 1\n",
      "batch number : 2\n",
      "batch number : 3\n",
      "batch number : 4\n",
      "batch number : 5\n",
      "batch number : 6\n",
      "batch number : 7\n",
      "batch number : 8\n",
      "batch number : 9\n",
      "batch number : 10\n",
      "batch number : 11\n",
      "batch number : 12\n",
      "batch number : 13\n",
      "batch number : 14\n",
      "batch number : 15\n",
      "batch number : 16\n",
      "batch number : 17\n",
      "batch number : 18\n",
      "batch number : 19\n",
      "batch number : 20\n",
      "batch number : 21\n",
      "batch number : 22\n",
      "batch number : 23\n",
      "batch number : 24\n",
      "batch number : 25\n",
      "batch number : 26\n",
      "batch number : 27\n",
      "batch number : 28\n",
      "batch number : 29\n",
      "batch number : 30\n",
      "batch number : 31\n",
      "batch number : 32\n",
      "batch number : 33\n",
      "batch number : 34\n",
      "batch number : 35\n",
      "batch number : 36\n",
      "batch number : 37\n",
      "batch number : 38\n",
      "batch number : 39\n",
      "batch number : 40\n",
      "batch number : 41\n",
      "batch number : 42\n",
      "batch number : 43\n",
      "batch number : 44\n",
      "batch number : 45\n",
      "batch number : 46\n",
      "batch number : 47\n",
      "batch number : 48\n",
      "batch number : 49\n",
      "batch number : 50\n",
      "batch number : 51\n",
      "batch number : 52\n",
      "batch number : 53\n",
      "batch number : 54\n",
      "batch number : 55\n",
      "batch number : 56\n",
      "batch number : 57\n",
      "batch number : 58\n",
      "batch number : 59\n",
      "batch number : 60\n",
      "batch number : 61\n",
      "batch number : 62\n",
      "batch number : 63\n",
      "batch number : 64\n",
      "batch number : 65\n",
      "batch number : 66\n",
      "batch number : 67\n",
      "batch number : 68\n",
      "batch number : 69\n",
      "batch number : 70\n",
      "batch number : 71\n",
      "batch number : 72\n",
      "batch number : 73\n",
      "batch number : 74\n",
      "batch number : 75\n",
      "batch number : 76\n",
      "batch number : 77\n",
      "batch number : 78\n",
      "batch number : 79\n",
      "batch number : 80\n",
      "batch number : 81\n",
      "batch number : 82\n",
      "batch number : 83\n",
      "batch number : 84\n",
      "batch number : 85\n",
      "batch number : 86\n",
      "batch number : 87\n",
      "batch number : 88\n",
      "batch number : 89\n",
      "batch number : 90\n",
      "batch number : 91\n",
      "batch number : 92\n",
      "batch number : 93\n",
      "batch number : 94\n",
      "batch number : 95\n",
      "batch number : 96\n",
      "batch number : 97\n",
      "batch number : 98\n",
      "batch number : 99\n",
      "batch number : 100\n",
      "batch number : 101\n",
      "batch number : 102\n",
      "batch number : 103\n",
      "batch number : 104\n",
      "batch number : 105\n",
      "batch number : 106\n",
      "batch number : 107\n",
      "batch number : 108\n",
      "batch number : 109\n",
      "batch number : 110\n",
      "batch number : 111\n",
      "batch number : 112\n",
      "batch number : 113\n",
      "batch number : 114\n",
      "batch number : 115\n",
      "batch number : 116\n",
      "batch number : 117\n",
      "batch number : 118\n",
      "batch number : 119\n",
      "batch number : 120\n",
      "batch number : 121\n",
      "batch number : 122\n",
      "batch number : 123\n",
      "batch number : 124\n",
      "batch number : 125\n",
      "batch number : 126\n",
      "batch number : 127\n",
      "batch number : 128\n",
      "batch number : 129\n",
      "batch number : 130\n",
      "batch number : 131\n",
      "batch number : 132\n",
      "batch number : 133\n",
      "batch number : 134\n",
      "batch number : 135\n",
      "batch number : 136\n",
      "batch number : 137\n",
      "batch number : 138\n",
      "batch number : 139\n",
      "batch number : 140\n",
      "batch number : 141\n",
      "batch number : 142\n",
      "batch number : 143\n",
      "batch number : 144\n",
      "batch number : 145\n",
      "batch number : 146\n",
      "batch number : 147\n",
      "batch number : 148\n",
      "batch number : 149\n",
      "batch number : 150\n",
      "batch number : 151\n",
      "batch number : 152\n",
      "batch number : 153\n",
      "batch number : 154\n",
      "batch number : 155\n",
      "batch number : 156\n",
      "batch number : 157\n",
      "batch number : 158\n",
      "batch number : 159\n",
      "batch number : 160\n",
      "batch number : 161\n",
      "batch number : 162\n",
      "batch number : 163\n",
      "batch number : 164\n",
      "batch number : 165\n",
      "batch number : 166\n",
      "batch number : 167\n",
      "batch number : 168\n",
      "batch number : 169\n",
      "batch number : 170\n",
      "batch number : 171\n",
      "batch number : 172\n",
      "batch number : 173\n",
      "batch number : 174\n",
      "batch number : 175\n",
      "batch number : 176\n",
      "batch number : 177\n",
      "batch number : 178\n",
      "batch number : 179\n",
      "batch number : 180\n",
      "batch number : 181\n",
      "batch number : 182\n",
      "batch number : 183\n",
      "batch number : 184\n",
      "batch number : 185\n",
      "Epoch 3/20, Loss: 277641.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [04:25<25:04, 88.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Spearman Correlation: 0.1399\n",
      "batch number : 0\n",
      "batch number : 1\n",
      "batch number : 2\n",
      "batch number : 3\n",
      "batch number : 4\n",
      "batch number : 5\n",
      "batch number : 6\n",
      "batch number : 7\n",
      "batch number : 8\n",
      "batch number : 9\n",
      "batch number : 10\n",
      "batch number : 11\n",
      "batch number : 12\n",
      "batch number : 13\n",
      "batch number : 14\n",
      "batch number : 15\n",
      "batch number : 16\n",
      "batch number : 17\n",
      "batch number : 18\n",
      "batch number : 19\n",
      "batch number : 20\n",
      "batch number : 21\n",
      "batch number : 22\n",
      "batch number : 23\n",
      "batch number : 24\n",
      "batch number : 25\n",
      "batch number : 26\n",
      "batch number : 27\n",
      "batch number : 28\n",
      "batch number : 29\n",
      "batch number : 30\n",
      "batch number : 31\n",
      "batch number : 32\n",
      "batch number : 33\n",
      "batch number : 34\n",
      "batch number : 35\n",
      "batch number : 36\n",
      "batch number : 37\n",
      "batch number : 38\n",
      "batch number : 39\n",
      "batch number : 40\n",
      "batch number : 41\n",
      "batch number : 42\n",
      "batch number : 43\n",
      "batch number : 44\n",
      "batch number : 45\n",
      "batch number : 46\n",
      "batch number : 47\n",
      "batch number : 48\n",
      "batch number : 49\n",
      "batch number : 50\n",
      "batch number : 51\n",
      "batch number : 52\n",
      "batch number : 53\n",
      "batch number : 54\n",
      "batch number : 55\n",
      "batch number : 56\n",
      "batch number : 57\n",
      "batch number : 58\n",
      "batch number : 59\n",
      "batch number : 60\n",
      "batch number : 61\n",
      "batch number : 62\n",
      "batch number : 63\n",
      "batch number : 64\n",
      "batch number : 65\n",
      "batch number : 66\n",
      "batch number : 67\n",
      "batch number : 68\n",
      "batch number : 69\n",
      "batch number : 70\n",
      "batch number : 71\n",
      "batch number : 72\n",
      "batch number : 73\n",
      "batch number : 74\n",
      "batch number : 75\n",
      "batch number : 76\n",
      "batch number : 77\n",
      "batch number : 78\n",
      "batch number : 79\n",
      "batch number : 80\n",
      "batch number : 81\n",
      "batch number : 82\n",
      "batch number : 83\n",
      "batch number : 84\n",
      "batch number : 85\n",
      "batch number : 86\n",
      "batch number : 87\n",
      "batch number : 88\n",
      "batch number : 89\n",
      "batch number : 90\n",
      "batch number : 91\n",
      "batch number : 92\n",
      "batch number : 93\n",
      "batch number : 94\n",
      "batch number : 95\n",
      "batch number : 96\n",
      "batch number : 97\n",
      "batch number : 98\n",
      "batch number : 99\n",
      "batch number : 100\n",
      "batch number : 101\n",
      "batch number : 102\n",
      "batch number : 103\n",
      "batch number : 104\n",
      "batch number : 105\n",
      "batch number : 106\n",
      "batch number : 107\n",
      "batch number : 108\n",
      "batch number : 109\n",
      "batch number : 110\n",
      "batch number : 111\n",
      "batch number : 112\n",
      "batch number : 113\n",
      "batch number : 114\n",
      "batch number : 115\n",
      "batch number : 116\n",
      "batch number : 117\n",
      "batch number : 118\n",
      "batch number : 119\n",
      "batch number : 120\n",
      "batch number : 121\n",
      "batch number : 122\n",
      "batch number : 123\n",
      "batch number : 124\n",
      "batch number : 125\n",
      "batch number : 126\n",
      "batch number : 127\n",
      "batch number : 128\n",
      "batch number : 129\n",
      "batch number : 130\n",
      "batch number : 131\n",
      "batch number : 132\n",
      "batch number : 133\n",
      "batch number : 134\n",
      "batch number : 135\n",
      "batch number : 136\n",
      "batch number : 137\n",
      "batch number : 138\n",
      "batch number : 139\n",
      "batch number : 140\n",
      "batch number : 141\n",
      "batch number : 142\n",
      "batch number : 143\n",
      "batch number : 144\n",
      "batch number : 145\n",
      "batch number : 146\n",
      "batch number : 147\n",
      "batch number : 148\n",
      "batch number : 149\n",
      "batch number : 150\n",
      "batch number : 151\n",
      "batch number : 152\n",
      "batch number : 153\n",
      "batch number : 154\n",
      "batch number : 155\n",
      "batch number : 156\n",
      "batch number : 157\n",
      "batch number : 158\n",
      "batch number : 159\n",
      "batch number : 160\n",
      "batch number : 161\n"
     ]
    }
   ],
   "source": [
    "spearman_corr = evaluate(model_zero, evaluation_simlex, glove_word_to_index)\n",
    "print(f'Before training, Spearman Correlation: {spearman_corr:.4f}')\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    model_zero.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for i,(syn_batch, ant_batch) in enumerate(zip(syn_data_loader, ant_data_loader)):\n",
    "        print(f'batch number : {i}')\n",
    "        syn_batch = syn_batch.to(device)\n",
    "        ant_batch = ant_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = forward(model_zero, syn_batch, ant_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Affichage de la perte totale de l'époque\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    # Évaluation de la corrélation après chaque époque\n",
    "    spearman_corr = evaluate(model_zero, evaluation_simlex, glove_word_to_index)\n",
    "    print(f'Epoch {epoch + 1}, Spearman Correlation: {spearman_corr:.4f}')\n",
    "\n",
    "# Évaluation finale après l'entraînement\n",
    "spearman_corr = evaluate(model_zero, evaluation_simlex, glove_word_to_index)\n",
    "print(f'After training, Spearman Correlation: {spearman_corr:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Courbes d'entraînement du modèle de zéro (4 points)\n",
    "\n",
    "Affichez la perte moyenne sur le jeu d'entraînement et la corrélation de Spearman sur le jeu de validation à chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-02T17:47:36.652172600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Évaluation du modèle de zéro et comparaison avec GloVe (5 points)\n",
    "\n",
    "Comparez le modèle de zéro après l'entraînement à GloVe (résultat de la partie 2.4) en termes de corrélation de Spearman sur le jeu de validation. Quelle méthode est la plus performante ? Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Intérêt de GloVe (18 Points)\n",
    "\n",
    "Dans la section précédente, nous avons entraîné un modèle de zéro.\n",
    "\n",
    "Nous allons maintenant évaluer si initialiser le modèle avec les plongements de GloVe permet d'améliorer les performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Initialisation avec GloVe (6 points)\n",
    "\n",
    "Entraînez le modèle `model_fine_tuned`, mais cette fois en ititialisant directement avec les plongements du modèle pré-entraîné GloVe.\n",
    "\n",
    "On utilisera Adam comme optimiseur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-02T17:47:36.652172600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_fine_tuned = None\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-02T17:47:36.652172600Z"
    }
   },
   "outputs": [],
   "source": [
    "spearman_corr = evaluate(model_fine_tuned, evaluation_simlex, glove_word_to_index)\n",
    "print(f'Before training, Spearman Correlation: {spearman_corr:.4f}')\n",
    "\n",
    "# TODO\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Courbes d'entraînement (4 points)\n",
    "\n",
    "Affichez la perte moyenne sur le jeu d'entraînement et la corrélation de Spearman sur le jeu de validation à chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T17:47:36.652172600Z",
     "start_time": "2024-10-02T17:47:36.652172600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Vérification sur un exemple (3 points)\n",
    "\n",
    "Avec le modèle `model_fine_tuned`, calculez la similarité cosinus entre 'fast' et 'slow' et entre 'fast' et 'rapid'. Commentez les résultats en les comparant avec ceux de la partie 1.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-02T17:47:36.652172600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Analyse, comparaison, conclusion (5 points)\n",
    "\n",
    "Comparez les performances des trois modèles (GloVe, zéro, fine-tuned). Quelle méthode est la plus performante ? Pourquoi ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrables\n",
    "Vous devez remettre votre notebook sur Moodle et Gradescope en ipynb et pdf. Pour Gradescope vous devez associer les numéros de questions avec vos réponses dans le pdf grâce à l'outil que fournit Gradescope.\n",
    "\n",
    "\n",
    "## Évaluation \n",
    "Votre TP sera évalué selon les critères suivants :\n",
    "1. Exécution correcte du code et obtention des sorties attendues\n",
    "2. Réponses correctes aux questions d'analyse\n",
    "3. Qualité du code (noms significatifs, structure, performance, gestion d’exception, etc.)\n",
    "4. Commentaires clairs et informatifs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
